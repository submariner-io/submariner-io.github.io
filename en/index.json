[
{
	"uri": "/en/contributing/website/",
	"title": "Contributing to the website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on hugo, grav, and the hugo-learn-theme and written in markdown format.\nIf you want to contribute I recommend reading the hugo-learn-theme documentation\nYou can always click the \u0026ldquo;Edit this page link\u0026rdquo; at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on github\n  Checkout your copy locally\n  $ git clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git $ cd submariner-website $ make server  Open your browser on http://localhost:1313\n  Edit files in src, the browser should automatically reload changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the github workflow here: contributing to a github project\n  "
},
{
	"uri": "/en/architecture/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Lorem Ipsum.\n"
},
{
	"uri": "/en/quickstart/",
	"title": "Quickstart guide",
	"tags": [],
	"description": "",
	"content": "Basic overview Submariner has two main core pieces (the broker and submariner), more information about this topic can be found in the Architecture section.\nThe broker The broker is an API to which all participating clusters are given access and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a Cluster, and the reachable cluster IPs from the endpoint.  The broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe submariner deployment on a cluster Once submariner is deployed on a cluster with the proper credentials to the broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n  At least 2 Kubernetes clusters, one of which is designated to serve as the central broker that is accessible by all of your connected clusters; this can be one of your connected clusters, but comes with the limitation that the cluster is required to be up to facilitate interconnectivity/negotiation.\n  Different service/pod CIDRs between clusters. This is to prevent routing conflicts.\n    Direct IP connectivity between the gateway nodes through the internet (or on the same network if not running Submariner over the internet). Submariner supports 1:1 NAT setups but has a few caveats/provider-specific configuration instructions in this configuration.  Knowledge of each cluster\u0026rsquo;s network configuration.\n  Worker node IPs on all the clusters must be outside of the cluster/service CIDR ranges.\n  An example of three clusters configured to use with Submariner would look like the following:\n   Cluster Name Provider Pods CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east OnPrem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Deployment The available methods for deployment are:\n subctl (+ submariner-operator). helm charts.  The community recommends the use of subctl, because it simplifies most of the manual steps required for deployment, as well as verification of connectivity between the clusters. In the future it may provide additional capabilities like:\n Detection of possible conflicts Upgrade management Status inspection of the deployment Configuration updates Maintenance and debugs tasks Wrapping of logs for support tasks.  To deploy submariner with subctl please follow the deployment guide. If helm fits better your deployment methodologies, please find the details here\n"
},
{
	"uri": "/en/contributing/developers/building_testing/",
	"title": "Building and Testing",
	"tags": [],
	"description": "",
	"content": "Standard Development Environment Submariner provides a standard, shared development environment suitable for all local work. The same environment is used in CI.\nThe submariner-io/shipyard project contains the logic to build the base container images used across all submariner-io repositories.\nPrescribed Tasks via Make Targets Make targets are provided to further ease the process of using the shared development environment. The specific make targets available differ by repository. For any submariner-io repository, see the Makefile at the root of the repository for the supported targets and the .travis.yml file for the targets actually used in CI.\nCommon Build and Testing Targets All submariner-io/* repositories provide a standard set of Make targets for similar building and testing actions.\nLinting To run static Go linting (goimports, golangci-lint):\nmake validate Unit tests To run Go unit tests:\nmake test End-to-end tests To run functional tests with a full multicluster deployment:\nmake e2e The e2e target supports flags to configure the deployment and testing. For example, here are two e2e flag variations used in submariner-io/submariner CI:\nmake e2e status=keep make e2e status=keep deploytool=helm globalnet=true To clean up the clusters deployed with e2e status=keep, use:\nmake e2e status=clean submariner-io/submariner Building Engine, Routeagent, and Globalnet Go binaries To build the submariner-route-agent, submariner-engine, and submariner-globalnet Go binaries, in the submariner-io/submariner repository:\nmake build There is an optional flag to build with debug flags set:\nmake build --build_debug=true Building Engine, Routeagent, and Globalnet container images To build the submariner/submariner, submariner/submariner-route-agent, and submariner/submariner-globalnet container images, in the submariner-io/submariner repository:\nmake package submariner-io/submariner-operator Building the Operator and subctl To build the submariner-operator container image and the subctl Go binary, in the submariner-io/submariner-operator repository:\nmake build submariner-io/lighthouse Building Lighthouse Controller, CoreDNS and DNSServer container images To build the lighthouse-controller, lighthouse-coredns, and lighthouse-dnsserver contaienr images, in the submariner-io/lighthouse repository:\nmake build-controller build-coredns build-dnsserver submariner-io/shipyard Building dapper-base container image To build the base container image used in the shared developer and CI enviroment, in the submariner-io/shipyard:\nmake dapper-image "
},
{
	"uri": "/en/contributing/release-process/",
	"title": "Release process",
	"tags": [],
	"description": "",
	"content": "This section explains the necessary steps to make a submariner release. It is assumed that you are familiar with the submariner project and the various repositories.\nStep 1: create a submariner release Assuming that you have an existing submariner git directory, the following steps create a release named \u0026ldquo;Globalnet Overlapping IP support RC0\u0026rdquo; with version v0.2.0-rc0 based on the master branch.\ncd submariner git stash git remote add upstream ssh://git@github.com/submariner-io/submariner git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner?tab=tags\n Step 2: create a lighthouse release Assuming that you have an existing lighthouse git directory, run the following steps .\ncd lighthouse git stash git remote add upstream ssh://git@github.com/submariner-io/lighthouse git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here\n https://github.com/submariner-io/lighthouse/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here\n https://travis-ci.com/github/submariner-io/lighthouse/branches\n For this example the build can be found here.\nVerify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available on quay.io at:\n https://quay.io/repository/submariner/lighthouse-controller?tab=tags https://quay.io/repository/submariner/lighthouse-coredns?tab=tags\n Step 3: update the operator version references and create a release Once the other builds have finished and you have 0.2.0-rc0 release tags for the submariner and lighthouse projects, you can proceed with changes to the operator.\nChange referenced versions Edit the operator versions file and change the project version constants to reference the new release, \u0026ldquo;0.2.0-rc0\u0026rdquo;.\n https://github.com/submariner-io/submariner-operator/edit/master/pkg/versions/versions.go\n Create a pull request, wait for the CI job to pass, and get approval/merge. See an example PR here\nCreate a submariner-operator release Assuming you have an existing submariner-operator git directory, run the following steps:\ncd submariner-operator git stash git remote add upstream ssh://git@github.com/submariner-io/submariner-operator git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner-operator/tags\n A build for v0.2.0-rc0 should start and appear under the under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner-operator/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner-operator?tab=tags\n Create the subctl binaries release git stash cd submariner-operator git fetch -a -v -t upstream git checkout v0.2.0-rc0 rm bin/subctl* make build-cross ls -la bin/subctl* At this point, you should see subctl binaries generated and listed for the various platforms under bin. Go to https://github.com/submariner-io/submariner-operator/tags, find the tag for v0.2.0-rc0 and select \u0026ldquo;Edit release\u0026rdquo; to the right. Then upload the generated subctl binaries.\nIf this is a pre-release, mark the checkbox \u0026ldquo;This is a pre-release\u0026rdquo;.\nVerify the version You can follow any of our quickstarts, for example this\nAnnounce email to:\n bit.ly/submariner-dev bit.ly/submariner-users  twitter under:\n twitter.com/submarinerio  "
},
{
	"uri": "/en/architecture/components/lighthouse/",
	"title": "Lighthouse",
	"tags": [],
	"description": "",
	"content": "Lighthouse provides DNS discovery for Kubernetes clusters connected by Submariner in multi-cluster environments. The solution is compatible with any CNI (Container Network Interfaces) plugin.\nArchitecture The below digram shows the basic Lighthouse architecture.\nLighthouse Controller This is the central discovery controller that gathers the information from the clusters, decides what information is to be shared and distributes the information as newly defined CRDs (Kubernetes custom resources).\nThe Lighthouse controller uses kubefed to discover clusters and to extract and distribute aggregated Service information used to perform DNS resolution.\nWorkFlow The workflow is as follows.\n Lighthouse controller registers to be notified when clusters join and unjoin the kubefed control plane. When notified about a join event, it retrieves the credentials from kubefed and registers a watch for Service creation and removal on that cluster. When notified of a new Service created, it creates a MultiClusterService resource with the Service info and distributes it to all the clusters that are connected to the kubefed control plane. When notified of a Service deleted, its info is removed from the MultiClusterService resource and re-distributed.  Lighthouse Plugin Lighthouse plugin can be installed as an external plugin for CoreDNS, and will work along with the default Kubernetes plugin. It uses the MultiClusterService resources that are distributed by the controller for DNS resolution. The below diagram indicates a high-level architecture.\nWorkFlow The workflow is as follows.\n A pod tries to resolve a Service name. The Kubernetes plugin in CoreDNS will first try to resolve the request. If no records are present the request will be sent to Lighthouse plugin. The Lighthouse plugin will use its MultiClusterService cache to try to resolve the request. If a record exists it will be returned, otherwise the plugin will pass the request to the next plugin registered in CoreDNS.  "
},
{
	"uri": "/en/deployment/subctl/",
	"title": "subctl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "The inner details of deploying a broker and connecting clusters to the broker are complicated, subctl automates and simplifies most of those details eliminating human error as much as possible. This is why subctl is the recommended deployment method, you can find a complete guide to the subctl tool here: subctl in detail. If you still believe helm works better for you, please go here.\nInstalling subctl Download the subctl binary and make it available on your PATH.\nmkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.0/subctl-v0.1.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile Deployment of broker Please remember, this cluster\u0026rsquo;s API should be accessible from all other clusters:\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; this will create:\n The submariner-k8s-broker namespace The Cluster and Endpoint (.submariner.io) CRDs in the cluster A service account in the namespace for subsequent subctl access. A random IPSEC PSK which will be stored only in the broker-info.subm file.  And generate a broker-info.subm file which contains the following elements:\n The API endpoint A CA certificate to for the API endpoint The service account token for accessing the API endpoint / submariner-k8s-broker namespace.  This cluster can also participate in the dataplane connectivity with the other clusters, but it will need to be joined (see following step)\n Joining clusters For each cluster you want to join:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm subctl will discover as much as it can, and ask you for any necessary detail it can\u0026rsquo;t figure out like the cluster ID which has to be different between all clusters.\nDiscovery Service discovery (via DNS and lighthouse project) is an experimental feature (developer preview), and the instructions to deploy with discovery can be found here\n"
},
{
	"uri": "/en/architecture/components/",
	"title": "Submariner components",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/contributing/developers/",
	"title": "Developers",
	"tags": [],
	"description": "",
	"content": "Submariner strives to be an open, welcoming community for developers. Substantial tooling is provided to ease the contribution experience.\n"
},
{
	"uri": "/en/quickstart/openshiftsd/",
	"title": "OpenShift with Service Discovery (AWS)",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  Please note that the prep_for_subm.sh script has a few pre-requirements, you will need to install: oc, aws-cli, terraform, and unzip.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.1/subctl-v0.1.1-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install kubefedctl Download the kubefedctl binary and make it available on your PATH.\nVERSION=0.1.0-rc3 OS=linux ARCH=amd64 curl -LO https://github.com/kubernetes-sigs/kubefed/releases/download/v${VERSION}/kubefedctl-${VERSION}-${OS}-${ARCH}.tgz tar -zxvf kubefedctl-*.tgz chmod u+x kubefedctl mv kubefedctl ~/.local/bin/ Install Submariner with Service Discovery The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery follow the steps below.\nCreate a merged kubeconfig export KUBECONFIG=$PWD/cluster-a/auth/kubeconfig:$PWD/cluster-b/auth/kubeconfig sed -i \u0026#39;s/admin/east/\u0026#39; cluster-a/auth/kubeconfig sed -i \u0026#39;s/admin/west/\u0026#39; cluster-b/auth/kubeconfig kubectl config view --flatten \u0026gt; ./merged_kubeconfig Use cluster-a as broker with service discovery enabled subctl deploy-broker --kubecontext west --kubeconfig ./merged_kubeconfig --service-discovery Join cluster-a and cluster-b to the broker subctl join --kubecontext west --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid west --broker-cluster-context west subctl join --kubecontext east --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid east --broker-cluster-context west Verify Deployment To verify the deployment follow the steps below.\nexport KUBECONFIG=./merged_kubeconfig kubectl --context east create deployment ngnix --image=nginx kubectl --context east expose deployment nginx --port=80 kubectl --context west run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx "
},
{
	"uri": "/en/quickstart/openshiftgn/",
	"title": "OpenShift with Service Discovery and Globalnet (AWS)",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create cluster A This step will create a cluster named \u0026ldquo;cluster-a\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a This may take some time to complete so you can move on to the next section in parallel if you wish.\nCreate cluster B This step will create a cluster named \u0026ldquo;cluster-b\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-b And finally deploy\nopenshift-install create cluster --dir cluster-b Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensure that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes which is necessary on at least one end of the IPSEC connections.\nTo handle these requirements, a script is provided that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  Please note that the prep_for_subm.sh script requires that you first have the following installed: oc, aws-cli, terraform, and unzip.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.1/subctl-v0.1.1-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery and Globalnet The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery and support for overlapping CIDRs follow the steps below.\nCreate a merged kubeconfig export KUBECONFIG=$PWD/cluster-a/auth/kubeconfig:$PWD/cluster-b/auth/kubeconfig sed -i \u0026#39;s/admin/east/\u0026#39; cluster-a/auth/kubeconfig sed -i \u0026#39;s/admin/west/\u0026#39; cluster-b/auth/kubeconfig kubectl config view --flatten \u0026gt; ./merged_kubeconfig Use cluster-a as broker with service discovery and globalnet enabled subctl deploy-broker --kubecontext west --kubeconfig ./merged_kubeconfig --service-discovery --globalnet Join cluster-a and cluster-b to the broker subctl join --kubecontext west --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid west --broker-cluster-context west subctl join --kubecontext east --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid east --broker-cluster-context west Verify Deployment To verify the deployment follow the steps below.\nkubectl --context east create deployment ngnix --image=nginx kubectl --context east expose deployment nginx --port=80 kubectl --context west run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx "
},
{
	"uri": "/en/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.2.0 Overlapping CIDR support  This release is focused on overlapping CIDR support between clusters\n  Support for Overlapping CIDRs between clusters (globalnet) Enhanced e2e scripts, which will be shared between repositories in the shipyard project (ongoing work) Improved e2e deployment by using a local registry. Refactoring to support pluggable drivers (in preparation for WireGuard support)  v0.1.1 Submariner with more light  This release has focused on stability for the Lighthouse support\n  Cleaner logging for the submariner-engine Cleaner logging for the submariner-route-agent Fixed issue with wrong token stored in subm file #244 Added flag to disable the OpenShift CVO #235 Fixed several service-discovery related bugs #194 , #167 Fixed several panics on nil network discovery Added checks to ensure the CIDRs for joining cluster don\u0026rsquo;t overlap with an existing ones. Fix context handling related to service-discovery / kubefed #180 Use the right CoreDNS image for OpenShift.  v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making https://github.com/submariner.io/lighthouse available as developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (#346, #348, #332) Migrated to Daemonsets for submariner gateway deployment Added support for hostNetwork to remote pod/service connectivity (#288) Auto detection and configuration of MTU for vx-submariner, jumbo frames support (#301) Support for updated strongswan (#288) Better iptables detection for some hosts (#227)   subctl and the submariner operator have the following improvements\n  support to verify-connectivity between two connected clusters deployment of submariner gateways based in daemonsets instead of deployments renaming submariner pods to \u0026ldquo;submariner-gateway\u0026rdquo; pods for clarity print version details on crash (subctl) stop storing IPSEC key on broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file version command for subctl nicer spinners during deployment (thanks to kind)  v0.0.3 \u0026ndash; KubeCon NA 2019  Submariner has been greatly enhanced to allow operators to deploy into Kubernetes clusters without the necessity for layer-2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). Subctl was created to make deployment of submariner easier.\n v0.0.1 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/en/reading_material/",
	"title": "Online resources",
	"tags": [],
	"description": "",
	"content": "There are multiple presentations/demo recordings on Submariner available online.\nConference presentations  KubeCon China 2019 slides and video recording KubeCon North America 2019 slides and video recording  Demo recordings  Deploying Submariner with subctl Connecting hybrid Kubernetes clusters using Submariner Cross-cluster service discovery in Submariner using Lighthouse  Blogs  Kubernetes Multi-Cloud and Multi-Cluster Connectivity with Submariner  If you find additional material that isn\u0026rsquo;t listed here, please feel free to add it to this page. The contributing guide can be found at here.\n"
},
{
	"uri": "/en/contributing/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": "Submariner Community Code of Conduct Submariner follows the CNCF Code of Conduct.\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the Submariner Committers.\n"
},
{
	"uri": "/en/deployment/helm/",
	"title": "helm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/quickstart/kind/",
	"title": "KIND",
	"tags": [],
	"description": "",
	"content": "KIND KIND is a tool to run local Kubernetes clusters inside Docker container nodes.\nSubmariner provides scripts that deploy 3 Kubernetes clusters locally, 1 broker and 2 data clusters with the Submariner dataplane components deployed on all the clusters.\nPlease note that docker must be installed and running on your computer. To deploy the setup, clone submariner repo and run\nmake ci e2e status=keep\nThis will deploy 3 Kubernetes clusters and run basic end-to-end tests. The status=keep option retains the cluster setup after the tests complete.\nMore details can be found here.\n"
},
{
	"uri": "/en/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "This is a preliminary community roadmap, it\u0026rsquo;s not written in stone, but it can serve as a guideline about what\u0026rsquo;s ahead.\nPlease see details of previous releases here\nv0.1.1  Stabilization of new lighthouse related features and deployment Bugfixes Working with the OpenShift CoreDNS distribution to make lighthouse compiled in their container  v0.2.0  Support for Overlapping CIDRs Improve documentation and website (internal) start using Armada to deploy multiple clusters in CI Support for multiple cable engines, including wireguard  v0.3.0  Auto detecting NAT vs non-NAT scenarios. Supporting different ports for IPSEC for each cluster Removing the kubefed dependency from Lighthouse service discovery Measuring and improving A/P HA (different scenarios) Libreswan support  v0.4.0  Support for network policies via coastguard Monitoring and reporting of tunnel endpoints (status of connection, bandwidth, pps, etc..) Monitoring connectivity over port 4800 between routeagent nodes. Support for non-kubeproxy / iptables based implementations, starting with OVN  v0.5.0  HA Active/Active gateway support (ECMP?) (keep in mind non-iptables-kubeproxy based implementations) Testing with Istio  "
},
{
	"uri": "/en/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "Getting in touch with the community  Join the developer mailing list. Join our weekly meetings and feel free to propose topics for discussion.  Pick up an issue to work on Or propose an enhancement via github issues Proposing a pull request Once your code is ready to be reviewed, you can propose a pull request.\nYou can find a good guide about the github workflow here: contributing to a github project\n"
},
{
	"uri": "/en/",
	"title": "Submariner",
	"tags": [],
	"description": "",
	"content": "Cross-Cluster Network for Kubernetes Submariner Submariner enables direct networking between pods and services in different Kubernetes clusters, on premises or in the cloud.\nSubmariner is completely opensource, and it\u0026rsquo;s designed to be network-plugin agnostic and works with most plugins based on kube-proxy (see compatibility matrix).\nSubmariner routes L3 traffic in the kernel, no traffic is handled at user level, and inter-cluster traffic is encrypted with IPSEC, although more options are being added.\n ** add simpler diagram here **\nJoining a cluster to an existing submariner broker is as simple as running:\nsubctl join --kubeconfig /path/to/your/config broker-info.subm\nCreating a broker is as simple as running:\nsubctl deploy-broker --kubeconfig /path/to/your/config\nSee the deployment section for more detailed deployment instructions.\n "
},
{
	"uri": "/en/quickstart/openshift/",
	"title": "OpenShift (AWS)",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  Please note that the prep_for_subm.sh script has a few pre-requirements, you will need to install: oc, aws-cli, terraform, and unzip.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.1/subctl-v0.1.1-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Use cluster-a as broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster pods and services\nsubctl verify-connectivity cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/en/deployment/with-discovery/",
	"title": "With Discovery (experimental)",
	"tags": [],
	"description": "",
	"content": "Deployment with discovery will include the (lighthouse components.\nProject status: The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster may disable some of the operator features.\n The Lighthouse project helps in cross-cluster service discovery. It has the below additional dependencies\n kubefedctl installed (0.1.0-rc3). kubectl installed.  Deploying Submariner with Lighthouse Deploy Broker subctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; --service-discovery --broker-cluster-context \u0026lt;BROKER-CONTEXT-NAME\u0026gt; kubefed will be installed in the broker cluster, as lighthouse currently depends on it for resource distribution. This dependency will be eliminated in the future.\nJoin Clusters To join all the other clusters with the broker cluster, run subctl using the broker-info.subm generated in the folder from which the previous step was run.\nYou will need a kubeconfig file with multiple contexts, a default admin context pointing to the cluster you\u0026rsquo;re trying to join, and another context which provides access to the broker cluster from the previous step. This is a requirement from kubefedctl.\n subctl join --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-DATA-CLUSTER\u0026gt; broker-info.subm --broker-cluster-context \u0026lt;BROKER-CONTEXT-NAME\u0026gt; As for a normal deployment, subctl will try to figure out all necessary information and will ask for anything it can\u0026rsquo;t figure out.\n"
},
{
	"uri": "/en/architecture/components/broker/",
	"title": "Broker",
	"tags": [],
	"description": "",
	"content": "Submariner uses a central broker to facilitate the exchange of metadata information between connected clusters. The broker is basically a set of custom resource definitions (CRDs) backed by the kubernetes datastore. The broker is a singleton component that is deployed on one of the clusters whose Kubernetes API must be accessible by all of the connected clusters.\n"
},
{
	"uri": "/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/architecture/components/gateway-engine/",
	"title": "Gateway Engine",
	"tags": [],
	"description": "",
	"content": "The gateway engine component is deployed in each connected cluster and is responsible for establishing IPsec tunnels to other clusters. Instances of the gateway engine run on specifically designated nodes in a cluster of which there may be more than one for fault tolerance. There is only one active gateway engine instance in a cluster - the others await in standby mode ready to take over should the active instance fail. The active gateway engine communicates with the central broker to advertise its endpoint to the other connected clusters and to learn about the gateway endpoints on the other clusters.\n"
},
{
	"uri": "/en/architecture/components/route-agent/",
	"title": "Route Agent",
	"tags": [],
	"description": "",
	"content": "The Route Agent runs on every node in each connected cluster. It is responsible for setting up VxLAN tunnels and routing the cross cluster traffic from the node to the cluster’s active Gateway Engine which subsequently sends the traffic to the destination cluster.\n"
},
{
	"uri": "/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]