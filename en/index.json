[
{
	"uri": "/en/contributing/website/",
	"title": "Contributing to the website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on hugo, grav, and the hugo-learn-theme and written in markdown format.\nIf you want to contribute I recommend reading the hugo-learn-theme documentation\nYou can always click the \u0026ldquo;Edit this page link\u0026rdquo; at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on github\n  Checkout your copy locally\n  $ git clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git $ cd submariner-website $ make server  Open your browser on http://localhost:1313\n  Edit files in src, the browser should automatically reload changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the github workflow here: contributing to a github project\n  "
},
{
	"uri": "/en/architecture/globalnet/",
	"title": "Globalnet Controller",
	"tags": [],
	"description": "",
	"content": "Introduction Submariner is a tool built to connect overlay networks of different Kubernetes clusters. These clusters can be on different public clouds or on-premise. An important use case for Submariner is to connect disparate independent clusters into a single cohesive multi-cluster fleet.\nHowever, by default, a limitation of submariner is that it doesn\u0026rsquo;t handle overlapping CIDRs (ServiceCIDR and ClusterCIDR) across these clusters. Each cluster must use distinct CIDRs that don\u0026rsquo;t conflict or overlap with any other cluster that is going to be part of cluster fleet.\nThis is largely problematic because most actual deployments use the default CIDRs for a cluster so every cluster ends up using the same CIDRs. Changing CIDRs on existing clusters is a very disruptive process and requires a cluster restart. So submariner needs a way to allow clusters with overlapping CIDRs to connect together.\nArchitecture To support overlapping CIDRs in clusters connected through submariner, submariner has a component called Global Private Network, GlobalNet (globalnet). This GlobalNet is a virtual network specifically to support submariner\u0026rsquo;s multi-cluster solution with a Global CIDR. Each cluster is given a subnet from this Global Private Network, configured as new cluster parameter GlobalCIDR (e.g. 169.254.0.0/16) which is configurable at time of deployment\nOnce configured, each service and pod that requires cross-cluster access is allocated an IP, named GlobalIp, from this GlobalCIDR that is annotated on the Pod/Service object. This GlobalIp is used for all cross-cluster communication to and from to this Pod/Service. Routing and IPTable/OVS/OVN rules are configured to use this IP for ingress and egress. All address translations occur on the Gateway node.\nsubmariner-globalnet Submariner GlobalNet is a component that provides cross-cluster connectivity between pods and services using their GlobalIps. Compiled as binary submariner-globalnet, it is responsible for maintaining a pool of global IPs, allocating IPs from the globalIp pool to pods and services, annotating services and pods with their GlobalIp, and configuring the required rules on the gateway node to provide cross-cluster connectivity using GlobalIps. It mainly consists of two key components: the IP Address Manager and Globalnet.\nIP Address Manager (IPAM) The IP Address Manager (IPAM) component does the following:\n Creates a pool of IP addresses based on the GlobalCIDR configured on cluster. On creation of a Pod/Service, allocates a GlobalIp from the GlobalIp pool. Annotates the Pod/Service with submariner.io/GlobalIp=\u0026lt;global-ip\u0026gt;. On deletion of a Pod/Service, releases its GlobalIp back to the pool.  Globalnet This component is responsible for programming the routing entries and IPTable rules and does the following:\n Creates initial IPTables chains for Globalnet rules. Whenever a pod is annotated with Global IP, creates an egress SNAT rule to convert source ip from PodIp to pod\u0026rsquo;s GlobalIp on the Gateway Node. Whenever a service is annotated with Global IP, creates an ingress rule to direct all traffic destined to service\u0026rsquo;s GlobalIp to the service\u0026rsquo;s kube-proxy IPTables chain which in turn directs traffic to service\u0026rsquo;s backend pods. On deletion of pod/service, clean up the rules from the gateway node.  Globalnet currently relies on kube-proxy and thus will only work with deployments that use kube-proxy.\nService Discovery - Lighthouse Connectivity is only part of the solution as pods still need to know the IPs of services on remote clusters.\nThis is achieved by enhancing lighthouse with support for GlobalNet. The Lighthouse controller adds the service\u0026rsquo;s GlobalIp to the MultiClusterService object that is distributed to all clusters. The lighthouse plugin then uses the service\u0026rsquo;s GlobalIp when replying to DNS queries for the service.\nBuilding Nothing extra needs to be done to build submariner-globalnet as it is built with the standard submariner build.\n"
},
{
	"uri": "/en/architecture/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Lorem Ipsum.\n"
},
{
	"uri": "/en/quickstart/",
	"title": "Quickstart guide",
	"tags": [],
	"description": "",
	"content": "Basic overview Submariner has two main core pieces (the broker and submariner), more information about this topic can be found in the Architecture section.\nThe broker The broker is an API to which all participating clusters are given access and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a Cluster, and the reachable cluster IPs from the endpoint.  The broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe submariner deployment on a cluster Once submariner is deployed on a cluster with the proper credentials to the broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n  At least 2 Kubernetes clusters, one of which is designated to serve as the central broker that is accessible by all of your connected clusters; this can be one of your connected clusters, but comes with the limitation that the cluster is required to be up to facilitate interconnectivity/negotiation.\n  Different service/pod CIDRs between clusters. This is to prevent routing conflicts.\n    Direct IP connectivity between the gateway nodes through the internet (or on the same network if not running Submariner over the internet). Submariner supports 1:1 NAT setups but has a few caveats/provider-specific configuration instructions in this configuration.  Knowledge of each cluster\u0026rsquo;s network configuration.\n  Worker node IPs on all the clusters must be outside of the cluster/service CIDR ranges.\n  An example of three clusters configured to use with Submariner would look like the following:\n   Cluster Name Provider Pods CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east OnPrem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Deployment The available methods for deployment are:\n subctl (+ submariner-operator). helm charts.  The community recommends the use of subctl, because it simplifies most of the manual steps required for deployment, as well as verification of connectivity between the clusters. In the future it may provide additional capabilities like:\n Detection of possible conflicts Upgrade management Status inspection of the deployment Configuration updates Maintenance and debugs tasks Wrapping of logs for support tasks.  To deploy submariner with subctl please follow the deployment guide. If helm fits better your deployment methodologies, please find the details here\n"
},
{
	"uri": "/en/contributing/building_testing/",
	"title": "Building and Testing",
	"tags": [],
	"description": "",
	"content": "Submariner strives to be an open, welcoming community for developers. Substantial tooling is provided to ease the contribution experience.\nStandard Development Environment Submariner provides a standard, shared development environment suitable for all local work. The same environment is used in CI.\nThe submariner-io/shipyard project contains the logic to build the base container images used across all submariner-io repositories.\nPrescribed Tasks via Make Targets Make targets are provided to further ease the process of using the shared development environment. The specific make targets available differ by repository. For any submariner-io repository, see the Makefile at the root of the repository for the supported targets and the .travis.yml file for the targets actually used in CI.\nCommon Build and Testing Targets All submariner-io/* repositories provide a standard set of Make targets for similar building and testing actions.\nLinting To run static Go linting (goimports, golangci-lint):\nmake validate Unit tests To run Go unit tests:\nmake test End-to-end tests To run functional tests with a full multicluster deployment:\nmake e2e The e2e target supports flags to configure the deployment and testing. For example, here are two e2e flag variations used in submariner-io/submariner CI:\nmake e2e status=keep make e2e status=keep deploytool=helm globalnet=true To clean up the clusters deployed with e2e status=keep, use:\nmake e2e status=clean submariner-io/submariner Building Engine, Routeagent, and Globalnet Go binaries To build the submariner-route-agent, submariner-engine, and submariner-globalnet Go binaries, in the submariner-io/submariner repository:\nmake build There is an optional flag to build with debug flags set:\nmake build --build_debug=true Building Engine, Routeagent, and Globalnet container images To build the submariner/submariner, submariner/submariner-route-agent, and submariner/submariner-globalnet container images, in the submariner-io/submariner repository:\nmake images submariner-io/submariner-operator Building the Operator and subctl To build the submariner-operator container image and the subctl Go binary, in the submariner-io/submariner-operator repository:\nmake build submariner-io/lighthouse Building Lighthouse Controller, CoreDNS and DNSServer container images To build the lighthouse-controller, lighthouse-coredns, and lighthouse-dnsserver contaienr images, in the submariner-io/lighthouse repository:\nmake build-controller build-coredns build-dnsserver submariner-io/shipyard Building dapper-base container image To build the base container image used in the shared developer and CI enviroment, in the submariner-io/shipyard:\nmake dapper-image "
},
{
	"uri": "/en/contributing/release-process/",
	"title": "Release process",
	"tags": [],
	"description": "",
	"content": "This section explains the necessary steps to make a submariner release. It is assumed that you are familiar with the submariner project and the various repositories.\nStep 1: create a submariner release Assuming that you have an existing submariner git directory, the following steps create a release named \u0026ldquo;Globalnet Overlapping IP support RC0\u0026rdquo; with version v0.2.0-rc0 based on the master branch.\ncd submariner git stash git remote add upstream ssh://git@github.com/submariner-io/submariner git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner?tab=tags\n Step 2: create a lighthouse release Assuming that you have an existing lighthouse git directory, run the following steps .\ncd lighthouse git stash git remote add upstream ssh://git@github.com/submariner-io/lighthouse git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here\n https://github.com/submariner-io/lighthouse/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here\n https://travis-ci.com/github/submariner-io/lighthouse/branches\n For this example the build can be found here.\nVerify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available on quay.io at:\n https://quay.io/repository/submariner/lighthouse-controller?tab=tags https://quay.io/repository/submariner/lighthouse-coredns?tab=tags\n Step 3: update the operator version references and create a release Once the other builds have finished and you have 0.2.0-rc0 release tags for the submariner and lighthouse projects, you can proceed with changes to the operator.\nChange referenced versions Edit the operator versions file and change the project version constants to reference the new release, \u0026ldquo;0.2.0-rc0\u0026rdquo;.\n https://github.com/submariner-io/submariner-operator/edit/master/pkg/versions/versions.go\n Create a pull request, wait for the CI job to pass, and get approval/merge. See an example PR here\nCreate a submariner-operator release Assuming you have an existing submariner-operator git directory, run the following steps:\ncd submariner-operator git stash git remote add upstream ssh://git@github.com/submariner-io/submariner-operator git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner-operator/tags\n A build for v0.2.0-rc0 should start and appear under the under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner-operator/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner-operator?tab=tags\n Create the subctl binaries release git stash cd submariner-operator git fetch -a -v -t upstream git checkout v0.2.0-rc0 rm bin/subctl* make build-cross ls -la bin/subctl* At this point, you should see subctl binaries generated and listed for the various platforms under bin. Go to https://github.com/submariner-io/submariner-operator/tags, find the tag for v0.2.0-rc0 and select \u0026ldquo;Edit release\u0026rdquo; to the right. Then upload the generated subctl binaries.\nIf this is a pre-release, mark the checkbox \u0026ldquo;This is a pre-release\u0026rdquo;.\nVerify the version You can follow any of our quickstarts, for example this\nAnnounce email to:\n bit.ly/submariner-dev bit.ly/submariner-users  twitter under:\n twitter.com/submarinerio  "
},
{
	"uri": "/en/architecture/components/lighthouse/",
	"title": "Lighthouse",
	"tags": [],
	"description": "",
	"content": "Lighthouse provides DNS discovery for Kubernetes clusters connected by Submariner in multi-cluster environments. The solution is compatible with any CNI (Container Network Interfaces) plugin.\nArchitecture The below digram shows the basic Lighthouse architecture.\nLighthouse Controller This is the central discovery controller that gathers the information from the clusters, decides what information is to be shared and distributes the information as newly defined CRDs (Kubernetes custom resources).\nThe Lighthouse controller uses kubefed to discover clusters and to extract and distribute aggregated Service information used to perform DNS resolution.\nWorkFlow The workflow is as follows.\n Lighthouse controller registers to be notified when clusters join and unjoin the kubefed control plane. When notified about a join event, it retrieves the credentials from kubefed and registers a watch for Service creation and removal on that cluster. When notified of a new Service created, it creates a MultiClusterService resource with the Service info and distributes it to all the clusters that are connected to the kubefed control plane. When notified of a Service deleted, its info is removed from the MultiClusterService resource and re-distributed.  Lighthouse Plugin Lighthouse plugin can be installed as an external plugin for CoreDNS, and will work along with the default Kubernetes plugin. It uses the MultiClusterService resources that are distributed by the controller for DNS resolution. The below diagram indicates a high-level architecture.\nWorkFlow The workflow is as follows.\n A pod tries to resolve a Service name. The Kubernetes plugin in CoreDNS will first try to resolve the request. If no records are present the request will be sent to Lighthouse plugin. The Lighthouse plugin will use its MultiClusterService cache to try to resolve the request. If a record exists it will be returned, otherwise the plugin will pass the request to the next plugin registered in CoreDNS.  "
},
{
	"uri": "/en/quickstart/openshift/",
	"title": "OpenShift (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks {{% notice info %}}\nPlease note that the prep_for_subm.sh script has a few pre-requirements, you will need to install: oc, aws-cli, terraform, and unzip.\n{{% /notice %}}\nInstall subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.1/subctl-v0.1.1-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Use cluster-a as broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster pods and services\nsubctl verify-connectivity cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/en/deployment/subctl/",
	"title": "subctl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "The inner details of deploying a broker and connecting clusters to the broker are complicated, subctl automates and simplifies most of those details eliminating human error as much as possible. This is why subctl is the recommended deployment method, you can find a complete guide to the subctl tool here: subctl in detail. If you still believe helm works better for you, please go here.\nInstalling subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.1/subctl-v0.1.1-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Deployment of broker Please remember, this cluster\u0026rsquo;s API should be accessible from all other clusters:\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; this will create:\n The submariner-k8s-broker namespace The Cluster and Endpoint (.submariner.io) CRDs in the cluster A service account in the namespace for subsequent subctl access. A random IPSEC PSK which will be stored only in the broker-info.subm file.  And generate a broker-info.subm file which contains the following elements:\n The API endpoint A CA certificate to for the API endpoint The service account token for accessing the API endpoint / submariner-k8s-broker namespace.  This cluster can also participate in the dataplane connectivity with the other clusters, but it will need to be joined (see following step)\n Joining clusters For each cluster you want to join:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm subctl will discover as much as it can, and ask you for any necessary detail it can\u0026rsquo;t figure out like the cluster ID which has to be different between all clusters.\nDiscovery Service discovery (via DNS and lighthouse project) is an experimental feature (developer preview), and the instructions to deploy with discovery can be found here\n"
},
{
	"uri": "/en/architecture/components/",
	"title": "Submariner components",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/quickstart/openshiftsd/",
	"title": "OpenShift with Service Discovery (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks {{% notice info %}}\nPlease note that the prep_for_subm.sh script has a few pre-requirements, you will need to install: oc, aws-cli, terraform, and unzip.\n{{% /notice %}}\nInstall subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.1/subctl-v0.1.1-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install kubefedctl Download the kubefedctl binary and make it available on your PATH.\nVERSION=0.1.0-rc3 OS=linux ARCH=amd64 curl -LO https://github.com/kubernetes-sigs/kubefed/releases/download/v${VERSION}/kubefedctl-${VERSION}-${OS}-${ARCH}.tgz tar -zxvf kubefedctl-*.tgz chmod u+x kubefedctl mv kubefedctl ~/.local/bin/ Install Submariner with Service Discovery The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery follow the steps below.\nCreate a merged kubeconfig export KUBECONFIG=$PWD/cluster-a/auth/kubeconfig:$PWD/cluster-b/auth/kubeconfig sed -i \u0026#39;s/admin/east/\u0026#39; cluster-a/auth/kubeconfig sed -i \u0026#39;s/admin/west/\u0026#39; cluster-b/auth/kubeconfig kubectl config view --flatten \u0026gt; ./merged_kubeconfig Use cluster-a as broker with service discovery enabled subctl deploy-broker --kubecontext west --kubeconfig ./merged_kubeconfig --service-discovery Join cluster-a and cluster-b to the broker subctl join --kubecontext west --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid west --broker-cluster-context west subctl join --kubecontext east --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid east --broker-cluster-context west Verify Deployment To verify the deployment follow the steps below.\nexport KUBECONFIG=./merged_kubeconfig kubectl --context east create deployment nginx --image=nginx kubectl --context east expose deployment nginx --port=80 kubectl --context west run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx "
},
{
	"uri": "/en/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.2.0 Overlapping CIDR support  This release is focused on overlapping CIDR support between clusters\n  Support for Overlapping CIDRs between clusters (globalnet) Enhanced e2e scripts, which will be shared between repositories in the shipyard project (ongoing work) Improved e2e deployment by using a local registry. Refactoring to support pluggable drivers (in preparation for WireGuard support)  v0.1.1 Submariner with more light  This release has focused on stability for the Lighthouse support\n  Cleaner logging for the submariner-engine Cleaner logging for the submariner-route-agent Fixed issue with wrong token stored in subm file #244 Added flag to disable the OpenShift CVO #235 Fixed several service-discovery related bugs #194 , #167 Fixed several panics on nil network discovery Added checks to ensure the CIDRs for joining cluster don\u0026rsquo;t overlap with an existing ones. Fix context handling related to service-discovery / kubefed #180 Use the right CoreDNS image for OpenShift.  v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making https://github.com/submariner.io/lighthouse available as developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (#346, #348, #332) Migrated to Daemonsets for submariner gateway deployment Added support for hostNetwork to remote pod/service connectivity (#288) Auto detection and configuration of MTU for vx-submariner, jumbo frames support (#301) Support for updated strongswan (#288) Better iptables detection for some hosts (#227)   subctl and the submariner operator have the following improvements\n  support to verify-connectivity between two connected clusters deployment of submariner gateways based in daemonsets instead of deployments renaming submariner pods to \u0026ldquo;submariner-gateway\u0026rdquo; pods for clarity print version details on crash (subctl) stop storing IPSEC key on broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file version command for subctl nicer spinners during deployment (thanks to kind)  v0.0.3 \u0026ndash; KubeCon NA 2019  Submariner has been greatly enhanced to allow operators to deploy into Kubernetes clusters without the necessity for layer-2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). Subctl was created to make deployment of submariner easier.\n v0.0.1 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/en/reading_material/",
	"title": "Online resources",
	"tags": [],
	"description": "",
	"content": "There are multiple presentations/demo recordings on Submariner available online.\nConference presentations  KubeCon China 2019 slides and video recording KubeCon North America 2019 slides and video recording  Demo recordings  Deploying Submariner with subctl Connecting hybrid Kubernetes clusters using Submariner Cross-cluster service discovery in Submariner using Lighthouse  Blogs  Kubernetes Multi-Cloud and Multi-Cluster Connectivity with Submariner  If you find additional material that isn\u0026rsquo;t listed here, please feel free to add it to this page. The contributing guide can be found here.\n"
},
{
	"uri": "/en/quickstart/openshiftgn/",
	"title": "OpenShift with Service Discovery and Globalnet (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create cluster A This step will create a cluster named \u0026ldquo;cluster-a\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a This may take some time to complete so you can move on to the next section in parallel if you wish.\nCreate cluster B This step will create a cluster named \u0026ldquo;cluster-b\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-b And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks {{% notice info %}}\nPlease note that the prep_for_subm.sh script has a few pre-requirements, you will need to install: oc, aws-cli, terraform, and unzip.\n{{% /notice %}}\nInstall subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.1/subctl-v0.1.1-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery and Globalnet The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery and support for overlapping CIDRs follow the steps below.\nCreate a merged kubeconfig export KUBECONFIG=$PWD/cluster-a/auth/kubeconfig:$PWD/cluster-b/auth/kubeconfig sed -i \u0026#39;s/admin/east/\u0026#39; cluster-a/auth/kubeconfig sed -i \u0026#39;s/admin/west/\u0026#39; cluster-b/auth/kubeconfig kubectl config view --flatten \u0026gt; ./merged_kubeconfig Use cluster-a as broker with service discovery and globalnet enabled subctl deploy-broker --kubecontext west --kubeconfig ./merged_kubeconfig --service-discovery --globalnet Join cluster-a and cluster-b to the broker subctl join --kubecontext west --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid west --broker-cluster-context west subctl join --kubecontext east --kubeconfig ./merged_kubeconfig broker-info.subm --clusterid east --broker-cluster-context west Verify Deployment To verify the deployment follow the steps below.\nexport KUBECONFIG=./merged_kubeconfig kubectl --context east create deployment nginx --image=nginx kubectl --context east expose deployment nginx --port=80 kubectl --context west run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx "
},
{
	"uri": "/en/contributing/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": "Submariner Community Code of Conduct Submariner follows the CNCF Code of Conduct.\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the Submariner Committers.\n"
},
{
	"uri": "/en/contributing/community-membership/",
	"title": "Community membership",
	"tags": [],
	"description": "",
	"content": "This is a stripped-down version of the Kubernetes Community Membership process.\nAlthough we aspire to follow the K8s process, some parts are not currently relevant to our structure or possible with our tooling:\n The SIG and subproject abstraction layers don\u0026rsquo;t apply to Submariner. Submariner is treated as a single project with file-based commit rights, not a \u0026ldquo;project\u0026rdquo; per repository. We hope to eventually move to K8s OWNERS and Prow, but until we do so we can\u0026rsquo;t support advanced role-based automation (reviewers vs approvers; PR workflow commands like /okay-to-test, /lgtm, /approved).   This doc outlines the various responsibilities of contributor roles in Submariner.\n   Role Responsibilities Requirements Defined by     Member Active contributor in the community Sponsored by 2 committers, multiple contributions to the project Submariner GitHub org member   Committer Approve contributions from other members History of review and authorship CODEOWNERS file entry   Owner Set direction and priorities for the project Demonstrated responsibility and excellent technical judgement for the project Submariner-owners GitHub team member    New contributors New contributors should be welcomed to the community by existing members, helped with PR workflow, and directed to relevant documentation and communication channels.\nEstablished community members Established community members are expected to demonstrate their adherence to the principles in this document, familiarity with project organization, roles, policies, procedures, conventions, etc., and technical and/or writing ability. Role-specific expectations, responsibilities, and requirements are enumerated below.\nMember Members are continuously active contributors in the community. They can have issues and PRs assigned to them and participate through GitHub teams. Members are expected to remain active contributors to the community.\nDefined by: Member of the Submariner GitHub organization.\nRequirements  Enabled two-factor authentication on their GitHub account Have made multiple contributions to the project or community. Contribution may include, but is not limited to:  Authoring or reviewing PRs on GitHub Filing or commenting on issues on GitHub Contributing to community discussions (e.g. meetings, Slack, email discussion forums, Stack Overflow)   Subscribed to submariner-dev@googlegroups.com Have read the contributor guide Actively contributing Sponsored by 2 committers. Note the following requirements for sponsors:  Sponsors must have close interactions with the prospective member - e.g. code/design/proposal review, coordinating on issues, etc. Sponsors must be committers in at least 1 CODEOWNERS file either in any repo in the Submariner org   Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Responsibilities and privileges  Responsive to issues and PRs assigned to them Responsive to mentions of teams they are members of Active owner of code they have contributed (unless ownership is explicitly transferred)  Code is well tested Tests consistently pass Addresses bugs or issues discovered after code is accepted   They can be assigned to issues and PRs, and people can ask members for reviews  Note: Members who frequently contribute code are expected to proactively perform code reviews and work towards becoming a committer.\nCommitters Committers are able to review code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles.\nUntil automation supports approvers vs reviewers: They also review for holistic acceptance of a contribution including: backwards / forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, etc.\nDefined by: Entry in an CODEOWNERS file in a repo owned by the Submariner project.\nCommitter status is scoped to a part of the codebase.\nRequirements The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Member for at least 3 months Primary reviewer for at least 5 PRs to the codebase Reviewed at least 20 substantial PRs to the codebase Knowledgeable about the codebase Sponsored by two committers or project owners  With no objections from other committers or project owners   May either self-nominate or be nominated by a committer/owner  Responsibilities and privileges The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Responsible for project quality control via code reviews  Focus on code quality and correctness, including testing and factoring Until automation supports approvers vs reviewers: Focus on holistic acceptance of contribution such as dependencies with other features, backwards / forwards compatibility, API and flag definitions, etc   Expected to be responsive to review requests as per community expectations Assigned PRs to review related to project of expertise Assigned test bugs related to project of expertise Granted \u0026ldquo;read access\u0026rdquo; to submariner repo May get a badge on PR and issue comments Demonstrate sound technical judgement Mentor contributors and reviewers  Project Owner Project owners are the technical authority for the Submariner project. They MUST have demonstrated both good judgement and responsibility towards the health the project. Project owners MUST set technical direction and make or approve design decisions for the project - either directly or through delegation of these responsibilities.\nDefined by: Member of the submariner-owners GitHub team.\nRequirements Unlike the roles outlined above, the owners of the project are typically limited to a relatively small group of decision makers and updated as fits the needs of the project.\nThe following apply to people who would be an owner:\n Deep understanding of the technical goals and direction of the project Deep understanding of the technical domain of the project Sustained contributions to design and direction by doing all of:  Authoring and reviewing proposals Initiating, contributing and resolving discussions (emails, GitHub issues, meetings) Identifying subtle or complex issues in designs and implementation PRs   Directly contributed to the project through implementation and / or review  Responsibilities and privileges The following apply to people who would be an owner:\n Make and approve technical design decisions for the project Set technical direction and priorities for the project Define milestones and releases Mentor and guide committers and contributors to the project Ensure continued health of project  Adequate test coverage to confidently release Tests are passing reliably (i.e. not flaky) and are fixed when they fail   Ensure a healthy process for discussion and decision making is in place Work with other project owners to maintain the project\u0026rsquo;s overall health and success holistically  "
},
{
	"uri": "/en/deployment/helm/",
	"title": "helm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/quickstart/kind/",
	"title": "KIND",
	"tags": [],
	"description": "",
	"content": "KIND KIND is a tool to run local Kubernetes clusters inside Docker container nodes.\nSubmariner provides scripts that deploy 3 Kubernetes clusters locally, 1 broker and 2 data clusters with the Submariner dataplane components deployed on all the clusters.\nPlease note that docker must be installed and running on your computer. To deploy the setup, clone submariner repo and run\nmake ci e2e status=keep\nThis will deploy 3 Kubernetes clusters and run basic end-to-end tests. The status=keep option retains the cluster setup after the tests complete.\nMore details can be found here.\n"
},
{
	"uri": "/en/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "This is a preliminary community roadmap, it\u0026rsquo;s not written in stone, but it can serve as a guideline about what\u0026rsquo;s ahead.\nPlease see details of previous releases here\nv0.2.0  Support for Overlapping CIDRs Improve documentation and website  Future releases  Removing the kubefed dependency from Lighthouse service discovery De-duplicate E2E scripts and go code, into shipyard. Libreswan support Support for multiple cable engines, including WireGuard Auto detecting NAT vs non-NAT scenarios. Supporting different ports for IPSEC for each cluster Measuring and improving A/P HA (different scenarios) Support for network policies via coastguard Monitoring and reporting of tunnel endpoints (status of connection, bandwidth, pps, etc..) Monitoring connectivity over port 4800 between routeagent nodes. Support for non-kubeproxy / iptables based implementations, starting with OVN HA Active/Active gateway support (ECMP?) (keep in mind non-iptables-kubeproxy based implementations) Testing with Istio  "
},
{
	"uri": "/en/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "Getting in touch with the community  Join the developer mailing list. Join our weekly meetings and feel free to propose topics for discussion.  Pick up an issue to work on Or propose an enhancement via github issues Proposing a pull request Once your code is ready to be reviewed, you can propose a pull request.\nYou can find a good guide about the github workflow here: contributing to a github project\n"
},
{
	"uri": "/en/",
	"title": "Submariner",
	"tags": [],
	"description": "",
	"content": "Submariner Submariner enables direct networking between pods and services in different Kubernetes clusters, either on premise or in the cloud. Why Submariner? As Kubernetes gains adoption, teams are finding they must deploy and manage multiple clusters to facilitate features like geo-redundancy, scale, and fault isolation for their applications. With Submariner, your applications and services can span multiple cloud providers, data centers, and regions.\nSubmariner is completely open source, and designed to be network plugin (CNI) agnostic.\nWhat Submariner Provides?  Cross-cluster L3 connectivity using encrypted VPN tunnels Service Discovery across clusters via Lighthouse subctl, a friendly deployment tool Support for interconnecting clusters with overlapping CIDRs  Check the Quickstart guide section for deployment instructions.\n "
},
{
	"uri": "/en/deployment/with-discovery/",
	"title": "With Discovery (experimental)",
	"tags": [],
	"description": "",
	"content": "Deployment with discovery will include the (lighthouse components.\nProject status: The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster may disable some of the operator features.\n The Lighthouse project helps in cross-cluster service discovery. It has the below additional dependencies\n kubefedctl installed (0.1.0-rc3). kubectl installed.  Deploying Submariner with Lighthouse Deploy Broker subctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; --service-discovery --broker-cluster-context \u0026lt;BROKER-CONTEXT-NAME\u0026gt; kubefed will be installed in the broker cluster, as lighthouse currently depends on it for resource distribution. This dependency will be eliminated in the future.\nJoin Clusters To join all the other clusters with the broker cluster, run subctl using the broker-info.subm generated in the folder from which the previous step was run.\nYou will need a kubeconfig file with multiple contexts, a default admin context pointing to the cluster you\u0026rsquo;re trying to join, and another context which provides access to the broker cluster from the previous step. This is a requirement from kubefedctl.\n subctl join --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-DATA-CLUSTER\u0026gt; broker-info.subm --broker-cluster-context \u0026lt;BROKER-CONTEXT-NAME\u0026gt; As for a normal deployment, subctl will try to figure out all necessary information and will ask for anything it can\u0026rsquo;t figure out.\n"
},
{
	"uri": "/en/quickstart/openshift/create_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b "
},
{
	"uri": "/en/quickstart/openshift/ready_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  Please note that the prep_for_subm.sh script has a few pre-requirements, you will need to install: oc, aws-cli, terraform, and unzip.\n "
},
{
	"uri": "/en/quickstart/openshift/setup_openshift/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n "
},
{
	"uri": "/en/architecture/components/broker/",
	"title": "Broker",
	"tags": [],
	"description": "",
	"content": "Submariner uses a central broker to facilitate the exchange of metadata information between connected clusters. The broker is basically a set of custom resource definitions (CRDs) backed by the kubernetes datastore. The broker is a singleton component that is deployed on one of the clusters whose Kubernetes API must be accessible by all of the connected clusters.\n"
},
{
	"uri": "/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/architecture/components/gateway-engine/",
	"title": "Gateway Engine",
	"tags": [],
	"description": "",
	"content": "The gateway engine component is deployed in each connected cluster and is responsible for establishing IPsec tunnels to other clusters. Instances of the gateway engine run on specifically designated nodes in a cluster of which there may be more than one for fault tolerance. There is only one active gateway engine instance in a cluster - the others await in standby mode ready to take over should the active instance fail. The active gateway engine communicates with the central broker to advertise its endpoint to the other connected clusters and to learn about the gateway endpoints on the other clusters.\n"
},
{
	"uri": "/en/architecture/components/route-agent/",
	"title": "Route Agent",
	"tags": [],
	"description": "",
	"content": "The Route Agent runs on every node in each connected cluster. It is responsible for setting up VxLAN tunnels and routing the cross cluster traffic from the node to the cluster’s active Gateway Engine which subsequently sends the traffic to the destination cluster.\n"
},
{
	"uri": "/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]