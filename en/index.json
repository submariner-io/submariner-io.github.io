[
{
	"uri": "/en/architecture/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Lorem Ipsum.\n"
},
{
	"uri": "/en/quickstart/",
	"title": "Quickstart guide",
	"tags": [],
	"description": "",
	"content": "Basic overview Submariner has two main core pieces (the broker and submariner), more information about this topic can be found in the Architecture section.\nThe broker The broker is an API to which all participating clusters are given access and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a Cluster, and the reachable cluster IPs from the endpoint.  The broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe submariner deployment on a cluster Once submariner is deployed on a cluster with the proper credentials to the broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n  At least 2 Kubernetes clusters, one of which is designated to serve as the central broker that is accessible by all of your connected clusters; this can be one of your connected clusters, but comes with the limitation that the cluster is required to be up to facilitate interconnectivity/negotiation.\n  Different service/pod CIDRs between clusters. This is to prevent routing conflicts.\n    Direct IP connectivity between the gateway nodes through the internet (or on the same network if not running Submariner over the internet). Submariner supports 1:1 NAT setups but has a few caveats/provider-specific configuration instructions in this configuration.  Knowledge of each cluster\u0026rsquo;s network configuration.\n  Worker node IPs on all the clusters must be outside of the cluster/service CIDR ranges.\n  An example of three clusters configured to use with Submariner would look like the following:\n   Cluster Name Provider Pods CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east OnPrem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Deployment The available methods for deployment are:\n subctl (+ submariner-operator). helm charts.  The community recommends the use of subctl, because it simplifies most of the manual steps required for deployment, as well as verification of connectivity between the clusters. In the future it may provide additional capabilities like:\n Detection of possible conflicts Upgrade management Status inspection of the deployment Configuration updates Maintenance and debugs tasks Wrapping of logs for support tasks.  To deploy submariner with subctl please follow the deployment guide. If helm fits better your deployment methodologies, please find the details here\n"
},
{
	"uri": "/en/contributing/website/",
	"title": "Contributing to the website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on hugo, grav, and the hugo-learn-theme and written in markdown format.\nIf you want to contribute I recommend reading the hugo-learn-theme documentation\nYou can always click the \u0026ldquo;Edit this page link\u0026rdquo; at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on github\n  Checkout your copy locally\n  $ git clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git $ cd submariner-website $ make server  Open your browser on http://localhost:1313\n  Edit files in src, the browser should automatically reload changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the github workflow here: contributing to a github project\n  "
},
{
	"uri": "/en/deployment/subctl/",
	"title": "subctl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "The inner details of deploying a broker and connecting clusters to the broker are complicated, subctl automates and simplifies most of those details eliminating human error as much as possible. This is why subctl is the recommended deployment method, you can find a complete guide to the subctl tool here: subctl in detail. If you still believe helm works better for you, please go here.\nInstalling subctl Download the subctl binary and make it available on your PATH.\nmkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.0/subctl-v0.1.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile Deployment of broker Please remember, this cluster\u0026rsquo;s API should be accessible from all other clusters:\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; this will create:\n The submariner-k8s-broker namespace The Cluster and Endpoint (.submariner.io) CRDs in the cluster A service account in the namespace for subsequent subctl access. A random IPSEC PSK which will be stored only in the broker-info.subm file.  And generate a broker-info.subm file which contains the following elements:\n The API endpoint A CA certificate to for the API endpoint The service account token for accessing the API endpoint / submariner-k8s-broker namespace.  This cluster can also participate in the dataplane connectivity with the other clusters, but it will need to be joined (see following step)\n Joining clusters For each cluster you want to join:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm subctl will discover as much as it can, and ask you for any necessary detail it can\u0026rsquo;t figure out like the cluster ID which has to be different between all clusters.\nDiscovery Service discovery (via DNS and lighthouse project) is an experimental feature (developer preview), and the instructions to deploy with discovery can be found here\n"
},
{
	"uri": "/en/architecture/components/",
	"title": "Submariner components",
	"tags": [],
	"description": "",
	"content": "Lorem Ipsum.\n"
},
{
	"uri": "/en/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making https://github.com/submariner.io/lighthouse available as developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (#346, #348, #332) Migrated to Daemonsets for submariner gateway deployment Added support for hostNetwork to remote pod/service connectivity (#288) Auto detection and configuration of MTU for vx-submariner, jumbo frames support (#301) Support for updated strongswan (#288) Better iptables detection for some hosts (#227)   subctl and the submariner operator have the following improvements\n  support to verify-connectivity between two connected clusters deployment of submariner gateways based in daemonsets instead of deployments renaming submariner pods to \u0026ldquo;submariner-gateway\u0026rdquo; pods for clarity print version details on crash (subctl) stop storing IPSEC key on broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file version command for subctl nicer spinners during deployment (thanks to kind)  v0.0.3 \u0026ndash; KubeCon NA 2019  Submariner has been greatly enhanced to allow operators to deploy into Kubernetes clusters without the necessity for layer-2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). Subctl was created to make deployment of submariner easier.\n v0.0.1 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/en/contributing/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/deployment/helm/",
	"title": "helm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "Getting in touch with the community  Join the developer mailing list. Join our weekly meetings and feel free to propose topics for discussion.  Pick up an issue to work on Or propose an enhancement via github issues Setting up a development environment Testing your code Proposing a pull request Once your code is ready to be reviewed, you can propose a pull request.\nYou can find a good guide about the github workflow here: contributing to a github project\n"
},
{
	"uri": "/en/quickstart/openshift/",
	"title": "OpenShift (AWS)",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  Please note that the prep_for_subm.sh script has a few pre-requirements, you will need to install: oc, aws-cli, terraform, and unzip.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.1.0/subctl-v0.1.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Use cluster-a as broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --cluster-id cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --cluster-id cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster pods and services\nsubctl verify-connectivity cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/en/",
	"title": "Submariner",
	"tags": [],
	"description": "",
	"content": "Cross-Cluster Network for Kubernetes Submariner Submariner enables direct networking between pods and services in different Kubernetes clusters, on premises or in the cloud.\nSubmariner is completely opensource, and it\u0026rsquo;s designed to be network-plugin agnostic and works with most plugins based on kube-proxy (see compatibility matrix).\nSubmariner routes L3 traffic in the kernel, no traffic is handled at user level, and inter-cluster traffic is encrypted with IPSEC, although more options are being added.\n ** add simpler diagram here **\nJoining a cluster to an existing submariner broker is as simple as running:\nsubctl join --kubeconfig /path/to/your/config broker-info.subm\nCreating a broker is as simple as running:\nsubctl deploy-broker --kubeconfig /path/to/your/config\nSee the deployment section for more detailed deployment instructions.\n "
},
{
	"uri": "/en/deployment/with-discovery/",
	"title": "With Discovery (experimental)",
	"tags": [],
	"description": "",
	"content": "Deployment with discovery will include the (lighthouse components.\nProject status: The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster may disable some of the operator features.\n The Lighthouse project helps in cross-cluster service discovery. It has the below additional dependencies\n kubefedctl installed (0.1.0-rc3). kubectl installed.  Deploying Submariner with Lighthouse Deploy Broker subctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; --service-discovery --broker-cluster-context \u0026lt;BROKER-CONTEXT-NAME\u0026gt; kubefed will be installed in the broker cluster, as lighthouse currently depends on it for resource distribution. This dependency will be eliminated in the future.\nJoin Clusters To join all the other clusters with the broker cluster, run subctl using the broker-info.subm generated in the folder from which the previous step was run.\nYou will need a kubeconfig file with multiple contexts, a default admin context pointing to the cluster you\u0026rsquo;re trying to join, and another context which provides access to the broker cluster from the previous step. This is a requirement from kubefedctl.\n subctl join --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-DATA-CLUSTER\u0026gt; broker-info.subm --broker-cluster-context \u0026lt;BROKER-CONTEXT-NAME\u0026gt; As for a normal deployment, subctl will try to figure out all necessary information and will ask for anything it can\u0026rsquo;t figure out.\n"
},
{
	"uri": "/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]