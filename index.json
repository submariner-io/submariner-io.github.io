[
{
	"uri": "/development/website/style_guide/",
	"title": "Docs Style Guide",
	"tags": [],
	"description": "",
	"content": "Documentation Style Guide This guide is meant to help keep our documentation consistent and ease the contribution and review process.\nSubmariner follows the Kubernetes Documentation Style Guide wherever relevant. This is a Submariner-specific extension of those practices.\nSubmariner.io Word List A list of Submariner-specific terms and words to be used consistently across the site.\n   Term Usage     Admiral The project name Admiral should always be capitalized.   Broker The design pattern component Broker should always be capitalized.   ClusterSet The Kubernetes object ClusterSet proposed in KEP1645 should always be CamelCase and formatted in code style.   Cluster set The words \u0026ldquo;cluster set\u0026rdquo; should be used as a term for a group of clusters, but not the proposed Kubernetes object.   Coastguard The project name Coastguard should always be capitalized.   Globalnet The feature name Globalnet is one word, and so should always be capitalized and should have a lowercase \u0026ldquo;n\u0026rdquo;.   iptables The application iptables consistently uses all-lowercase. Follow their convention, but avoid starting a sentence with \u0026ldquo;iptables\u0026rdquo;.   K8s The project nickname K8s should typically be expanded to \u0026ldquo;Kubernetes\u0026rdquo;.   kind The tool kind consistently uses all-lowercase. Follow their convention, but avoid starting a sentence with \u0026ldquo;kind\u0026rdquo;.   Lighthouse The project name Lighthouse should always be capitalized.   Operator The design pattern Operator should always be capitalized.   Shipyard The project name Shipyard should always be capitalized.   subctl The artifact subctl should not be capitalized and should be formatted in code style.   Submariner The project name Submariner should always be capitalized.    Pronunciation of \u0026ldquo;Submariner\u0026rdquo; Both the \u0026ldquo;Sub-mariner\u0026rdquo; (\u0026ldquo;Sub-MARE-en-er\u0026rdquo;, like the watch) and \u0026ldquo;Submarine-er\u0026rdquo; (\u0026ldquo;Sub-muh-REEN-er\u0026rdquo;, like the Navy job) pronunciations are okay.\nThe second option, \u0026ldquo;Submarine-er\u0026rdquo;, has historically been more common as Chris Kim (the initial creator) imagined the iconography of the project as related to submarine cables.\n"
},
{
	"uri": "/getting-started/quickstart/managed-kubernetes/gke/",
	"title": "Google (GKE)",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers deploying two Google Kubernetes Engine (GKE) clusters on Google Cloud Platform (GCP) and connecting them with Submariner and Service Discovery.\nThe guide assumes clusters have non-overlapping Pod and Service CIDRs. Globalnet can be used if overlapping CIDRs can\u0026rsquo;t be avoided.\n The guide assumes you have the gcloud binary installed and configured and a GCP account with billing enabled for the active project.\n Cluster Creation Create two identical Kubernetes clusters on GKE. For this guide, the following minimal configuration was used, however not everything is required (see the note part below).\ngcloud container clusters create \u0026#34;cluster-a\u0026#34; \\  --zone \u0026#34;europe-west3-a\u0026#34; \\  --enable-ip-alias \\  --cluster-ipv4-cidr \u0026#34;10.0.0.0/14\u0026#34; \\  --services-ipv4-cidr=\u0026#34;10.4.0.0/20\u0026#34; \\  --cluster-version \u0026#34;1.17.13-gke.2001\u0026#34; \\  --username \u0026#34;admin\u0026#34; \\  --machine-type \u0026#34;g1-small\u0026#34; \\  --image-type \u0026#34;UBUNTU\u0026#34; \\  --disk-type \u0026#34;pd-ssd\u0026#34; \\  --disk-size \u0026#34;15\u0026#34; \\  --num-nodes \u0026#34;3\u0026#34; \\  --network \u0026#34;default\u0026#34; gcloud container clusters create \u0026#34;cluster-b\u0026#34; \\  --zone \u0026#34;europe-west3-a\u0026#34; \\  --enable-ip-alias \\  --cluster-ipv4-cidr \u0026#34;10.8.0.0/14\u0026#34; \\  --services-ipv4-cidr=\u0026#34;10.12.0.0/20\u0026#34; \\  --cluster-version \u0026#34;1.17.13-gke.2001\u0026#34; \\  --username \u0026#34;admin\u0026#34; \\  --machine-type \u0026#34;g1-small\u0026#34; \\  --image-type \u0026#34;UBUNTU\u0026#34; \\  --disk-type \u0026#34;pd-ssd\u0026#34; \\  --disk-size \u0026#34;15\u0026#34; \\  --num-nodes \u0026#34;3\u0026#34; \\  --network \u0026#34;default\u0026#34;  Make sure to use Kubernetes version 1.17 or higher, set by --cluster-version. The latest versions are listed in the GKE release notes.\n Prepare Clusters for Submariner The clusters need some changes in order for Submariner to successfully open the IPsec tunnel between them.\nPreparation: Node Configuration As of version 0.8 of Submariner (the current one while writing this), Google\u0026rsquo;s native CNI plugin is not directly supported. GKE clusters can be generated with Calico CNI instead, but this was not tested during this demo and therefore could hold surprises as well.\nSo as this guide uses Google\u0026rsquo;s native CNI plugin, configuration is needed for the eth0 interface of each node on every cluster. The used workaround deploys netshoot pods onto each node that configure the reverse path filtering. The scripts in this Github repository need to be executed in all clusters.\nwget https://raw.githubusercontent.com/sridhargaddam/k8sscripts/main/rp_filter_settings/update-rp-filter.sh wget https://raw.githubusercontent.com/sridhargaddam/k8sscripts/main/rp_filter_settings/configure-rp-filter.sh chmod +x update-rp-filter.sh chmod +x configure-rp-filter.sh gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; ./configure-rp-filter.sh gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; ./configure-rp-filter.sh Preparation: Firewall Configuration Submariner requires UDP ports 500, 4500, and 4800 to be open in both directions. Additionally the microservices\u0026rsquo; traffic needs to flow through the IPsec tunnel as TCP packets. Hence the TCP traffic has source and destination addresses originating in the participating clusters. Create those firewall rules on the GCP project. Use the same IP ranges as in the cluster creation steps above.\ngcloud compute firewall-rules create \u0026#34;allow-tcp-in\u0026#34; --allow=tcp \\  --direction=IN --source-ranges=10.12.0.0/20,10.8.0.0/14,10.4.0.0/20,10.0.0.0/14 gcloud compute firewall-rules create \u0026#34;allow-tcp-out\u0026#34; --allow=tcp --direction=OUT \\  --destination-ranges=10.12.0.0/20,10.8.0.0/14,10.4.0.0/20,10.0.0.0/14 gcloud compute firewall-rules create \u0026#34;udp-in-500\u0026#34; --allow=udp:500 --direction=IN gcloud compute firewall-rules create \u0026#34;udp-in-4500\u0026#34; --allow=udp:4500 --direction=IN gcloud compute firewall-rules create \u0026#34;udp-in-4800\u0026#34; --allow=udp:4800 --direction=IN gcloud compute firewall-rules create \u0026#34;udp-out-500\u0026#34; --allow=udp:500 --direction=OUT gcloud compute firewall-rules create \u0026#34;udp-out-4500\u0026#34; --allow=udp:4500 --direction=OUT gcloud compute firewall-rules create \u0026#34;udp-out-4800\u0026#34; --allow=udp:4800 --direction=OUT After this, the clusters are finally ready for Submariner!\nDeploy Submariner Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Deploy the Broker on cluster-a.\ngcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; subctl deploy-broker The command will output a file named broker-info.subm to the directory it is run from, which will be used to setup the IPsec tunnel between clusters.\nVerify the Broker components are installed:\n$ kubectl get crds | grep submariner clusters.submariner.io endpoints.submariner.io gateways.submariner.io serviceimports.lighthouse.submariner.io kubectl get crds --context cluster-a | grep multicluster serviceexports.multicluster.x-k8s.io serviceimports.multicluster.x-k8s.io $ kubectl get ns | grep submariner submariner-k8s-broker Now it is time to register every cluster in the future ClusterSet to the Broker.\nFirst join the Broker-hosting cluster itself to the Broker:\ngcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; subctl join broker-info.subm --clusterid cluster-a --servicecidr 10.4.0.0/20 Submariner will figure out most required information on its own. The --clusterid and --servicecidr flags should be used to pass the same values as during the cluster creation steps above. You will also see a dialogue on the terminal that asks you to decide which of the three nodes will be the Gateway. Any node will work. It will be annotated with submariner.io/gateway: true.\nWhen a cluster is joined, the Submariner Operator is installed. It creates several components in the submariner-operator namespace:\n submariner-gateway DaemonSet, to open a gateway for the IPsec tunnel on one node submariner-routeagent DaemonSet, which runs on every worker node in order to route the internal traffic to the local gateway via VXLAN tunnels submariner-lighthouse-agent Deployment, which accesses the Kubernetes API server in the Broker cluster to exchange Service information with the Broker submariner-lighthouse-coredns Deployment, which - as an external DNS server - gets forwarded requests to the *.clusterset.local domain for cross-cluster communication by Kubernetes\u0026rsquo; internal DNS server  Check the DaemonSets and Deployments with the following command:\n$ gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; $ kubectl get ds,deploy -n submariner-operator NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/submariner-gateway 1 1 1 1 1 submariner.io/gateway=true 5m29s daemonset.apps/submariner-routeagent 3 3 3 3 3 \u0026lt;none\u0026gt; 5m27s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/submariner-lighthouse-agent 1/1 1 1 5m28s deployment.apps/submariner-lighthouse-coredns 2/2 2 2 5m27s deployment.apps/submariner-operator 1/1 1 1 5m43s Now join the second cluster to the Broker:\ngcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; subctl join broker-info.subm --clusterid cluster-b --servicecidr 10.12.0.0/20 Then verify connectivity and CIDR settings within the ClusterSet:\n$ gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; $ subctl show all CLUSTER ID ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster-a 10.156.0.8 34.107.75.239 libreswan local cluster-b 10.156.0.13 35.242.247.43 libreswan remote GATEWAY CLUSTER REMOTE IP CABLE DRIVER SUBNETS STATUS gke-cluster-b-default-pool-e2e7 cluster-b 10.156.0.13 libreswan 10.12.0.0/20, 10.8.0.0/14 connected NODE HA STATUS SUMMARY gke-cluster-a-default-pool-4e5f active All connections (1) are established COMPONENT REPOSITORY VERSION submariner quay.io/submariner 0.8.0-rc0 submariner-operator quay.io/submariner 0.8.0-rc0 service-discovery quay.io/submariner 0.8.0-rc0 $ gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; $ subctl show all CLUSTER ID ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster-b 10.156.0.13 35.242.247.43 libreswan local cluster-a 10.156.0.8 34.107.75.239 libreswan remote GATEWAY CLUSTER REMOTE IP CABLE DRIVER SUBNETS STATUS gke-cluster-a-default-pool-4e5f cluster-a 10.156.0.8 libreswan 10.4.0.0/20, 10.0.0.0/14 connected NODE HA STATUS SUMMARY gke-cluster-b-default-pool-e2e7 active All connections (1) are established COMPONENT REPOSITORY VERSION submariner quay.io/submariner 0.8.0-rc0 submariner-operator quay.io/submariner 0.8.0-rc0 service-discovery quay.io/submariner 0.8.0-rc0 Workaround for KubeDNS GKE uses KubeDNS by default for cluster-internal DNS queries. Submariner however only works with CoreDNS as of version 0.7. As a consequence, the *.clusterset.local domain stub needs to be added manually to KubeDNS. Query the ClusterIP of the submariner-lighthouse-coredns Service in cluster-a and cluster-b:\n$ gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; $ CLUSTER_IP=$(kubectl get svc submariner-lighthouse-coredns -n submariner-operator -o=custom-columns=ClusterIP:.spec.clusterIP | tail -n +2) $ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap data: stubDomains: | {\u0026#34;clusterset.local\u0026#34;:[\u0026#34;$CLUSTER_IP\u0026#34;]} metadata: labels: addonmanager.kubernetes.io/mode: EnsureExists name: kube-dns namespace: kube-system EOF $ gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; $ CLUSTER_IP=$(kubectl get svc submariner-lighthouse-coredns -n submariner-operator -o=custom-columns=ClusterIP:.spec.clusterIP | tail -n +2) $ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap data: stubDomains: | {\u0026#34;clusterset.local\u0026#34;:[\u0026#34;$CLUSTER_IP\u0026#34;]} metadata: labels: addonmanager.kubernetes.io/mode: EnsureExists name: kube-dns namespace: kube-system EOF Automated Verification This will perform automated verifications between the clusters.\nKUBECONFIG=cluster-a.yml gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; KUBECONFIG=cluster-b.yml gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; subctl verify cluster-a.yml cluster-b.yml --only service-discovery,connectivity --verbose Reconfig after Node Restart If the GKE Nodes were at some point drained or deleted, the Submariner Pods needed to terminate. Once the Nodes are up again, remember to\n label one Node with kubectl label node \u0026lt;name\u0026gt; submariner.io/gateway=true in order for the Gateway to be deployed on this Node apply the Node Configuration workaround again change the applied KubeDNS workaround to reflect the current submariner-lighthouse-coredns IP.  This makes Submariner functional again and work can be continued.\nClean Up When you\u0026rsquo;re done, delete your clusters:\ngcloud container clusters delete cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; gcloud container clusters delete cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; "
},
{
	"uri": "/development/shipyard/first-time/",
	"title": "Adding Shipyard to a Project",
	"tags": [],
	"description": "",
	"content": "To use Shipyard in your project, it\u0026rsquo;s easiest to use Dapper and Make. To use Dapper you\u0026rsquo;ll need a specific Dockerfile that Dapper consumes to create a consistent environment based upon Shipyard\u0026rsquo;s base image. To use Make you\u0026rsquo;ll need some commands to enable Dapper and also include the targets which ship in the base image.\nDockerfile.dapper The project should have a Dockerfile.dapper Dockerfile which builds upon quay.io/submariner/shipyard-dapper-base.\nFor example:\nFROMquay.io/submariner/shipyard-dapper-baseENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT\u0026#34; \\  DAPPER_SOURCE=\u0026lt;your source directory\u0026gt; DAPPER_DOCKER_SOCKET=trueENV DAPPER_OUTPUT=${DAPPER_SOURCE}/outputWORKDIR${DAPPER_SOURCE}ENTRYPOINT [\u0026#34;./scripts/entry\u0026#34;]CMD [\u0026#34;ci\u0026#34;]You can also refer to the project\u0026rsquo;s own Dockerfile.dapper as an example.\nBuilding The Base Image To build the base container image used in the shared developer and CI enviroment, simply run:\nmake dapper-image Makefile The project\u0026rsquo;s Makefile should include targets to run everything in Dapper. These are defined in Shipyard\u0026rsquo;s Makefile.dapper which can be copied as is into your project and included in the Makefile. To use Shipyard\u0026rsquo;s built-in targets available in the base Dapper image, include the Makefile.inc file in the project\u0026rsquo;s Makefile within the section where the Dapper environment is detected.\nThe simplest Makefile would look like this:\nifneq (,$(DAPPER_HOST_ARCH)) # Running in Dapper  include $(SHIPYARD_DIR)/Makefile.inc else # Not running in Dapper  include Makefile.dapper endif # Disable rebuilding Makefile Makefile Makefile.dapper Makefile.inc: ; You can also refer to the project\u0026rsquo;s own Makefile as an example.\n"
},
{
	"uri": "/development/building-testing/",
	"title": "Building and Testing",
	"tags": [],
	"description": "",
	"content": "Submariner strives to be an open, welcoming community. Substantial tooling is provided to ease the contribution experience.\nStandard Development Environment Submariner provides a standard, shared environment for both development and CI that is maintained in the Shipyard project.\nLearn more about working with Shipyard here.\nBuilding and Testing Submariner provides a set of Make targets for building and testing in the standard development environment.\nLinting To run all linting:\nmake lint There are also Make targets for each type of linting:\nmake gitlint golangci-lint markdownlint yamllint See the linter configuration files at the root of each repository for details about which checks are enabled.\nNote that a few linters only run in CI via GitHub Actions and are not available in the standard development environment.\nUnit Tests To run Go unit tests:\nmake unit Building To build the Go binaries provided by a repository:\nmake build To package those Go binaries into container images:\nmake images Note that Submariner will automatically rebuild binaries and images when they have been modified and are required by tests.\nTo prune all Submariner-provided images, ensuring they will be rebuilt or pulled the next time they’re required:\nmake prune-images End-to-End Tests To run functional end-to-end tests with a full multi-cluster deployment:\nmake e2e Different types of deployments can be configured with using flags:\nmake e2e using=helm,globalnet See Shipyard\u0026rsquo;s Makefile.inc for the currently-supported using flags.\nA subset of tests can be selected with Ginkgo focus flags:\nmake e2e focus=dataplane To create a multi-cluster deployment and install Submariner but not run tests:\nmake deploy To create a multi-cluster deployment without Submariner:\nmake clusters To clean up a multi-cluster deployment from one of the previous commands:\nmake cleanup Shell Session in Development Environment To jump into a shell in Submariner\u0026rsquo;s standard development environment:\nmake shell "
},
{
	"uri": "/getting-started/quickstart/kind/",
	"title": "Sandbox Environment (kind)",
	"tags": [],
	"description": "",
	"content": "Deploy kind with Submariner Locally kind is a tool for running local Kubernetes clusters using Docker container nodes. This guide uses kind to demonstrate deployment and operation of Submariner in three Kubernetes clusters running locally on your computer.\nSubmariner provides automation to deploy clusters using kind and connect them using Submariner.\nPrerequisites  Install Docker and ensure it is running properly on your computer. Install and set up kubectl.  Deploy Automatically To create kind clusters and deploy Submariner, run:\ngit clone https://github.com/submariner-io/submariner cd submariner make deploy By default, the automation configuration in the main submariner-io/submariner repository deploys three clusters, with cluster1 configured as the Broker. See the cluster-settings file for details.\nDeploy Manually If you wish to try out Submariner deployment manually, an easy option is to create kind clusters using our scripts and deploy Submariner with subctl.\nCreate kind Clusters To create kind clusters, run:\ngit clone https://github.com/submariner-io/submariner cd submariner make clusters This creates three Kubernetes clusters: cluster1, cluster2 and cluster3. To see the list of kind clusters, use the following command:\n$ kind get clusters cluster1 cluster2 cluster3 To list the local Kubernetes contexts, use the following command:\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE cluster1 cluster1 cluster1 cluster2 cluster2 cluster2 * cluster3 cluster3 cluster3 Since multiple clusters are running, you need to choose which cluster kubectl talks to. You can set a default cluster for kubectl by setting the current context in the Kubernetes kubeconfig file. Additionally, you can run the following command to set the current context for kubectl:\nkubectl config use-context \u0026lt;cluster name\u0026gt;  For more information on interacting with kind, please refer to the kind documentation.\n Install subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Use cluster1 as Broker subctl deploy-broker --kubeconfig output/kubeconfigs/kind-config-cluster1 --service-discovery Join cluster2 and cluster3 to the Broker subctl join --kubeconfig output/kubeconfigs/kind-config-cluster2 broker-info.subm --clusterid cluster2 --natt=false subctl join --kubeconfig output/kubeconfigs/kind-config-cluster3 broker-info.subm --clusterid cluster3 --natt=false You now have a Submariner environment that you can experiment with.\nVerify Deployment Verify Automatically with subctl This will perform automated verifications between the clusters.\nsubctl verify output/kubeconfigs/kind-config-cluster2 output/kubeconfigs/kind-config-cluster3 --only service-discovery,connectivity --verbose Verify Manually To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster3.\nDeploy ClusterIP Service kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 create deployment nginx --image=nginx kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 expose deployment nginx --port=80 subctl export service --kubeconfig output/kubeconfigs/kind-config-cluster3 --namespace default nginx Deploy Headless Service Headless Services can only be exported on non-Globalnet deployments.\n kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 create deployment nginx --image=nginx kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 expose deployment nginx --port=80 --cluster-ip=None subctl export service --kubeconfig output/kubeconfigs/kind-config-cluster3 --namespace default nginx Verify Run nettest from cluster2 to access the nginx service:\nkubectl --kubeconfig output/kubeconfigs/kind-config-cluster2 -n default run --generator=run-pod/v1 \\ tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local Cleanup When you are done experimenting and you want to delete the clusters deployed in any of the previous steps, use the following command:\nmake cleanup "
},
{
	"uri": "/community/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": "Submariner Community Code of Conduct Submariner follows the CNCF Code of Conduct.\nPlease report instances of abusive, harassing, or otherwise unacceptable behavior by contacting one or more of the Submariner Project Owners.\n"
},
{
	"uri": "/operations/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "Submariner provides a Go-based Kubernetes custom controller, called an Operator, that provides easy API-based installation and management. A command line utility, subctl, wraps the Operator to aid users with manual deployments and easy experimentation. subctl greatly simplifies the deployment of Submariner, and is therefore the recommended deployment method. For complete information about subctl, please refer to this page.\nIn addition to Operator and subctl, Submariner also provides Helm Charts.\nInstalling subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Deployment of the Broker The Broker is a set of Custom Resource Definitions (CRDs) backed by the Kubernetes datastore. The Broker must be deployed on a cluster whose Kubernetes API is accessible by all of the participating clusters.\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; This will create:\n The submariner-k8s-broker namespace. The Endpoint and Cluster CRDs in the cluster. A Service Account (SA) in the namespace for subsequent subctl access.  It also generates the broker-info.subm file which contains the following elements:\n The API endpoint. A CA certificate for the API endpoint. The Service Account token for accessing the API endpoint. A random IPsec PSK which will be stored only in this file. Service Discovery settings.  The cluster in which the Broker is deployed can also participate in the dataplane connectivity with other clusters, but it will need to be joined (see following step).\n Joining clusters For each cluster you want to join, issue the following command:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm --clusterid \u0026lt;ID\u0026gt; subctl will automatically discover as much as it can, and prompt the user for any missing necessary information. Note that each cluster must have a unique cluster ID; the cluster ID can be specified, or otherwise is going to be generated by default based on the cluster name in the kubeconfig file.\n"
},
{
	"uri": "/getting-started/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Submariner connects multiple Kubernetes clusters in a way that is secure and performant. Submariner flattens the networks between the connected clusters, and enables IP reachability between Pods and Services. Submariner also provides, via Lighthouse, service discovery capabilities. The service discovery model is built using the proposed Kubernetes Multi Cluster Services.\nSubmariner consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes clusters, both on-premises and on public clouds:\n Gateway Engine: manages the secure tunnels to other clusters. Route Agent: routes cross-cluster traffic from nodes to the active Gateway Engine. Broker: facilitates the exchange of metadata between Gateway Engines enabling them to discover one another. Service Discovery: provides DNS discovery of Services across clusters.  Submariner has optional components that provide additional functionality:\n Globalnet Controller: handles interconnection of clusters with overlapping CIDRs.  The diagram below illustrates the basic architecture of Submariner:\nTerminology and Concepts   ClusterSet - a group of two or more clusters with a high degree of mutual trust that share Services amongst themselves. Within a cluster set, all namespaces with a given name are considered to be the same namespace.\n  ServiceExport (CRD) - used to specify which Services should be exposed across all clusters in the cluster set. If multiple clusters export a Service with the same name and from the same namespace, they will be recognized as a single logical Service.\n  ServiceExports must be explicitly created by the user in each cluster and within the namespace in which the underlying Service resides, in order to signify that the Service should be visible and discoverable to other clusters in the cluster set. The ServiceExport object can be created manually or via the subctl export command.\n  When a Service is exported, it then becomes accessible as \u0026lt;service\u0026gt;.\u0026lt;ns\u0026gt;.svc.clusterset.local.\n    ServiceImport (CRD) - representation of a multi-cluster Service in each cluster. Created and used internally by Lighthouse and does not require any user action.\n  "
},
{
	"uri": "/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes clusters. For more information about Submariner\u0026rsquo;s architecture, please refer to the Architecture section.\nThe Broker The Broker is an API to which all participating clusters are given access to, and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a cluster, and the reachable cluster IPs from the endpoint.  The Broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe Submariner Deployment on a Cluster Once Submariner is deployed on a cluster with the proper credentials to the Broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n At least two Kubernetes clusters, one of which is designated to serve as the central Broker that is accessible by all of your connected clusters; this can be one of your connected clusters, or a dedicated cluster. Minimum supported Kubernetes version is 1.17. Non-overlapping Pod and Service CIDRs between clusters. This is to prevent routing conflicts. For cases where addresses do overlap, Globalnet can be set up. IP reachability between the gateway nodes. When connecting two clusters, at least one of the clusters should have a publicly routable IP address designated to the Gateway node. This is needed for creating the IPsec tunnel between the clusters. The default ports used by IPsec are 4500/UDP and 500/UDP. For clusters behind corporate firewalls that block the default ports, Submariner also supports NAT Traversal (NAT-T) with the option to set custom non-standard ports like 4501/UDP and 501/UDP. Submariner uses UDP port 4800 to encapsulate Pod traffic from worker and master nodes to the Gateway nodes. This is required in order to preserve the source IP addresses of the Pods. Ensure that firewall configuration allows 4800/UDP across all nodes in the cluster in both directions. Submariner uses TCP port 8080 to export metrics on the Gateway nodes. Ensure that firewall configuration allows ingress 8080/TCP on the Gateway nodes so that other nodes in the cluster can access it. Also, no other workload on the Gateway nodes should be listening on TCP port 8080. Worker node IPs on all connected clusters must be outside of the Pod/Service CIDR ranges.  An example of three clusters configured to use with Submariner (without Globalnet) would look like the following:\n   Cluster Name Provider Pod CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east On-Prem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Support Matrix Submariner is designed to be cloud provider agnostic, and should run in any standard Kubernetes cluster. Presently, Submariner has been tested with the following network (CNI) Plugins:\n OpenShift-SDN Weave Flannel Canal Calico (see the Calico-specific deployment instructions) OVN  Deployment The available methods for deployment are:\n subctl Operator Helm  subctl greatly simplifies the deployment of Submariner, and is therefore the recommended deployment method.\n"
},
{
	"uri": "/getting-started/architecture/broker/",
	"title": "Broker",
	"tags": [],
	"description": "",
	"content": "Submariner uses a central Broker component to facilitate the exchange of metadata information between Gateway Engines deployed in participating clusters. The Broker is basically a set of Custom Resource Definitions (CRDs) backed by the Kubernetes datastore. The Broker also defines a ServiceAccount and RBAC components to enable other Submariner components to securely access the Broker\u0026rsquo;s API. There are no Pods or Services deployed with the Broker.\nSubmariner defines two CRDs that are exchanged via the Broker: Endpoint and Cluster. The Endpoint CRD contains the information about the active Gateway Engine in a cluster, such as its IP, needed for clusters to connect to one another. The Cluster CRD contains static information about the originating cluster, such as its Service and Pod CIDRs.\nThe Broker is a singleton component that is deployed on a cluster whose Kubernetes API must be accessible by all of the participating clusters. If there is a mix of on-premises and public clusters, the Broker can be deployed on a public cluster. The Broker cluster may be one of the participating clusters or a standalone cluster without the other Submariner components deployed. The Gateway Engine components deployed in each participating cluster are configured with the information to securely connect to the Broker cluster\u0026rsquo;s API.\nThe availability of the Broker cluster does not affect the operation of the dataplane on the participating clusters, that is the dataplane will continue to route traffic using the last known information while the Broker is unavailable. However, during this time, control plane components will be unable to advertise new or updated information to other clusters and learn about new or updated information from other clusters. When connection is re-established to the Broker, each component will automatically re-synchronize its local information with the Broker and update the dataplane if necessary.\n"
},
{
	"uri": "/operations/deployment/subctl/",
	"title": "subctl",
	"tags": [],
	"description": "",
	"content": "The subctl command-line utility simplifies the deployment and maintenance of Submariner by automating interactions with the Submariner Operator\nSynopsis subctl [command] [--flags] ...\nInstallation Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Installing specific versions By default, https://get.submariner.io will provide the latest release for subctl, and hence Submariner. Specific versions can be requested by using the VERSION environment variable.\nAvalailable options are:\n latest: the latest stable release (default) devel: the devel branch code. rc: the latest release candidate. x.x.x (like 0.6.1, 0.5.0, etc)  For example\ncurl https://get.submariner.io | VERSION=devel bash Commands deploy-broker subctl deploy-broker [flags]\nThe deploy-broker command configures the cluster specified by the --kubeconfig flag (or KUBECONFIG env var) and the --kubecontext flag as the Broker. It installs the necessary CRDs and the submariner-k8s-broker namespace.\nIn addition, it generates a broker-info.subm file which can be used with the join command to connect clusters to the Broker. This file contains the following details:\n Encryption PSK key Broker access details for subsequent subctl runs Service Discovery settings  deploy-broker flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default $HOME/.kube/config)   --kubecontext \u0026lt;string\u0026gt; kubeconfig context to use   --service-discovery Enable Multi Cluster Service Discovery (default true)   --globalnet Enable support for overlapping Cluster/Service CIDRs in connecting clusters (default disabled)   --globalnet-cidr-range \u0026lt;string\u0026gt; Global CIDR supernet range for allocating GlobalCIDRs to each cluster (default \u0026ldquo;169.254.0.0/16\u0026rdquo;)   --ipsec-psk-from \u0026lt;string\u0026gt; Import IPsec PSK from existing Submariner broker file, like broker-info.subm (default broker-info.subm)    export export service subctl export service [flags] \u0026lt;name\u0026gt; creates a ServiceExport resource for the given Service name. This makes the corresponding Service discoverable from other clusters in the Submariner deployment.\nexport service flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default $HOME/.kube/config)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use   --namespace \u0026lt;string\u0026gt; Namespace in which the Service to be exported belongs    If no namespace flag is specified, it uses the default namespace from the current context, if present, otherwise it uses default.\njoin subctl join broker-info.subm [flags]\nThe join command deploys the Submariner Operator in a cluster using the settings provided in the broker-info.subm file. The service account credentials needed for the new cluster to access the Broker cluster will be created and provided to the Submariner Operator deployment.\njoin flags (general)    Flag Description     --cable-driver \u0026lt;string\u0026gt; Cable driver implementation. Available options are libreswan (default) and wireguard   --clusterid \u0026lt;string\u0026gt; Cluster ID used to identify the tunnels. Every cluster needs to have a unique cluster ID. If not provided, one will be generated by default based on the cluster name in the kubeconfig file   --clustercidr \u0026lt;string\u0026gt; Specifies the cluster\u0026rsquo;s CIDR used to generate Pod IP addresses. If not specified, subctl will try to discover it and if unable to do so, it will prompt the user   --label-gateway Label getways (enabled by default). --label-gateway=false disables the prompt for a Worker node to use as gateway   --pod-debug Enable Submariner pod debugging (verbose logging in the deployed pods)    join flags (Globalnet)    Flag Description     --globalnet Enable/disable Globalnet for this cluster (default true). This has no effect if Globalnet is not enabled globally via the Broker   --globalnet-cluster-size \u0026lt;value\u0026gt; If Globalnet is enabled, the cluster size for the GlobalCIDR allocated to this cluster (amount of global IPs)   --globalnet-cidr \u0026lt;string\u0026gt; If Globalnet is enabled, the specific Globalnet CIDR to use for this cluster. This setting is exclusive with --globalnet-cluster-size    join flags (IPsec)    Flag Description     --natt Enable NAT for IPsec (default enabled)   --ikeport \u0026lt;value\u0026gt; IPsec IKE port (default 500)   --ipsec-debug Enable IPsec debugging (verbose logging)   --nattport \u0026lt;value\u0026gt; IPsec NAT-T port (default 4500)    join flags (images and repositories)    Flag Description     --repository \u0026lt;string\u0026gt; The repository from where the various Submariner images will be sourced (default \u0026ldquo;quay.io/submariner\u0026rdquo;)   --version \u0026lt;string\u0026gt; Image version   --image-override \u0026lt;string\u0026gt;=\u0026lt;string\u0026gt; Component image override. This flag can be used more than once (example: \u0026ndash;image-override=submariner=quay.io/myUser/submariner:latest)    join flags (health check)    Flag Description     --health-check Enable/disable Gateway health check (default true)   --health-check-interval \u0026lt;uint\u0026gt; The interval in seconds at which health check packets will be sent (default 1)   --health-check-max-packet-loss-count \u0026lt;uint\u0026gt; The maximum number of packets lost at which the health checker will mark the connection as down (default 5)    show show networks subctl show networks [flags]\nInspects the cluster and reports information about the detected network plugin and detected Cluster and Service CIDRs.\nshow versions subctl show versions [flags]\nShows the version and image repository of each Submariner component in the cluster.\nshow gateways subctl show gateways [flags]\nShows summary information about the Submariner gateways in the cluster.\nshow connections subctl show connections [flags]\nShows information about the Submariner endpoint connections with other clusters.\nshow endpoints subctl show endpoints [flags]\nShows information about the Submariner endpoints in the cluster.\nshow all subctl show all [flags]\nShows the aggregated information from all the other show commands.\nshow flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default $HOME/.kube/config)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use    verify subctl verify \u0026lt;kubeConfig1\u0026gt; \u0026lt;kubeConfig2\u0026gt; [flags]\nThe verify command verifies a Submariner deployment between two clusters is functioning properly. The kubeConfig1 file will be ClusterA in the reports, while kubeConfig2 will be ClusterB in the reports. The --verbose flag is recommended to see what\u0026rsquo;s happening during the tests.\nThere are several suites of verifications that can be performed. By default all verifications are performed. Some verifications are deemed disruptive in that they change some state of the clusters as a side effect. If running the command interactively, you will be prompted for confirmation to perform disruptive verifications unless the --disruptive-tests flag is also specified. If running non-interactively (that is with no stdin), --disruptive-tests must be specified otherwise disruptive verifications are skipped.\nThe connectivity suite verifies dataplane connectivity across the clusters for the following cases:\n Pods (on Gateway nodes) to Services Pods (on non-Gateway nodes) to Services Pods (on Gateway nodes) to Pods Pods (on non-Gateway nodes) to Pods  The service-discovery suite verifies DNS discovery of \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local entries across the clusters.\nThe gateway-failover suite verifies the continuity of cross-cluster dataplane connectivity after a gateway failure in a cluster occurs. This suite requires a single gateway configured on ClusterA and other available Worker nodes capable of serving as gateways. Please note that this verification is disruptive.\nverify flags    Flag Description     --connection-attempts \u0026lt;value\u0026gt; The maximum number of connection attempts (default 2)   --connection-timeout \u0026lt;value\u0026gt; The timeout in seconds per connection attempt (default 60)   --operation-timeout \u0026lt;value\u0026gt; Operation timeout for Kubernetes API calls (default 240)   --report-dir \u0026lt;string\u0026gt; XML report directory (default \u0026ldquo;.\u0026quot;)   --verbose Produce verbose logs during connectivity verification   --only Comma separated list of specific verifications to perform   --disruptive-tests Enable verifications which are potentially disruptive to your deployment    benchmark benchmark throughput subctl benchmark throughput \u0026lt;kubeconfig1\u0026gt; [\u0026lt;kubeconfig2\u0026gt;] [flags]\nThe benchmark throughput command runs a throughput benchmark test between two specified clusters or within a single cluster. It deploys a Pod to run the iperf tool and logs the output to the console. When running benchmark throughput, two types of tests will be executed:\n Pod to Pod - where both Pods are scheduled on Gateway nodes Pod to Pod - where both Pods are scheduled on non-Gateway nodes  benchmark latency subctl benchmark latency \u0026lt;kubeconfig1\u0026gt; [\u0026lt;kubeconfig2\u0026gt;] [flags]\nThe benchmark latency command runs a latency benchmark test between two specified clusters or within a single cluster. It deploys a Pod to run the netperf tool and logs the output to the console. When running benchmark latency, two types of tests will be executed:\n Pod to Pod - where both Pods are scheduled on Gateway nodes Pod to Pod - where both Pods are scheduled on non-Gateway nodes  benchmark flags    Flag Description     --intra-cluster Performs the benchmark test within a single cluster between Pods from a Non-Gateway node to a Gateway node   --verbose Produce verbose logs during benchmark tests    version subctl version\nPrints the version details for the subctl binary.\n"
},
{
	"uri": "/operations/usage/",
	"title": "User Guide",
	"tags": [],
	"description": "",
	"content": "Overview This guide is intended for users who have a Submariner environment set up and want to verify the installation and learn more about how to use Submariner and the main capabilities it provides. This guide assumes that there are two Kubernetes clusters, cluster2 and cluster3, forming a cluster set, and that the Broker is deployed into a separate cluster cluster1.\nMake sure you have subctl set up. Regardless of how Submariner was deployed, subctl can be used for various verification and troubleshooting tasks, as shown in this guide.\n This guide focuses on a non-Globalnet Submariner deployment.\n 1. Validate the Installation On the Broker The Broker facilitates the exchange of metadata information between the connected clusters, enabling them to discover one another. The Broker consists of only a set of Custom Resource Definitions (CRDs); there are no Pods or Services deployed with it.\nThis command validates that the Broker namespace has been created in the Broker cluster:\n$ export KUBECONFIG=cluster1/auth/kubeconfig $ kubectl config use-context cluster1 Switched to context \u0026#34;cluster1\u0026#34;. $ kubectl get namespace submariner-k8s-broker NAME STATUS AGE submariner-k8s-broker Active 5m This command validates that the Submariner CRDs have been created in the Broker cluster:\n$ kubectl get crds | grep -iE \u0026#39;submariner|multicluster.x-k8s.io\u0026#39; clusters.submariner.io 2020-11-30T13:49:16Z endpoints.submariner.io 2020-11-30T13:49:16Z gateways.submariner.io 2020-11-30T13:49:16Z serviceimports.multicluster.x-k8s.io 2020-11-30T13:52:39Z This command validates that the participating clusters have successfully joined the Broker:\n$ kubectl -n submariner-k8s-broker get clusters.submariner.io NAME AGE cluster2 5m9s cluster3 2m9s On Connected Clusters The commands below can be used on either cluster2 or cluster3 to verify that the two clusters have successfully formed a cluster set and are properly connected to one another. In this example, the commands are being issued on cluster2.\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. The command below lists all the Submariner related Pods. Ensure that the STATUS for each is Running, noting that some could have an intermediate transient status, like Pending or ContainerCreating, indicating they are still starting up. To continuously monitor the Pods, you can specify the --watch flag with the command:\n$ kubectl -n submariner-operator get pods NAME READY STATUS RESTARTS AGE submariner-gateway-btzrq 1/1 Running 0 76s submariner-lighthouse-agent-586cf4899-wn747 1/1 Running 0 75s submariner-lighthouse-coredns-c88f64f5-h77kw 1/1 Running 0 73s submariner-lighthouse-coredns-c88f64f5-qlw4x 1/1 Running 0 73s submariner-operator-dcbdf5669-n7jgp 1/1 Running 0 89s submariner-routeagent-bmgbc 1/1 Running 0 75s submariner-routeagent-rl9nh 1/1 Running 0 75s submariner-routeagent-wqmzs 1/1 Running 0 75s This command verifies on which Kubernetes node the Gateway Engine is running:\n$ kubectl get node --selector=submariner.io/gateway=true -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cluster2-worker Ready worker 6h59m v1.17.0 172.17.0.7 3.81.125.62 Ubuntu 19.10 5.8.18-200.fc32.x86_64 containerd://1.3.2 This command verifies the connection between the participating clusters:\n$ subctl show connections --kubecontext cluster2 Showing information for cluster \u0026#34;cluster2\u0026#34;: GATEWAY CLUSTER REMOTE IP CABLE DRIVER SUBNETS STATUS cluster3-worker cluster3 172.17.0.10 libreswan 100.3.0.0/16, 10.3.0.0/16 connected This command shows detailed information about the Gateway including the connections to other clusters. The section highlighted in bold shows the connection information for cluster3, including the connection status and latency statistics:\n $ kubectl -n submariner-operator describe Gateway Name: cluster2-worker Namespace: submariner-operator Labels:  Annotations: update-timestamp: 1606751397 API Version: submariner.io/v1 Kind: Gateway Metadata: Creation Timestamp: 2020-11-30T13:51:39Z Generation: 538 Resource Version: 28717 Self Link: /apis/submariner.io/v1/namespaces/submariner-operator/gateways/cluster2-worker UID: 682f791a-00b5-4f51-8249-80c7c82c4bbf Status: Connections: Endpoint: Backend: libreswan cable_name: submariner-cable-cluster3-172-17-0-10 cluster_id: cluster3 Health Check IP: 10.3.224.0 Hostname: cluster3-worker nat_enabled: false private_ip: 172.17.0.10 public_ip: Subnets: 100.3.0.0/16 10.3.0.0/16 Latency RTT: Average: 1.16693ms Last: 1.128109ms Max: 1.470344ms Min: 1.110059ms Std Dev: 68.57µs Status: connected Status Message:  Ha Status: active Local Endpoint: Backend: libreswan cable_name: submariner-cable-cluster2-172-17-0-7 cluster_id: cluster2 Health Check IP: 10.2.224.0 Hostname: cluster2-worker nat_enabled: false private_ip: 172.17.0.7 public_ip: Subnets: 100.2.0.0/16 10.2.0.0/16 Status Failure: Version: v0.8.0-pre0-1-g5d7f163 Events:   To validate that Service Discovery (Lighthouse) is installed properly, check that the ServiceExport and ServiceImport CRDs have been deployed in the cluster:\n$ kubectl get crds | grep -iE \u0026#39;multicluster.x-k8s.io\u0026#39; serviceexports.multicluster.x-k8s.io 2020-11-30T13:50:34Z serviceimports.multicluster.x-k8s.io 2020-11-30T13:50:33Z Verify that the submariner-lighthouse-coredns Service is ready:\n$ kubectl -n submariner-operator get service submariner-lighthouse-coredns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE submariner-lighthouse-coredns ClusterIP 100.2.177.123 \u0026lt;none\u0026gt; 53/UDP 126m Verify that CoreDNS was properly configured to forward requests sent for the clusterset.local domain to the to Lighthouse CoreDNS Server in the cluster:\n$ kubectl -n kube-system describe configmap coredns Name: coredns Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== Corefile: ---- #lighthouse-start AUTO-GENERATED SECTION. DO NOT EDIT clusterset.local:53 { forward . 100.2.177.123 } #lighthouse-end .:53 { errors health { lameduck 5s } ready kubernetes cluster2.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } Note that 100.2.177.123 is the ClusterIP address of the submariner-lighthouse-coredns Service we verified earlier.\n2. Export Services Across Clusters At this point, we have enabled secure IP communication between the connected clusters and formed the cluster set infrastructure. However, further configuration is required in order to signify that a Service should be visible and discoverable to other clusters in the cluster set. In following sections, we will define a Service and show how to export it to other clusters.\nThis guide uses a simple nginx server for demonstration purposes.\nIn the example below, we create the nginx resources within the nginx-test namespace. Note that the namespace must be created in both clusters for service discovery to work properly.\n Test ClusterIP Services 1. Create an nginx Deployment on cluster3 $ export KUBECONFIG=cluster3/auth/kubeconfig $ kubectl config use-context cluster3 Switched to context \u0026#34;cluster3\u0026#34;. The following commands create an nginx Service in the nginx-test namespace which targets TCP port 8080 on any Pod with the app: nginx label and exposes it on an abstracted Service port. When created, the Service is assigned a unique IP address (also called ClusterIP):\n$ kubectl create namespace nginx-test namespace/nginx-test created $ kubectl -n nginx-test create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine deployment.apps/nginx created $ kubectl -n nginx-test expose deployment nginx --port=8080 service/nginx exposed Verify that the Service exists and is running:\n$ kubectl -n nginx-test get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 100.3.220.176 \u0026lt;none\u0026gt; 8080/TCP 2m41s $ kubectl -n nginx-test get pods -l app=nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-667744f849-t26s5 1/1 Running 0 3m 10.3.0.5 cluster3-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 2. Export the Service In order to signify that the Service should be visible and discoverable to other clusters in the cluster set, a ServiceExport object needs to be created. This is done using the subctl export command:\n$ subctl export service --namespace nginx-test nginx Service exported successfully After creation of the ServiceExport, the nginx Service will be exported to other clusters via the Broker. The Status information on the ServiceExport object will indicate this:\n$ kubectl -n nginx-test describe serviceexports Name: nginx Namespace: nginx-test Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-12-01T12:35:32Z Generation: 1 Resource Version: 302209 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/nginx-test/serviceexports/nginx UID: afe0533c-7cca-4443-9d8a-aee8e888e8bc Status: Conditions: Last Transition Time: 2020-12-01T12:35:32Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-12-01T12:35:32Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; Once exported, the Service can be discovered as nginx.nginx-test.svc.clusterset.local across the cluster set.\n3. Consume the Service on cluster2 Verify that the exported nginx Service was imported to cluster2 as expected. Submariner (via Lighthouse) automatically creates a corresponding ServiceImport:\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. $ kubectl get -n submariner-operator serviceimport NAME TYPE IP AGE nginx-nginx-test-cluster3 ClusterSetIP [100.3.220.176] 13m Next, run a test Pod on cluster2 and try to access the nginx Service from within the Pod:\n$ kubectl create namespace nginx-test namespace/nginx-test created $ kubectl -n nginx-test run --generator=run-pod/v1 \\ tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash bash-5.0# curl nginx.nginx-test.svc.clusterset.local:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; bash-5.0# dig nginx.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; nginx.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 34800 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 6ff7ea72c14ce2d4 (echoed) ;; QUESTION SECTION: ;nginx.nginx-test.svc.clusterset.local. IN\tA ;; ANSWER SECTION: nginx.nginx-test.svc.clusterset.local. 5 IN A\t100.3.220.176 ;; Query time: 16 msec ;; SERVER: 100.2.0.10#53(100.2.0.10) ;; WHEN: Mon Nov 30 17:52:55 UTC 2020 ;; MSG SIZE rcvd: 125 Note that DNS resolution works across the clusters, and that the IP address 100.3.220.176 returned is the same ClusterIP associated with the nginx Service on cluster3.\n4. Create an nginx Deployment on cluster2 If multiple clusters export a Service with the same name and from the same namespace, it will be recognized as a single logical Service. To test this, we will deploy the same nginx Service in the same namespace on cluster2:\n$ kubectl -n nginx-test create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine deployment.apps/nginx created $ kubectl -n nginx-test expose deployment nginx --port=8080 service/nginx exposed Verify the Service exists and is running:\n$ kubectl -n nginx-test get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 100.2.29.136 \u0026lt;none\u0026gt; 8080/TCP 1m40s $ kubectl -n nginx-test get pods -l app=nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5578584966-d7sj7 1/1 Running 0 22s 10.2.224.3 cluster2-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5. Export the Service As before, use the subctl export command to export the Service:\n$ subctl export service --namespace nginx-test nginx Service exported successfully After creation of the ServiceExport, the nginx Service will be exported to other clusters via the Broker. The Status information on the ServiceExport object will indicate this:\n$ kubectl -n nginx-test describe serviceexports Name: nginx Namespace: nginx-test Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-12-07T17:37:59Z Generation: 1 Resource Version: 3131 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/nginx-test/serviceexports/nginx UID: 7348eb3c-9558-4dc7-be1d-b0255a2038fd Status: Conditions: Last Transition Time: 2020-12-07T17:37:59Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-12-07T17:37:59Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; 6. Consume the Service from cluster2 Run a test Pod on cluster2 and try to access the nginx Service from within the Pod:\n$ kubectl -n nginx-test run --generator=run-pod/v1 \\ tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash bash-5.0# curl nginx.nginx-test.svc.clusterset.local:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; bash-5.0# dig nginx.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; nginx.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 55022 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 6f9db9800a9a9779 (echoed) ;; QUESTION SECTION: ;nginx.nginx-test.svc.clusterset.local. IN\tA ;; ANSWER SECTION: nginx.nginx-test.svc.clusterset.local. 5 IN A\t100.2.29.136 ;; Query time: 5 msec ;; SERVER: 100.3.0.10#53(100.3.0.10) ;; WHEN: Tue Dec 01 07:45:48 UTC 2020 ;; MSG SIZE rcvd: 125 At this point we have the same nginx Service deployed within the nginx-test namespace on both clusters. Note that DNS resolution works, and the IP address 100.2.29.136 returned is the ClusterIP associated with the local nginx Service deployed on cluster2. This is expected, as Submariner prefers to handle the traffic locally whenever possible.\n Service Discovery for Services Deployed to Multiple Clusters Submariner follows this logic for service discovery across the cluster set:\n  If an exported Service is not available in the local cluster, Lighthouse DNS returns the IP address of the ClusterIP Service from one of the remote clusters on which the Service was exported.\n  If an exported Service is available in the local cluster, Lighthouse DNS always returns the IP address of the local ClusterIP Service. In this example, if a Pod from cluster2 tries to access the nginx Service as nginx.nginx-test.svc.clusterset.local now, Lighthouse DNS resolves the Service as 100.2.29.136 which is the local ClusterIP Service on cluster2. Similarly, if a Pod from cluster3 tries to access the nginx Service as nginx.nginx-test.svc.clusterset.local, Lighthouse DNS resolves the Service as 100.3.220.176 which is the local ClusterIP Service on cluster3.\n  If multiple clusters export a Service with the same name and from the same namespace, Lighthouse DNS load-balances between the clusters in a round-robin fashion. If, in our example, a Pod from a third cluster that joined the cluster set tries to access the nginx Service as nginx.nginx-test.svc.clusterset.local, Lighthouse will round-robin the DNS responses across cluster2 and cluster3, causing requests to be served by both clusters. Note that Lighthouse returns IPs from connected clusters only. Clusters in disconnected state are ignored.\n  Applications can always access a Service from a specific cluster by prefixing the DNS query with cluster-id as follows: \u0026lt;cluster-id\u0026gt;.\u0026lt;svcname\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local. In our example, querying for cluster2.nginx.nginx-test.svc.clusterset.local always returns the ClusterIP Service on cluster2. Similarly, cluster3.nginx.nginx-test.svc.clusterset.local always returns the ClusterIP Service on cluster3.\n  Test StatefulSet and Headless Service Submariner also supports Headless Services with StatefulSets, making it possible to access individual Pods via their stable DNS name. Kubernetes supports this by introducing stable Pod IDs composed of \u0026lt;pod-name\u0026gt;.\u0026lt;svc-name\u0026gt;.\u0026lt;ns\u0026gt;.svc.cluster.local within a single cluster, which Submariner extends to \u0026lt;pod-name\u0026gt;.\u0026lt;cluster-id\u0026gt;.\u0026lt;svc-name\u0026gt;.\u0026lt;ns\u0026gt;.svc.clusterset.local across the cluster set. The Headless Service in this case offers one single Service for all the underlying Pods.\nLike a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. StatefulSets are typically used for applications that require stable unique network identifiers, persistent storage, and ordered deployment and scaling.\n1. Create a StatefulSet and Headless Service on cluster3 kubectl apply the following yaml within the nginx-test namespace:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web This specification will create a StatefulSet named web which indicates that two replicas of the nginx container will be launched in unique Pods. This also creates a Headless Service called nginx-ss on the nginx-test namespace. Note that Headless Service is requested by explicitly specifying \u0026ldquo;None\u0026rdquo; for the clusterIP (.spec.clusterIP).\n$ kubectl -n nginx-test apply -f ./nginx-ss.yaml service/nginx-ss created statefulset.apps/web created Verify the Service and StatefulSet:\n$ kubectl -n nginx-test get service nginx-ss NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ss ClusterIP None \u0026lt;none\u0026gt; 80/TCP 83s $ kubectl -n nginx-test describe statefulset web Name: web Namespace: nginx-test CreationTimestamp: Mon, 30 Nov 2020 21:53:01 +0200 Selector: app.kubernetes.io/instance=nginx-ss,app.kubernetes.io/name=nginx-ss Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Replicas: 2 desired | 2 total Update Strategy: RollingUpdate Partition: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app.kubernetes.io/instance=nginx-ss app.kubernetes.io/name=nginx-ss Containers: nginx-ss: Image: nginxinc/nginx-unprivileged:stable-alpine Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Volume Claims: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 94s statefulset-controller create Pod web-0 in StatefulSet web successful Normal SuccessfulCreate 85s statefulset-controller create Pod web-1 in StatefulSet web successful 2. Export the Service on cluster-3 As before, use the subctl export command to export the Service:\n$ subctl export service --namespace nginx-test nginx-ss Service exported successfully After creation of the ServiceExport, the nginx-ss Service will be exported to other clusters via the Broker. The Status information on the ServiceExport object will indicate this:\n$ kubectl -n nginx-test describe serviceexport nginx-ss Name: nginx-ss Namespace: nginx-test Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-11-30T19:59:44Z Generation: 1 Resource Version: 83431 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/nginx-test/serviceexports/nginx-ss UID: 2c0d6419-6160-431e-990c-8a9993363b10 Status: Conditions: Last Transition Time: 2020-11-30T19:59:44Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-11-30T19:59:44Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; Once the Service is exported successfully, it can be discovered as nginx-ss.nginx-test.svc.clusterset.local across the cluster set. In addition, the individual Pods can be accessed as web-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local and web-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local.\n3. Consume the Service from cluster2 Verify that the exported nginx-ss Service was imported to cluster2. Submariner (via Lighthouse) automatically creates a corresponding ServiceImport:\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. $ kubectl get -n submariner-operator serviceimport NAME TYPE IP AGE nginx-nginx-test-cluster3 ClusterSetIP [100.3.220.176] 166m nginx-ss-nginx-test-cluster3 Headless 5m48s Next, run a test Pod on cluster2 and try to access the nginx-ss Service from within the Pod:\nkubectl -n nginx-test run --generator=run-pod/v1 \\ tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash bash-5.0# dig nginx-ss.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; nginx-ss.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 19729 ;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 0b17506cb2b4a93b (echoed) ;; QUESTION SECTION: ;nginx-ss.nginx-test.svc.clusterset.local. IN A ;; ANSWER SECTION: nginx-ss.nginx-test.svc.clusterset.local. 5 IN A\t10.3.0.5 nginx-ss.nginx-test.svc.clusterset.local. 5 IN A\t10.3.224.3 ;; Query time: 1 msec ;; SERVER: 100.2.0.10#53(100.2.0.10) ;; WHEN: Mon Nov 30 20:18:08 UTC 2020 ;; MSG SIZE rcvd: 184 You can also access the individual Pods:\nbash-5.0# nslookup web-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local Server:\t100.2.0.10 Address:\t100.2.0.10#53 Name:\tweb-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local Address: 10.3.0.5 bash-5.0# nslookup web-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local Server:\t100.2.0.10 Address:\t100.2.0.10#53 Name:\tweb-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local Address: 10.3.224.3 Clean the Created Resources To remove the previously created Kubernetes resources, simply delete the nginx-test namespace from both clusters:\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. $ kubectl delete namespace nginx-test namespace \u0026#34;nginx-test\u0026#34; deleted $ export KUBECONFIG=cluster3/auth/kubeconfig $ kubectl config use-context cluster3 Switched to context \u0026#34;cluster3\u0026#34;. $ kubectl delete namespace nginx-test namespace \u0026#34;nginx-test\u0026#34; deleted "
},
{
	"uri": "/operations/",
	"title": "Operations",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/operations/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner provides a number of Prometheus metrics, and sets up ServiceMonitor instances which allow these metrics to be scraped by an in-cluster Prometheus deployment. Prometheus is a pluggable metrics collection and storage system and can act as a data source for Grafana, a metrics visualization frontend. Unlike some metrics collectors, Prometheus requires the collectors to pull metrics from each source.\nPrometheus Operator To start monitoring Submariner using the Prometheus Operator, Prometheus needs to be configured to scrape the Submariner Operator’s namespace (submariner-operator by default). The specifics depend on your Prometheus deployment, but typically, this will require you to:\n  Add the Submariner Operator’s namespace to Prometheus’ ClusterRoleBinding.\n  Ensure that Prometheus’ configuration doesn’t prevent it from scraping this namespace.\n  A minimal Prometheus object providing access to the Submariner metrics is as follows:\napiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: name: prometheus labels: prometheus: prometheus spec: replicas: 1 serviceAccountName: prometheus serviceMonitorNamespaceSelector: {} serviceMonitorSelector: matchLabels: name: submariner-operator OpenShift Setup OpenShift 4.5 or later can automatically discover the Submariner metrics. This requires enabling user workload monitoring; see the OpenShift 4.5 or OpenShift 4.6 documentation for details.\nMetrics Reference Submariner metrics provide insights into both the state of Submariner itself, as well as the inter-cluster network behavior of your cluster set. All Submariner metrics are exported within the submariner-operator namespace by default.\nThe following metrics are exposed currently:\nSubmariner Gateway    Name Label Description     submariner_gateways  The number of gateways in the cluster   submariner_gateway_creation_timestamp local_cluster, local_hostname Timestamp of gateway creation time   submariner_gateway_sync_iterations  Gateway synchronization iterations   submariner_gateway_rx_bytes cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Count of bytes received by cable driver and cable   submariner_gateway_tx_bytes cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Count of bytes transmitted by cable driver and cable    Submariner Connections    Name Label Description     submariner_requested_connections local_cluster, local_hostname, remote_cluster, remote_hostname, status: “connecting”, “connected”, or “error” The number of connections by endpoint and status   submariner_connections cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip, status: “connecting”, “connected”, or “error” The number of connections and corresponding status by cable driver and cable   submariner_connection_established_timestamp cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Timestamp of last successful connection established by cable driver and cable   submariner_connection_latency_seconds cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Connection latency in seconds; last RTT, by cable driver and cable    Globalnet    Name Label Description     submariner_global_IP_availability cidr Count of available global IPs per CIDR   submariner_global_IP_allocated cidr Count of global IPs allocated for Pods/Services per CIDR    Service Discovery    Name Label Description     submariner_service_import direction, operation, syncer_name Count of imported Services   submariner_service_export direction, operation, syncer_name Count of exported Services    "
},
{
	"uri": "/getting-started/quickstart/managed-kubernetes/rancher/",
	"title": "Rancher",
	"tags": [],
	"description": "",
	"content": " Prerequisites These instructions were developed with Rancher v2.4.x\nMake sure you are familiar with Rancher, and creating clusters. You can create either node driver clusters or Custom clusters, as long as your designated gateway nodes can communicate with each other.\nCreate and Deploy Cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.42.0.0/16 10.43.0.0/16    Use the Rancher UI to create a cluster, leaving the default options selected.\nMake sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nCreate and Deploy Cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.44.0.0/16 10.45.0.0/16    Create your cluster, but select Edit as YAML in the cluster creation UI. Edit the services stanza to reflect the options below, while making sure to keep the options that were already defined.\nservices: kube-api: service_cluster_ip_range: 10.45.0.0/16 kube-controller: cluster_cidr: 10.44.0.0/16 service_cluster_ip_range: 10.45.0.0/16 kubelet: cluster_domain: cluster.local cluster_dns_server: 10.45.0.10 Make sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nOnce you have done this, you can deploy your cluster.\nInstall subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Obtain the kubeconfig files from the Rancher UI for each of your clusters, placing them in the respective kubeconfigs.\n   Cluster Kubeconfig File Name     Cluster A kubeconfig-cluster-a   Cluster B kubeconfig-cluster-b    Use cluster-a as Broker subctl deploy-broker --kubeconfig kubeconfig-cluster-a Join cluster-a and cluster-b to the Broker subctl join --kubeconfig kubeconfig-cluster-a broker-info.subm --clusterid cluster-a subctl join --kubeconfig kubeconfig-cluster-b broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster Pods and Services\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only connectivity --verbose "
},
{
	"uri": "/development/code-review/",
	"title": "Code Review Guide",
	"tags": [],
	"description": "",
	"content": "Code Review Guide This guide is meant to facilitate Submariner code review by sharing norms, best practices, and useful patterns.\nSubmariner follows the Kubernetes Code Review Guide wherever relevant. This guide collects the most important highlights of the Kubernetes process and adds Submariner-specific extensions.\nTwo non-author Committer approvals required Pull Requests to Submariner require two approvals from a Committer to the relevant part of the code base, as defined by the CODEOWNERS file at the root of each repository and the Community Membership/Committers process.\nNo merge commits Kubernetes recommends avoiding merge commits.\nWith our current GitHub setup, pull requests are liable to include merge commits temporarily. Whenever a PR is updated through the UI, GitHub merges the target branch into the PR. However, since we merge PRs by either squashing or rebasing them, those merge commits disappear from the series of commits which ultimately ends up in the target branch.\nSquash/amend commits into discrete steps Kubernetes recommends squashing commits using these guidelines.\nAfter a review, prepare your PR for merging by squashing your commits.\nAll commits left on your branch after a review should represent meaningful milestones or units of work. Use commits to add clarity to the development and review process. Keep in mind that smaller commits are easier to review.\nBefore merging a PR, squash the following kinds of commits:\n Fixes/review feedback Typos Merges and rebases Work in progress Aim to have every commit in a PR compile and pass tests independently if you can, but it\u0026rsquo;s not a requirement.  Address code review feedback with new commits When addressing review comments, as a general rule, push a new commit instead of amending to the prior commit as the former makes it easy for reviewers to determine what changed.\nTo avoid cluttering the git log, squash the review commits into the appropriate commit before merging. The committer can do this in GitHub via the \u0026ldquo;Squash and merge\u0026rdquo; option. However you may want to preserve other commits, in which case squashing will need to be done manually via the Git CLI. To make that simpler, you can commit the review-prompted changes with git commit --fixup with the appropriate commit hash. This will keep them as separate commits, and if you later rebase with the --autosquash option (that is git rebase --autosquash -i) they will automatically be selected for squashing.\nCommit message formatting Kubernetes recommends these commit message practices.\nIn summary:\n Separate subject from body with a blank line Limit the subject line to 50 characters Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs how  GitLint will automatically be run against all commits to try to validate these conventions.\nDismiss reviews after substantial changes If a PR is substantially changed after a code review, the author should dismiss the stale reviews.\nWith the current GitHub configuration, reviews are not automatically dismissed when PRs are updated. This is to cause less drag for the typical cases, like minor merge conflicts. As Submariner grows, it might make sense to trade this low-drag solution for one where only exactly the reviewed code can be merged.\nAddress all -1s before merging If someone requests changes (\u0026ldquo;votes -1\u0026rdquo;) for a PR, a best-effort should be made to address those concerns and achieve a neutral position or approval (0/+1 vote) before the PR is merged.\nUpdate branch only after required reviews To avoid wasting resources by running unnecessary jobs, only use the Update branch button to add a merge commit once a PR is actually ready to merge (has required reviews and no -1s). Unless other relevant code has changed, the new job results don\u0026rsquo;t tell us anything new. Since changes are constantly being merged, it\u0026rsquo;s likely another merge commit and set of jobs will be necessary right before merging anyway.\nMark work-in-progress PRs as drafts To clearly indicate a PR is still under development and not yet ready for review, mark it as a draft. It is not necessary to modify PR summaries or commit messages (e.g. \u0026ldquo;WIP\u0026rdquo;, \u0026ldquo;DO NOT MERGE\u0026rdquo;). Keeping the same PR summary keeps email notifications threaded, and using the commit message you plan to merge will allow gitlint to verify it. PRs should typically be marked as drafts if any CI is failing that the author can fix before asking for code review.\nPlease do this when opening the PR: instead of clicking on the “Create pull request” button, click on the drop-down arrow next to it, and select “Create draft pull request”. This will avoid notifying code owners; they will be notified when the PR is marked as ready for review.\nUse private forks for debugging PRs by running CI If a PR is not expected to pass CI but the author wants to see the results to enable development, use a personal fork to run CI. This avoids clogging the GitHub Actions job queue of the Submariner-io GitHub Organization. After the same git push to your personal fork you\u0026rsquo;d typically do for a PR, simply choose your fork as the \u0026ldquo;base repository\u0026rdquo; of the PR in GitHub\u0026rsquo;s \u0026ldquo;Open a pull request\u0026rdquo; UI. Make sure your fork\u0026rsquo;s main branch is up-to-date. After creating the PR, CI will trigger as usual but the jobs will count towards your personal queue. You will need to open a new PR against the main repository once your proposed change is ready for review.\n"
},
{
	"uri": "/development/shipyard/advanced/",
	"title": "Advanced Features",
	"tags": [],
	"description": "",
	"content": "Shipyard has many advanced features to use in consuming projects.\nTo utilize an advanced feature in a project consuming Shipyard, a good practice is to change the project\u0026rsquo;s Makefile to have the advanced logic that is always needed. Any variable functionality can then be passed as desired in the command line.\nImage Building Helper Script Shipyard ships an image building script build_image.sh which can be used to build the image(s) that you require. The script has built in caching capabilities to speed up local and pull request CI builds, by utilizing docker\u0026rsquo;s ability to reuse layers from a cached image.\nThe script accepts several flags:\n tag: The tag to set for the local image (defaults to dev). repo: The repo portion to use for the image name. image (-i): The image name itself. dockerfile (-f): The Dockerfile to build the image from. [no]cache: Turns the caching on (or off).  For example, to build the submariner image use:\n${SCRIPTS_DIR}/build_image.sh -i submariner -f package/Dockerfile Deployment Scripts Features Per Cluster Settings Shipyard supports specifying different settings for each deployed cluster. A default cluster_settings is supplied in the image, so any custom settings override those. The settings are sent to supporting scripts using a --cluster_settings flag.\nCurrently, the following settings are supported:\n  clusters: An array of the clusters to deploy.\nclusters=(cluster1,cluster2)   cluster_nodes: A map of cluster names to a space separated string, representing a list of nodes to deploy. Supported values are control-plane and worker.\ncluster_nodes[cluster1]=\u0026#34;control-plane worker\u0026#34; cluster_nodes[cluster2]=\u0026#34;control-plane worker worker\u0026#34;   cluster_subm: A map of cluster names to values specifying if Submariner should be installed. Set to true to have Submariner installed, or to false to skip the installation.\ncluster_subm[cluster1]=\u0026#34;false\u0026#34; cluster_subm[cluster2]=\u0026#34;true\u0026#34;   Example: Custom Per Cluster Settings As an example, in order to customize the clusters to have two workers, and no submariner on the 1st cluster, create a cluster_settings file in the project:\ncluster_nodes[\u0026#39;cluster1\u0026#39;]=\u0026#34;control-plane\u0026#34; cluster_nodes[\u0026#39;cluster2\u0026#39;]=\u0026#34;control-plane worker worker\u0026#34; cluster_nodes[\u0026#39;cluster3\u0026#39;]=\u0026#34;control-plane worker worker\u0026#34; cluster_subm[\u0026#39;cluster1\u0026#39;]=\u0026#34;false\u0026#34; Then, to apply these settings, add this snippet to the Makefile:\nCLUSTER_SETTINGS_FLAG = --cluster_settings $(DAPPER_SOURCE)/path/to/cluster_settings CLUSTERS_ARGS += $(CLUSTER_SETTINGS_FLAG) DEPLOY_ARGS += $(CLUSTER_SETTINGS_FLAG) The path to cluster_settings should be specified relative to the project root; this ends up available in the build container in the directory referenced by $DAPPER_SOURCE.\nClusters Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make clusters via a make variable CLUSTERS_ARGS (or an environment variable with the same name). These flags affect how the clusters are deployed (and possibly influence how Submariner works).\nFlags of note:\n  k8s_version: Allows to specify the Kubernetes version that kind will deploy. Available versions can be found here.\nmake clusters CLUSTERS_ARGS=\u0026#39;--k8s_version 1.18.0\u0026#39;   globalnet: When set, deploys the clusters with overlapping Pod \u0026amp; Service CIDRs to simulate this scenario.\nmake clusters CLUSTERS_ARGS=\u0026#39;--globalnet\u0026#39;   Submariner Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make deploy via a make variable DEPLOY_ARGS (or an environment variable with the same name). These flags affect how Submariner is deployed on the clusters.\nSince deploy relies on clusters then effectively you could also specify CLUSTERS_ARGS to control the cluster deployment (provided the cluster hasn\u0026rsquo;t been deployed yet).\nFlags of note:\n  deploytool: Specifies the deployment tool to use: operator (default) or helm.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator\u0026#39;   deploytool_broker_args: Any extra arguments to pass to the deploy tool when deploying the broker.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_broker_args \u0026#34;--service-discovery\u0026#34;\u0026#39;   deploytool_submariner_args: Any extra arguments to pass to the deploy tool when deploying Submariner.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_submariner_args \u0026#34;--cable-driver wireguard\u0026#34;\u0026#39;   globalnet: When set, deploys Submariner with the globalnet controller, and assigns a unique Global CIDR to each cluster.\nmake deploy DEPLOY_ARGS=\u0026#39;--globalnet\u0026#39;   Example: Passing Deployment Variables As an example, in order to deploy with Lighthouse and support both Operator and Helm deployments, one can add this snippet to the Makefile:\nifeq ($(deploytool),operator) DEPLOY_ARGS += --deploytool operator --deploytool_broker_args \u0026#39;--service-discovery\u0026#39; else DEPLOY_ARGS += --deploytool helm --deploytool_broker_args \u0026#39;--set submariner.serviceDiscovery=true\u0026#39; --deploytool_submariner_args \u0026#39;--set submariner.serviceDiscovery=true,lighthouse.image.repository=localhost:5000/lighthouse-agent,serviceAccounts.lighthouse.create=true\u0026#39; endif In such a case, the call to deploy the environment would look like this:\nmake deploy [deploytool=operator] Note that deploytool is a variable used to determine the tool to use, but isn\u0026rsquo;t passed to or used by Shipyard.\n"
},
{
	"uri": "/getting-started/quickstart/openshift/aws/",
	"title": "On AWS",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two OpenShift Container Platform (OCP) clusters on AWS with full stack automation, also known as installer-provisioned infrastructure (IPI). Once the OpenShift clusters are deployed, we deploy Submariner with Service Discovery to interconnect the two clusters. Note that this guide focuses on Submariner deployment on clusters with non-overlapping Pod and Service CIDRs. For connecting clusters with overlapping CIDRs, please refer to the Submariner with Globalnet guide.\nPrerequisites Before we begin, the following tools need to be downloaded and added to your $PATH:\n OpenShift installer, pull secret, and command line interface. All can be downloaded from here. AWS CLI which can be downloaded from here.  Please ensure that the tools you downloaded above are compatible with your OpenShift Container Platform version. For more information, please refer to the official OpenShift documentation.\n  Setup Your AWS Profile Configure the AWS CLI with the settings required to interact with AWS. These include your security credentials, the default AWS Region, and the default output format:\n$ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text  Create and Deploy cluster-a In this step you will deploy cluster-a using the default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nCreate and Deploy cluster-b In this step you will deploy cluster-b, modifying the default IP CIDRs to avoid IP address conflicts with cluster-a. You can change the IP addresses block and prefix based on your requirements. For more information on IPv4 CIDR conversion, please check this page.\nIn this example, we will use the following IP ranges:\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the Pod network CIDR from 10.128.0.0/14 to 10.132.0.0/14:\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the Service network CIDR from 172.30.0.0/16 to 172.31.0.0/16:\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy the cluster:\nopenshift-install create cluster --dir cluster-b When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nPrepare AWS Clusters for Submariner Submariner Gateway nodes need to be able to accept traffic over UDP ports (4500 and 500 by default) when using IPsec. Submariner also uses UDP port 4800 to encapsulate traffic from the worker and master nodes to the Gateway nodes, and TCP port 8080 to retrieve metrics from the Gateway nodes. Additionally, the default OpenShift deployment does not allow assigning an elastic public IP to existing worker nodes, which may be necessary on one end of the IPsec connection.\nprep_for_subm is a script designed to update your OpenShift installer provisioned AWS infrastructure for Submariner deployments, handling the requirements specified above.\n Download the prep_for_subm.sh script and set permissions:  curl https://raw.githubusercontent.com/submariner-io/submariner/devel/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh  Please note that oc, aws-cli, terraform, and wget need to be installed before the prep_for_subm.sh script can be run. Also note that the script is known to be working with Terraform version 0.12. Maximum compatible version is 0.12.12.\n The script deploys an m5n.large EC2 instance type by default, optimized for improved network throughput and packet rate performance, for the Submariner gateway node. Please ensure that the AWS Region you deploy to supports this instance type. Alternatively, you can customize the AWS instance type as shown below.\n  Run the prep_for_subm.sh script for cluster-a and cluster-b:  ./prep_for_subm.sh cluster-a # respond \u0026#34;yes\u0026#34; when Terraform asks for approval, or otherwise add the -auto-approve flag ./prep_for_subm.sh cluster-b # respond \u0026#34;yes\u0026#34; when Terraform asks for approval, or otherwise add the -auto-approve flag Note that certain parameters, such as the IPsec UDP ports and AWS instance type for the gateway, can be customized before running the script. For example:\nexport IPSEC_NATT_PORT=4501 export IPSEC_IKE_PORT=501 export GW_INSTANCE_TYPE=m4.xlarge  Install subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Install Submariner with Service Discovery To install Submariner with multi-cluster Service Discovery follow the steps below:\nUse cluster-a as Broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the Broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-b.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the Headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification This will perform automated verifications between the clusters.\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/community/contributor-roles/",
	"title": "Contributor Roles",
	"tags": [],
	"description": "",
	"content": "This is a stripped-down version of the Kubernetes Community Membership process.\nAlthough we aspire to follow the Kubernetes process, some parts are not currently relevant to our structure or possible with our tooling:\n The SIG and subproject abstraction layers don\u0026rsquo;t apply to Submariner. Submariner is treated as a single project with file-based commit rights, not a \u0026ldquo;project\u0026rdquo; per repository. We hope to eventually move to Kubernetes OWNERS and Prow, but until we do so we can\u0026rsquo;t support advanced role-based automation (reviewers vs approvers; PR workflow commands like /okay-to-test, /lgtm, /approved).   This doc outlines the various responsibilities of contributor roles in Submariner.\n   Role Responsibilities Requirements Defined by     Member Active contributor in the community Sponsored by 2 committers, multiple contributions to the project Submariner GitHub org member   Committer Approve contributions from other members History of review and authorship CODEOWNERS file entry   Owner Set direction and priorities for the project Demonstrated responsibility and excellent technical judgement for the project Submariner-owners GitHub team member and *entry in all CODEOWNERS files    New Contributors New contributors should be welcomed to the community by existing members, helped with PR workflow, and directed to relevant documentation and communication channels.\nWe require every contributor to certify that they are legally permitted to contribute to our project. A contributor expresses this by consciously signing their commits, and by this act expressing that they comply with the Developer Certificate Of Origin.\nEstablished Community Members Established community members are expected to demonstrate their adherence to the principles in this document, familiarity with project organization, roles, policies, procedures, conventions, etc., and technical and/or writing ability. Role-specific expectations, responsibilities, and requirements are enumerated below.\nMember Members are continuously active contributors in the community. They can have issues and PRs assigned to them and participate through GitHub teams. Members are expected to remain active contributors to the community.\nDefined by: Member of the Submariner GitHub organization.\nMember Requirements  Enabled two-factor authentication on their GitHub account Have made multiple contributions to the project or community. Contribution may include, but is not limited to:  Authoring or reviewing PRs on GitHub Filing or commenting on issues on GitHub Contributing to community discussions (e.g. meetings, Slack, email discussion forums, Stack Overflow)   Subscribed to submariner-dev@googlegroups.com Have read the community and development guides Actively contributing Sponsored by 2 committers. Note the following requirements for sponsors:  Sponsors must have close interactions with the prospective member - e.g. code/design/proposal review, coordinating on issues, etc. Sponsors must be committers in at least 1 CODEOWNERS file either in any repo in the Submariner org   Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the member template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Member Responsibilities and Privileges  Responsive to issues and PRs assigned to them Responsive to mentions of teams they are members of Active owner of code they have contributed (unless ownership is explicitly transferred)  Code is well tested Tests consistently pass Addresses bugs or issues discovered after code is accepted   They can be assigned to issues and PRs, and people can ask members for reviews  Note: Members who frequently contribute code are expected to proactively perform code reviews and work towards becoming a committer.\nCommitters Committers are able to review code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles.\nUntil automation supports approvers vs reviewers: They also review for holistic acceptance of a contribution including: backwards / forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, etc.\nDefined by: Entry in an CODEOWNERS file in a repo owned by the Submariner project.\nCommitter status is scoped to a part of the codebase.\nCommitter Requirements The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Member for at least 3 months Primary reviewer for at least 5 PRs to the codebase Reviewed at least 20 substantial PRs to the codebase Knowledgeable about the codebase Sponsored by two committers or project owners  With no objections from other committers or project owners   May either self-nominate or be nominated by a committer/owner Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the committer template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers/owners reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Committer Responsibilities and Privileges The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Responsible for project quality control via code reviews  Focus on code quality and correctness, including testing and factoring Until automation supports approvers vs reviewers: Focus on holistic acceptance of contribution such as dependencies with other features, backwards / forwards compatibility, API and flag definitions, etc   Expected to be responsive to review requests as per community expectations Assigned PRs to review related to project of expertise Assigned test bugs related to project of expertise Granted \u0026ldquo;read access\u0026rdquo; to submariner repo May get a badge on PR and issue comments Demonstrate sound technical judgement Mentor contributors and reviewers  Project Owner Project owners are the technical authority for the Submariner project. They MUST have demonstrated both good judgement and responsibility towards the health the project. Project owners MUST set technical direction and make or approve design decisions for the project - either directly or through delegation of these responsibilities.\nDefined by: Member of the submariner-owners GitHub team and *entry in all CODEOWNERS files.\nOwner Requirements Unlike the roles outlined above, the owners of the project are typically limited to a relatively small group of decision makers and updated as fits the needs of the project.\nThe following apply to people who would be an owner:\n Deep understanding of the technical goals and direction of the project Deep understanding of the technical domain of the project Sustained contributions to design and direction by doing all of:  Authoring and reviewing proposals Initiating, contributing and resolving discussions (emails, GitHub issues, meetings) Identifying subtle or complex issues in designs and implementation PRs   Directly contributed to the project through implementation and / or review  Owner Responsibilities and Privileges The following apply to people who would be an owner:\n Make and approve technical design decisions for the project Set technical direction and priorities for the project Define milestones and releases Mentor and guide committers and contributors to the project Ensure continued health of project  Adequate test coverage to confidently release Tests are passing reliably (i.e. not flaky) and are fixed when they fail   Ensure a healthy process for discussion and decision making is in place Work with other project owners to maintain the project\u0026rsquo;s overall health and success holistically  "
},
{
	"uri": "/operations/deployment/helm/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": "Deploying with Helm Installing Helm The latest Submariner charts require Helm 3; once you have that, run\nexport KUBECONFIG=\u0026lt;kubeconfig-of-broker\u0026gt; helm repo add submariner-latest https://submariner-io.github.io/submariner-charts/charts Exporting environment variables needed later export BROKER_NS=submariner-k8s-broker export SUBMARINER_NS=submariner-operator export SUBMARINER_PSK=$(LC_CTYPE=C tr -dc \u0026#39;a-zA-Z0-9\u0026#39; \u0026lt; /dev/urandom | fold -w 64 | head -n 1) Deploying the Broker helm install \u0026#34;${BROKER_NS}\u0026#34; submariner-latest/submariner-k8s-broker \\  --create-namespace \\  --namespace \u0026#34;${BROKER_NS}\u0026#34; \\  --set submariner.serviceDiscovery=true Setup more environment variables we will need later for joining clusters.\nexport SUBMARINER_BROKER_CA=$(kubectl -n \u0026#34;${BROKER_NS}\u0026#34; get secrets \\  -o jsonpath=\u0026#34;{.items[?(@.metadata.annotations[\u0026#39;kubernetes\\.io/service-account\\.name\u0026#39;]==\u0026#39;${BROKER_NS}-client\u0026#39;)].data[\u0026#39;ca\\.crt\u0026#39;]}\u0026#34;) export SUBMARINER_BROKER_TOKEN=$(kubectl -n \u0026#34;${BROKER_NS}\u0026#34; get secrets \\  -o jsonpath=\u0026#34;{.items[?(@.metadata.annotations[\u0026#39;kubernetes\\.io/service-account\\.name\u0026#39;]==\u0026#39;${BROKER_NS}-client\u0026#39;)].data.token}\u0026#34; \\  | base64 --decode) export SUBMARINER_BROKER_URL=$(kubectl -n default get endpoints kubernetes \\  -o jsonpath=\u0026#34;{.subsets[0].addresses[0].ip}:{.subsets[0].ports[?(@.name==\u0026#39;https\u0026#39;)].port}\u0026#34;) Joining a cluster This step needs to be repeated for every cluster you want to connect with Submariner.\nexport KUBECONFIG=kubeconfig-of-the-cluster-to-join export CLUSTER_ID=the-id-of-the-cluster export CLUSTER_CIDR=x.x.x.x/x # the cluster\u0026#39;s Pod IP CIDR export SERVICE_CIDR=x.x.x.x/x # the cluster\u0026#39;s Service IP CIDR If your clusters have overlapping IPs (Cluster/Service CIDRs), please set:\nexport GLOBALNET=true export GLOBAL_CIDR=169.254.x.x/x # using an individual non-overlapping # range for each cluster you join. Joining the cluster:\nhelm install submariner-operator submariner-latest/submariner-operator \\  --create-namespace \\  --namespace \u0026#34;${SUBMARINER_NS}\u0026#34; \\  --set ipsec.psk=\u0026#34;${SUBMARINER_PSK}\u0026#34; \\  --set broker.server=\u0026#34;${SUBMARINER_BROKER_URL}\u0026#34; \\  --set broker.token=\u0026#34;${SUBMARINER_BROKER_TOKEN}\u0026#34; \\  --set broker.namespace=\u0026#34;${BROKER_NS}\u0026#34; \\  --set broker.ca=\u0026#34;${SUBMARINER_BROKER_CA}\u0026#34; \\  --set submariner.cableDriver=libreswan \\ # or wireguard --set submariner.clusterId=\u0026#34;${CLUSTER_ID}\u0026#34; \\  --set submariner.clusterCidr=\u0026#34;${CLUSTER_CIDR}\u0026#34; \\  --set submariner.serviceCidr=\u0026#34;${SERVICE_CIDR}\u0026#34; \\  --set submariner.globalCidr=\u0026#34;${GLOBAL_CIDR}\u0026#34; \\  --set serviceAccounts.globalnet.create=\u0026#34;${GLOBALNET}\u0026#34; \\  --set submariner.natEnabled=\u0026#34;true\u0026#34; \\  # disable this if no NAT will happen between gateways --set brokercrd.create=false \\  --set submariner.serviceDiscovery=true \\  --set serviceAccounts.lighthouse.create=true Some image override settings you could use\n--set operator.image.repository=\u0026#34;localhost:5000/submariner-operator\u0026#34; \\  --set operator.image.tag=\u0026#34;local\u0026#34; \\  --set operator.image.pullPolicy=\u0026#34;IfNotPresent\u0026#34; If installing on OpenShift, please also add the Submariner service accounts (SAs) to the privileged Security Context Constraint.\noc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-routeagent oc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-engine Perform automated verification Automated verification of the deployment can be performed by using the verification tests embedded in the subctl command line tool via the subctl verify command.\nInstall subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Run the verification subctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/getting-started/quickstart/managed-kubernetes/",
	"title": "Managed Kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/getting-started/quickstart/",
	"title": "Quickstart Guides",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/getting-started/architecture/gateway-engine/",
	"title": "Gateway Engine",
	"tags": [],
	"description": "",
	"content": "The Gateway Engine component is deployed in each participating cluster and is responsible for establishing secure tunnels to other clusters.\nThe Gateway Engine has a pluggable architecture for the cable engine component that maintains the tunnels. The following implementations are available:\n an IPsec implementation using Libreswan. This is currently the default. an implementation for WireGuard (via the wgctrl library).  The cable driver can be specified via the --cable-driver flag while joining a cluster using subctl. For more information, please refer to the subctl guide.\nWireGuard needs to be installed on Gateway nodes. See the WireGuard installation instructions.\n Instances of the Gateway Engine run on specifically designated nodes in a cluster of which there may be more than one for fault tolerance. Submariner supports active/passive High Availability for the Gateway Engine component, which means that there is only one active Gateway Engine instance at a time in a cluster. They perform a leader election process to determine the active instance and the others await in standby mode ready to take over should the active instance fail.\nThe Gateway Engine is deployed as a DaemonSet that is configured to only run on nodes labelled with \u0026ldquo;submariner.io/gateway=true\u0026rdquo;.\n The active Gateway Engine communicates with the central Broker to advertise its Endpoint and Cluster resources to the other clusters connected to the Broker, also ensuring that it is the sole Endpoint for its cluster. The Route Agent Pods running in the cluster learn about the local Endpoint and setup the necessary infrastructure to route cross-cluster traffic from all nodes to the active Gateway Engine node. The active Gateway Engine also establishes a watch on the Broker to learn about the active Endpoint and Cluster resources advertised by the other clusters. Once two clusters are aware of each other\u0026rsquo;s Endpoints, they can establish a secure tunnel through which traffic can be routed.\nGateway Failover If the active Gateway Engine fails, another Gateway Engine on one of the other designated nodes will gain leadership and perform reconciliation to advertise its Endpoint and to ensure that it is the sole Endpoint. The remote clusters will learn of the new Endpoint via the Broker and establish a new tunnel. Similarly, the Route Agent Pods running in the local cluster automatically update the route tables on each node to point to the new active Gateway node in the cluster.\nThe impact on datapath for various scenarios in a kind setup are captured in the following spreadsheet.\nGateway Health Check The Gateway Engine continuously monitors the health of connected clusters. It periodically pings each cluster and collects statistics including basic connectivity, round trip time (RTT) and average latency. This information is updated in the Gateway resource. Whenever the Gateway Engine detects that a ping to a particular cluster has failed, its connection status is marked with an error state. Service Discovery uses this information to avoid unhealthy clusters during Service discovery.\nThe health checking feature can be enabled/disabled via an option on the subctl join command.\n"
},
{
	"uri": "/community/",
	"title": "Community",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/operations/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Overview You have followed steps in Deployment but something has gone wrong. You\u0026rsquo;re not sure what and how to fix it, or what information to collect to raise an issue. Welcome to the Submariner troubleshooting guide where we will help you get your deployment working again.\nBasic familiarity with the Submariner components and architecture will be helpful when troubleshooting so please review the Architecture section.\nThe guide has been broken into different sections for easy navigation.\nPre-requisite Before we begin troubleshooting, run subctl version to obtain which version of the Submariner components you are running.\nRun kubectl get services -n \u0026lt;service-namespace\u0026gt; | grep \u0026lt;service-name\u0026gt; to get information about the service you\u0026rsquo;re trying to access. This will provide you with the Service Name, Namespace and ServiceIP. If Globalnet is enabled, you will also need the globalIp of the service by running\nkubectl get service \u0026lt;service-name\u0026gt; -o jsonpath='{.metadata.annotations.submariner\\.io/globalIp}'\nConnectivity Issues Submariner deployment completed successfully but Services/Pods on one cluster are unable to connect to Services on another cluster. This can be due to multiple factors outlined below.\nCheck the Connection Statistics If you are unable to connect to a remote cluster, check its connection status in the Gateway resource.\nkubectl describe Gateway -n submariner-operator\nSample output:\n- endpoint: backend: libreswan cable_name: submariner-cable-cluster1-172-17-0-7 cluster_id: cluster1 healthCheckIP: 10.1.128.0 hostname: cluster1-worker nat_enabled: false private_ip: 172.17.0.7 public_ip: \u0026#34;\u0026#34; subnets: - 100.1.0.0/16 - 10.1.0.0/16 latencyRTT: average: 447.358µs last: 281.577µs max: 5.80437ms min: 158.725µs stdDev: 364.154µs status: connected statusMessage: Connected to 172.17.0.7:4500 - encryption alg=AES_GCM_16, keysize=128 rekey-time=13444  This feature is only available for non-Globalnet deployments at the moment.\n The Gateway Engine uses the Health Check IP of the endpoint to verify connectivity. The connection Status will be marked as error, if it cannot reach this IP, and the Status Message will provide more information about the possible failure reason. It also provides the statistics for the connection.\nService Discovery Issues If you are able to connect to remote service by using ServiceIP or globalIp, but not by service name, it is a Service Discovery Issue.\nService Discovery not working This is good time to familiarize yourself with Service Discovery Architecture if you haven\u0026rsquo;t already.\nCheck ServiceExport for your Service For a Service to be accessible across clusters, you must first export the Service via subctl which creates a ServiceExport resource. Ensure the ServiceExport resource exists and check if its status condition indicates `Exported\u0026rsquo;. Otherwise, its status condition will indicate the reason it wasn\u0026rsquo;t exported.\nkubectl describe serviceexport -n \u0026lt;service-namespace\u0026gt; \u0026lt;service-name\u0026gt;\nNote that you can also use shorthand svcex for serviceexport and svcim for serviceimport.\nSample output:\nName: nginx-demo Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-11-25T06:21:01Z Generation: 1 Resource Version: 5254 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/default/serviceexports/nginx-demo UID: 77509e43-8fd1-4173-805c-e03c4581ebbf Status: Conditions: Last Transition Time: 2020-11-25T06:21:01Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-11-25T06:21:01Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; Check Lighthouse CoreDNS Service All cross-cluster service queries are handled by Lighthouse CoreDNS server. First we check if the Lighthouse CoreDNS Service is running properly.\nkubectl -n submariner-operator get service submariner-lighthouse-coredns\nIf it is running fine, note down the ServiceIP for the next steps. If not, check the logs for an error.\nIf the error is due to a wrong image, run kubectl -n submariner-operator get deployment submariner-lighthouse-coredns and make sure Image is set to quay.io/submariner/lighthouse-coredns:\u0026lt;version\u0026gt; and refers to the correct version.\nFor any other errors, capture the information and raise a new issue.\nIf there\u0026rsquo;s no error, then check if the Lighthouse CoreDNS server is configured correctly. Run kubectl -n submariner-operator describe configmap submariner-lighthouse-coredns and make sure it has following configuration:\nclusterset.local:53 { lighthouse errors health ready } Check CoreDNS Configuration Submariner requires the CoreDNS deployment to forward requests for the domain clusterset.local to the Lighthouse CoreDNS server in the cluster making the query. Ensure this configuration exists and is correct.\nFirst we check if CoreDNS is configured to forward requests for domain clusterset.local to Lighthouse CoreDNS Server in the cluster making the query.\nkubectl -n kube-system describe configmap coredns\nIn the output look for something like this:\nclusterset.local:53 { forward . \u0026lt;lighthouse-coredns-serviceip\u0026gt; ======\u0026gt; ServiceIP of lighthouse-coredns service as noted in previous section } If the entries highlighted above are missing or ServiceIp is incorrect, it means CoreDNS wasn\u0026rsquo;t configured correctly. It can be fixed by running kubectl edit configmap coredns and making the changes manually. You may need to repeat this step on every cluster.\nCheck submariner-lighthouse-agent Next we check if the submariner-lighthouse-agent is properly running. Run kubectl -n submariner-operator get pods submariner-lighthouse-agent and check the status of Pods.\nIf the status indicates the ImagePullBackOff error, run kubectl -n submariner-operator describe deployment submariner-lighthouse-agent and check if Image is set correctly to quay.io/submariner/lighthouse-agent:\u0026lt;version\u0026gt;. If it is and the same error still occurs, raise an issue here or ping us on the community slack channel.\nIf the status indicates any other error, run kubectl -n submariner-operator get pods to get the name of the lighthouse-agent Pod. Then run kubectl -n submariner-operator logs \u0026lt;lighthouse-agent-pod-name\u0026gt; to get the logs. See if there are any errors in the log. If yes, raise an issue with the log contents, or you can continue reading through this guide to troubleshoot further.\nIf there are no errors, grep the log for the service name that you\u0026rsquo;re trying to query as we may need the log entries later for raising an issue.\nCheck ServiceImport resources If the steps above did not indicate an issue, next we check if the ServiceImport resources were properly created for the service you\u0026rsquo;re trying to access. The format of a ServiceImport resources\u0026rsquo;s name is as follows:\n\u0026lt;service-name\u0026gt;-\u0026lt;service-namespace\u0026gt;-\u0026lt;cluster-id\u0026gt;\nRun kubectl get serviceimports --all-namespaces |grep \u0026lt;your-service-name\u0026gt; on the Broker cluster to check if a resource was created for your service. If not, then check the Lighthouse Agent logs on the cluster where service was created and look for any error or warning messages indicating a failure to create the ServiceImport resource for your service. The most common error is Forbidden if the RBAC wasn\u0026rsquo;t configured correctly. Depending on the deployment method used, \u0026lsquo;subctl\u0026rsquo; or \u0026lsquo;helm\u0026rsquo;, it should\u0026rsquo;ve been done for you. Create an issue with relevant log entries.\nIf the ServiceImport resource was created correctly on the Broker cluster, the next step is to check if it exists on the cluster where you\u0026rsquo;re trying to access the service. Follow the same steps as earlier to get the list of the ServiceImport resources and check if the ServiceImport for your service exists. If not, check the logs of the Lighthouse Agent on the cluster where you are trying to access the service. As described earlier, it will most commonly be an issue with RBAC otherwise create an issue with relevant log entries.\nIf the ServiceImport resource was created properly on the cluster, run kubectl -n submariner-operator describe serviceimport \u0026lt;your-serviceimport-name\u0026gt; and check if it has the correct ClusterID and ServiceIP:\nName: nginx-demo-default-cluster2 Namespace: submariner-operator Labels: lighthouse.submariner.io/sourceCluster=cluster2 lighthouse.submariner.io/sourceName=nginx-demo lighthouse.submariner.io/sourceNamespace=default submariner-io/clusterID=cluster2 Annotations: cluster-ip: 100.2.33.171 origin-name: nginx-demo origin-namespace: default API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceImport Metadata: Creation Timestamp: 2020-11-25T06:21:02Z Generation: 1 Resource Version: 5312 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/submariner-operator/serviceimports/nginx-demo-default-cluster2 UID: a4c4abe0-1c84-4118-ae09-760b26f7fe3c Spec: Ips: 100.2.33.171 Ports: Session Affinity Config: Type: ClusterSetIP Events: \u0026lt;none\u0026gt; For headless Service, you need to check EndpointSlice resource.\nIf the data is not correct, you can manually edit the ServiceImport resource to set the correct IP as a workaround and create an issue with relevant information.\nIf the ServiceImport Ips are correct but still not being returned from DNS queries, check the connectivity to the cluster using subctl show endpoint. The Lighthouse CoreDNS Server only returns IPs from connected clusters.\nCheck EndpointSlice resources For a headless Service, next we check if the EndpointSlice resources were properly created for the service you\u0026rsquo;re trying to access. EndpointSlice resources are created in the same namespace as the source Service. The format of a EndpointSlice resource\u0026rsquo;s name is as follows:\n\u0026lt;service-name\u0026gt;--\u0026lt;cluster-id\u0026gt;\nRun kubectl get endpointslices --all-namespaces |grep \u0026lt;your-service-name\u0026gt; on the Broker cluster to check if a resource was created for your Service. If not, then check the Lighthouse Agent logs on the cluster where the Service was created and look for any error or warning messages indicating a failure to create the ServiceImport resource for your Service. The most common error is Forbidden if the RBAC wasn\u0026rsquo;t configured correctly. This is supposed to be done automatically during deployment so please file an issue with the relevant log entries.\nIf the EndpointSlice resource was created correctly on the Broker cluster, the next step is to check if it exists on the cluster where you\u0026rsquo;re trying to access the Service. Follow the same steps as earlier to get the list of the EndpointSlice resources and check if the EndpointSlice for the Service exists. If not, check the logs of the Lighthouse Agent on the cluster where you are trying to access the Service. As described earlier, it will most commonly be an issue with RBAC so create an issue with relevant log entries.\nIf the EndpointSlice resource was created properly on the cluster, run kubectl -n \u0026lt;your-service-namespace\u0026gt; describe endpointslice \u0026lt;your-endpointslice-name\u0026gt; and check if it has the correct endpoint addresses:\nName: nginx-ss-cluster2 Namespace: default Labels: endpointslice.kubernetes.io/managed-by=lighthouse-agent.submariner.io lighthouse.submariner.io/sourceCluster=cluster2 lighthouse.submariner.io/sourceName=nginx-ss lighthouse.submariner.io/sourceNamespace=default multicluster.kubernetes.io/service-name=nginx-ss-default-cluster2 Annotations: \u0026lt;none\u0026gt; AddressType: IPv4 Ports: Name Port Protocol ---- ---- -------- web 80 TCP Endpoints: - Addresses: 10.242.0.5 -----\u0026gt; Pod IP Conditions: Ready: true Hostname: web-0 -----\u0026gt; Pod hostname Topology: kubernetes.io/hostname=cluster2-worker2 - Addresses: 10.242.224.4 Conditions: Ready: true Hostname: web-1 Topology: kubernetes.io/hostname=cluster2-worker Events: \u0026lt;none\u0026gt; If the Addresses are correct but still not being returned from DNS queries, try querying IPs in a specific cluster by prefixing the query with \u0026lt;cluster-id\u0026gt;. If that returns the IPs correctly, then check the connectivity to the cluster using subctl show endpoint. The Lighthouse CoreDNS Server only returns IPs from connected clusters.\nFor errors querying specific Pods of a StatefulSet, check that the Hostname is correct for the endpoint.\nIf still not working, file an issue with relevant log entries.\n"
},
{
	"uri": "/getting-started/quickstart/openshift/globalnet/",
	"title": "On AWS with Globalnet",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two OpenShift Container Platform (OCP) clusters on AWS with full stack automation, also known as installer-provisioned infrastructure (IPI). Once the OpenShift clusters are deployed, we deploy Submariner to interconnect the two clusters. Since the two clusters share the same Cluster and Service CIDR ranges, Globalnet will be enabled.\nPrerequisites Before we begin, the following tools need to be downloaded and added to your $PATH:\n OpenShift installer, pull secret, and command line interface. All can be downloaded from here. AWS CLI which can be downloaded from here.  Please ensure that the tools you downloaded above are compatible with your OpenShift Container Platform version. For more information, please refer to the official OpenShift documentation.\n Setup Your AWS Profile Configure the AWS CLI with the settings required to interact with AWS. These include your security credentials, the default AWS Region, and the default output format:\n$ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text  Create and Deploy cluster-a In this step you will deploy cluster-a using the default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nCreate and Deploy cluster-b In this step you will deploy cluster-b using the same default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-b openshift-install create cluster --dir cluster-b When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nPrepare AWS Clusters for Submariner Submariner Gateway nodes need to be able to accept traffic over UDP ports (4500 and 500 by default) when using IPsec. Submariner also uses UDP port 4800 to encapsulate traffic from the worker and master nodes to the Gateway nodes, and TCP port 8080 to retrieve metrics from the Gateway nodes. Additionally, the default OpenShift deployment does not allow assigning an elastic public IP to existing worker nodes, which may be necessary on one end of the IPsec connection.\nprep_for_subm is a script designed to update your OpenShift installer provisioned AWS infrastructure for Submariner deployments, handling the requirements specified above.\n Download the prep_for_subm.sh script and set permissions:  curl https://raw.githubusercontent.com/submariner-io/submariner/devel/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh  Please note that oc, aws-cli, terraform, and wget need to be installed before the prep_for_subm.sh script can be run. Also note that the script is known to be working with Terraform version 0.12. Maximum compatible version is 0.12.12.\n The script deploys an m5n.large EC2 instance type by default, optimized for improved network throughput and packet rate performance, for the Submariner gateway node. Please ensure that the AWS Region you deploy to supports this instance type. Alternatively, you can customize the AWS instance type as shown below.\n  Run the prep_for_subm.sh script for cluster-a and cluster-b:  ./prep_for_subm.sh cluster-a # respond \u0026#34;yes\u0026#34; when Terraform asks for approval, or otherwise add the -auto-approve flag ./prep_for_subm.sh cluster-b # respond \u0026#34;yes\u0026#34; when Terraform asks for approval, or otherwise add the -auto-approve flag Note that certain parameters, such as the IPsec UDP ports and AWS instance type for the gateway, can be customized before running the script. For example:\nexport IPSEC_NATT_PORT=4501 export IPSEC_IKE_PORT=501 export GW_INSTANCE_TYPE=m4.xlarge  Install subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Install Submariner with Service Discovery and Globalnet To install Submariner with multi-cluster service discovery and support for overlapping CIDRs follow the steps below.\nUse cluster-a as Broker with service discovery and globalnet enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --service-discovery --globalnet Join cluster-a and cluster-b to the Broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-b.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the Headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification This will perform automated verifications between the clusters.\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/getting-started/quickstart/openshift/",
	"title": "OpenShift",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/development/website/",
	"title": "Contributing to the Website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on Hugo, Grav, Hugo-theme-learn, and is written in Markdown format.\nYou can always click the Edit this page link at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on GitHub.\n  Check out your copy locally:\ngit clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git cd submariner-website make server   An instance of the website is now running locally on your machine and is accessible at http://localhost:1313.\n  Edit files in src. The browser should automatically reload so you can view your changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the GitHub workflow here.\n  Your changes will be verified by CI. Check the job results for details of any errors.\n  "
},
{
	"uri": "/operations/deployment/calico/",
	"title": "Calico CNI",
	"tags": [],
	"description": "",
	"content": "Typically, the Kubernetes network plugin (based on kube-proxy) programs iptables rules for Pod networking within a cluster. When a Pod in a cluster tries to access an external IP, the plugin performs specific Network Address Translation (NAT) manipulation on the traffic as it does not belong to the local cluster. Similarly, Submariner also programs certain iptables rules and it requires these rules to be applied prior to the ones programmed by the network plugin. Submariner tries to preserve the source IP of the Pods for cross-cluster communication for visibility, ease of debugging, and security purposes.\nOn clusters deployed with Calico as the network plugin, the rules inserted by Calico take precedence over Submariner, causing issues with cross-cluster communication. To make Calico compatible with Submariner, it needs to be configured, via IPPools, not to perform NAT on the subnets associated with the Pod and Service CIDRs of the remote clusters. Once the IPPools are configured in the clusters, Calico will not perform NAT for the configured CIDRs and allows Submariner to support cross-cluster connectivity.\nWhen using Submariner Globalnet with Calico, please avoid the default globalnet-cidr (i.e., 169.254.0.0/16) as its used internally within Calico. You can explicitly specify a non-overlapping globalnet-cidr while deploying Submariner.\n As an example, consider two clusters, East and West, deployed with the Calico network plugin and connected via Submariner. For cluster East, the Service CIDR is 100.93.0.0/16 and the Pod CIDR is 10.243.0.0/16. For cluster West, they are 100.92.0.0/16 and 10.242.0.0/16. The following IPPools should be created:\nOn East Cluster:\n$ cat \u0026gt; svcwestcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: svcwestcluster spec: cidr: 100.92.0.0/16 natOutgoing: false disabled: true EOF cat \u0026gt; podwestcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: podwestcluster spec: cidr: 10.242.0.0/16 natOutgoing: false disabled: true EOF DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-eastcluster.yaml\u0026gt; calicoctl create -f svcwestcluster.yaml DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-eastcluster.yaml\u0026gt; calicoctl create -f podwestcluster.yaml On West Cluster:\ncat \u0026gt; svceastcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: svceastcluster spec: cidr: 100.93.0.0/16 natOutgoing: false disabled: true EOF cat \u0026gt; podeastcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: podeastcluster spec: cidr: 10.243.0.0/16 natOutgoing: false disabled: true EOF DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-westcluster.yaml\u0026gt; calicoctl create -f svceastcluster.yaml DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-westcluster.yaml\u0026gt; calicoctl create -f podeastcluster.yaml "
},
{
	"uri": "/community/getting-help/",
	"title": "Getting Help",
	"tags": [],
	"description": "",
	"content": "Talk to Us We would love to hear from you, how you are using Submariner, and what we can do to make it better.\nGitHub Project Check out the project and consider contributing. Pick up an issue to work on or propose an enhancement by reporting a new issue; once your code is ready to be reviewed, you can propose a pull request. You can find a good guide about the GitHub workflow here.\n#submariner Share your ideas in the #submariner channel in Kubernetes\u0026rsquo; Slack. If you need it, you can request an invite to Kubernetes Slack instance.\nCommunity Calendar As a member of the Submariner Community, join any of our community meetings - no registration required. The weekly Submariner Community Meeting (Tuesdays at 5:00pm CET) is a good place to start.\nMailing List Join the developer mailing list.\n"
},
{
	"uri": "/community/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.8.1  Submariner Gateway Health Check is now supported with Globalnet deployments. Support deploying OVN in kind using make clusters using=ovn for E2E testing and development environments. Support debugging the Libreswan cable driver. Fix the cable driver label in the Prometheus latency metrics. Support non-TLS connections for OVN databases. Services can now be recreated without needing to recreate their associated ServiceExport objects. Service Discovery no longer depends on Submariner-provided connectivity. Improved Service Discovery verification suite. The ServiceImport object now includes Port information from the original Service. subctl show now indicates when the target cluster doesn\u0026rsquo;t have Submariner installed.  v0.8.0  Added support for connecting clusters that use the OVNKubernetes CNI plugin in non-Globalnet deployments. Support for Globalnet will be available in a future release. The active Gateway now performs periodic health checks on the connections to remote clusters, updates the Gateway connection status, and adds latency statistics. Gateways now export the following connection metrics on TCP port 8080 which can be used with Prometheus. These are currently only supported for the Libreswan cable driver:  The count of bytes transmitted and received between Gateways. The number of connections between Gateways and their corresponding status. The timestamp of the last successful connection established between Gateways. The RTT latency between Gateways.   The Libreswan cable driver is now the default. The strongSwan cable driver is deprecated and will be removed in a future release. The Lighthouse DNS always returns the IP address of the local exported ClusterIP Service, if available, otherwise it load-balances between the same Services exported from other clusters in a round-robin fashion. Lighthouse has fully migrated to use the proposed Kubernetes Multicluster Services API (ServiceExport and ServiceImport). The Lighthouse-specific API is deprecated and will be removed in a future release. On upgrade from v0.7.0, exported Services will automatically be migrated to the new CRDs. Broker resiliency has been improved. The dataplane is no longer affected in any way if the Broker is unavailable. The subctl benchmark tests now accept a verbose flag to enable full logging. Otherwise only the results are presented.  v0.7.0 StatefulSet support for service discovery and benchmark tooling  This release mainly focused on adding support for StatefulSets in Lighthouse for service discovery and adding new subctl commands to benchmark the network performance across clusters.\n  Lighthouse enhancements/changes:  Added support for accessing individual Pods in a StatefulSet using their host names. A Service in a specific cluster can now be explicitly queried. Removed support for the supercluster.local domain to align with the Kubernetes MultiCluster Service API.   Added new subctl benchmark commands for measuring the throughput and round trip latency between two Pods in separate clusters or within the same cluster. The data path is no longer disrupted when the Globalnet Pod is restarted. The Route Agent component now runs on all worker nodes including those with taints.  When upgrading to 0.7.0 on a cluster already running Submariner, the current state must be cleared:\n Remove the Submariner namespaces: kubectl delete ns submariner-operator submariner-k8s-broker Remove the Submariner cluster roles: kubectl delete clusterroles submariner-lighthouse submariner-operator submariner-operator:globalnet  v0.6.0 Improved Submariner High Availability and various Lighthouse enhancements  This release mainly focused on support for headless Services in Lighthouse, as well as improving Submariner\u0026rsquo;s High Availability (HA).\n The DNS domains have been updated from \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.supercluster.local to \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local to align with the change in Kubernetes Multicluster Service API. Both domains will be supported for 0.6.0 but 0.7.0 will remove support for supercluster.local. Please update your deployments and applications.\n  Lighthouse has been enhanced to:  Be aware of the local cluster Gateway connectivity so as not to announce the IP addresses for disconnected remote clusters. Support headless Services for non-Globalnet deployments. Support for Globalnet will be available in a future release. Be aware of a Service\u0026rsquo;s backend Pods so as not to announce IP addresses for Services that have no active Pods. Use Round Robin IP resolution for Services available in multiple clusters. Enable service discovery by default for subctl deployments.   subctl auto-detects the cluster ID from the kubeconfig file\u0026rsquo;s information when possible. Submariner\u0026rsquo;s Pods now shut down gracefully and do proper cleanup which reduces downtime during Gateway failover. The Operator now automatically exports Prometheus metrics; these integrate seamlessly with OpenShift Prometheus if user workload monitoring is enabled, and can be included in any other Prometheus setup. Minimum Kubernetes version is now 1.17. HostNetwork to remote Service connectivity fixes for AWS clusters. The project\u0026rsquo;s codebase quality and readability has been improved using various linters.  v0.5.0 Lighthouse service discovery alignment  This release mainly focused on continuing the alignment of Lighthouse\u0026rsquo;s service discovery support with the Kubernetes Multicluster Services KEP.\n  Lighthouse has been modified per the Kubernetes Multicluster Services KEP as follows:  The MultiClusterService resource has been replaced by ServiceImport. The ServiceExport resource is now updated with status information as lifecycle events occur.   Lighthouse now allows a ServiceExport resource to be created prior to the associated Service. Network discovery was moved from subctl to the Submariner Operator. Several new commands were added to subctl: export service, show versions, show connections, show networks, show endpoints, and show gateways. The subctl info command has been removed in lieu of the new show networks command. The Globalnet configuration has been moved from the broker-info.subm file to a ConfigMap resource stored on the Broker cluster. Therefore, the new subctl cannot be used on brownfield Globalnet deployments where this information was stored as part of broker-info.subm. subctl now supports joining multiple clusters in parallel without having to explicitly specify the globalnet-cidr for the cluster to work around this issue. The globalnet-cidr will automatically be allocated by subctl for each cluster. The separate --operator-image parameter has been removed from subctl join and the --repository and --version parameters are now used for all images. The Submariner Operator status now includes Gateway information. Closed technical requirements for Submariner to become a CNCF project, including Developer Certificate of Origin compliance and additional source code linting.  v0.4.0 Libreswan cable driver, Kubernetes multicluster service discovery  This release is mainly focused on Submariner\u0026rsquo;s Libreswan cable driver implementation, as well as standardizing Lighthouse\u0026rsquo;s service discovery support with the Kubernetes Multicluster Services KEP.\n  Libreswan IPsec cable driver is available for testing and is covered in Submariner\u0026rsquo;s CI. Lighthouse has been modified per the Kubernetes Multicluster Services KEP as follows:  A ServiceExport object needs to be created alongside any Service that is intended to be exported to participant clusters. Supercluster services can be accessed with \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local.   Globalnet overlapping CIDR support improvements and bug fixes. Multiple CI improvements implemented from Shipyard. CI tests are now run via GitHub Actions. Submariner\u0026rsquo;s Operator now completely handles the Lighthouse deployment via the ServiceDiscovery CRD. subctl verify is now available for connectivity, service-discovery and gateway-failover.  v0.3.0 Lighthouse Service Discovery without KubeFed  This release is focused on removing the KubeFed dependency from Lighthouse, improving the user experience, and adding experimental WireGuard support as an alternative to IPsec.\n  Lighthouse no longer depends on KubeFed. All metadata exchange is handled over the Broker as MultiClusterService CRs. Experimental WireGuard support has been added as a pluggable CableDriver option in addition to the current default IPsec. Submariner reports the active and passive gateways as a gateway.submariner.io resource. The Submariner Operator reports a detailed status of the deployment. The gateway redundancy/failover tests are now enabled and stable in CI. Globalnet hostNetwork to remote globalIP is now supported. Previously, when a Pod used hostNetworking it was unable to connect to a remote Service via globalIP. A GlobalCIDR can be manually specified when joining a cluster with Globalnet enabled. This enables CI speed optimizations via better parallelism. Operator and subctl are more robust via standard retries on updates. subctl creates a new individual access token for every new joined cluster.  v0.2.0 Overlapping CIDR support  This release is focused on overlapping CIDR support between clusters.\n  Support for overlapping CIDRs between clusters (Globalnet). Enhanced end-to-end scripts, which will be shared between repositories in the Shipyard project (ongoing work). Improved end-to-end deployment by using a local registry. Refactoring to support pluggable drivers (in preparation for WireGuard support).  v0.1.1 Submariner with more light  This release is focused on stability for Lighthouse.\n  Cleaner logging for submariner-engine. Cleaner logging for submariner-route-agent. Fixed issue with wrong token stored in .subm file (submariner-operator#244). Added flag to disable the OpenShift CVO (submariner-operator#235). Fixed several service discovery bugs (submariner-operator#194, submariner-operator#167). Fixed several panics on nil network discovery. Added checks to ensure the CIDRs for joining cluster don\u0026rsquo;t overlap with existing ones. Fix context handling related to service discovery/KubeFed (submariner-operator#180). Use the correct CoreDNS image for OpenShift.  v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making Lighthouse available as a developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (submariner#346, submariner#348, submariner#332). Migrated to DaemonSets for Submariner gateway deployment. Added support for hostNetwork to remote Pod/Service connectivity (submariner#298). Auto detection and configuration of MTU for vx-submariner, jumbo frames support (submariner#301). Support for updated strongSwan (submariner#288). Better iptables detection for some hosts (submariner#227).   subctl and the Submariner Operator have the following improvements:\n  Support for verify-connectivity checks between two connected clusters. Deployment of Submariner gateways based on DaemonSet instead of Deployment. Rename submariner Pods to submariner-gateway Pods for clarity. Print version details on crash (subctl). Stop storing IPsec key on Broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file. Version command for subctl. Nicer spinners during deployment (thanks to kind).  v0.0.3 \u0026ndash; KubeCon NA 2019 Submariner has been greatly enhanced to allow administrators to deploy into Kubernetes clusters without the necessity for Layer 2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). subctl was created to make deployment of Submariner easier.\nv0.0.2 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/operations/known-issues/",
	"title": "Known Issues",
	"tags": [],
	"description": "",
	"content": "General  Minimum supported Kubernetes version is 1.17. Submariner only supports kube-proxy in iptables mode. IPVS is not supported at this time. CoreDNS is supported out of the box for *.clusterset.local service discovery. KubeDNS needs manual configuration. Please refer to the GKE Quickstart Guide for more information. Clusters deployed with the Calico network plug-in require further configuration to be compatible with Submariner. Please refer to the Calico-specific deployment instructions. Within a cluster set, all the member clusters should be using the same IPsec IKE and NAT-T UDP ports. Currently, Submariner does not support using custom ikeport and nattport on a per connection basis.  Globalnet  Globalnet only supports Pod to remote Service connectivity using Global IPs. Pod to Pod connectivity is not supported at this time. Globalnet is not compatible with Headless Services. Only ClusterIP Services are supported at this time. Currently, Globalnet is not supported with the OVN network plug-in. The subctl benchmark latency command is not compatible with Globalnet deployments at this time.  "
},
{
	"uri": "/development/release-process/",
	"title": "Release Process",
	"tags": [],
	"description": "",
	"content": "This section describes how to create a Submariner product release. It is assumed that you are familiar with the various Submariner projects and their repositories and are familiar with Git and GitHub.\nProject dependencies The Submariner projects have a dependency hierarchy with respect to their Go libraries and Docker images. Therefore the releases for each individual project must be created in a specific order.\nThe Go dependency hierarchy flow is as follows:\nshipyard \u0026lt;- admiral \u0026lt;- [submariner, lighthouse] \u0026lt;- submariner-operator\nNote that the submariner and lighthouse projects are siblings and thus do not depend on one another. Also the submariner-operator components expect that lighthouse and submariner are aligned on the same exact version.\nThe Docker image dependency hierarchy flow is as follows:\nsubctl binary \u0026lt;- shipyard dapper base image \u0026lt;- [admiral, submariner, lighthouse, submariner-operator]\nThe Dapper base image that is provided by shipyard for building and E2E tests in all of the other projects pulls in the subctl binary.\nRelease versions The vx.x.x version format is used for the git projects while x.x.x is used for Docker images.\nThe typical workflow is to first create release candidate(s) for testing before creating the final release. The suggested naming convention is to append -rcN to the final version, for example v0.8.0-rc0, v0.8.0-rc1 etc.\nSometimes you may want to create a specific project release for testing prior to creating a release candidate. In this case, the suggested naming convention is to append -preN.\nStable branches and versions Stable branches will be maintained for major and minor versions. For example, versions v0.8.0, v0.8.1 and any other 0.8.x releases will all be maintained and released on the release-0.8 branch.\nThe automated release process described here can also create these stable branches when necessary. The branch should be specified in the release YAML file. If a stable branch isn\u0026rsquo;t specified, the development branch is used.\nCreate Submariner product release The following sections outline the steps to be taken in order to create a full Submariner product release. As an example, we\u0026rsquo;ll use version v0.8.0.\nThe release process is mostly automated and uses a YAML file created in the releases repository that describes the release. This file is updated for each step in the release process.\nOnce the changes for a step are reviewed and merged, a CI job will run to create the release(s) for the step and create the required pull requests in preparation for the next step to be reviewed and merged. Once all these pull requests have been merged, you can continue onto the next step.\nFor most projects, after a release is created, another job will be initiated to build release artifacts and publish to Quay. This will take several minutes. You can monitor the progress from the project\u0026rsquo;s main page. In the branches/tags pull-down above the file list heading, select the tag for the new version. A small yellow circle icon should be present to the right of the file list heading which indicates a job is in progress. You can click it to see details. There may be several checks for the job listed but the important one is Release Images. When complete, the indicator icon will change to either a green check mark on success or a red X on failure. A failure likely means the artifacts were not published to Quay, in which case select the failed check, inspect the logs, correct the issue and re-run the job.\nStep 0: Create a stable branch (if necessary) If you\u0026rsquo;re creating a new stable release, the release automation process can create the stable branches. To do so, navigate to the releases repository and fork it.\n  Create a new branch for the intended release.\n  Create a new file in the releases directory (you can copy the example.yaml file). For our example, we\u0026rsquo;ll name it v0.8.0.yaml.\n  Fill in the general fields for the release with the status field set to branch.\nversion: v0.8.0 name: 0.8.0 status: branch   Create a link for the file to make it the target release. This file is used by the CI jobs.\nln -s v0.8.0.yaml target   Commit the new file, create a new pull request, and have it reviewed and merged.\n  Once the pull request is merged, it will trigger a CI job to create the stable branches and pin them to Shipyard on that stable branch.\nStep 1: Create release for the shipyard project Navigate to the releases repository and fork it.\n  Create a new branch for the intended release.\n  Create a new file in the releases directory (you can copy the example.yaml file). For our example, we\u0026rsquo;ll name it v0.8.0.yaml.\n  Fill in the general fields for the release with the status field set to shipyard. Also add the shipyard component with the hash of the desired or latest commit ID on which to base the release. To obtain the latest, first navigate to the shipyard project. The heading above the file list shows the latest commit on the devel branch including the first 7 hex digits of the commit ID hash.\nIf this is not a final release, set the pre-release field to true (that is uncomment the pre-release line below). This includes release candidates. This is important so it is not labeled as the Latest release in GitHub.\nWhen releasing on a stable branch, make sure to specify the branch as outlined below.\nversion: v0.8.0 name: 0.8.0 #pre-release: true branch: release-0.8 status: shipyard components: shipyard: \u0026lt;hash goes here\u0026gt;   Create a link for the file to make it the target release. This file is used by the CI jobs.\nln -s v0.8.0.yaml target   Commit the new files, create a new pull request, and have it reviewed and merged.\n  Once the pull request is merged, it will trigger a CI job to create a shipyard release and build the Dapper base image. In addition, it creates pull requests in the projects that consume shipyard to update them to the new version in preparation for the subsequent steps.\nOn successful completion, the generated image version (0.8.0) should be available on Quay here:\n https://quay.io/repository/submariner/shipyard-dapper-base?tab=tags\n Step 2: Create release for the admiral project Once the pull request to pin the admiral project to the new shipyard version is merged, we can proceed to updating the release YAML file to create an admiral release.\n  On your releases repository fork, you can either reuse the previous branch or create a new one. For the former, be sure to first rebase the branch on the latest upstream.\n  Edit the release yaml file (v0.8.0.yaml). Update the status field to admiral and add the admiral component with the latest commit ID hash:\n-status: shipyard +status: admiral  components: shipyard: \u0026lt;hash goes here\u0026gt; + admiral: \u0026lt;hash goes here\u0026gt;   Commit the modified file, create a new pull request, and have it reviewed and merged.\n  Once the pull request is merged, it will trigger a CI job to create an admiral release and pull requests in the consuming projects to pin them to the new version in preparation for the subsequent steps.\nStep 3: Create releases for the lighthouse and submariner projects Once the pull requests to pin the lighthouse and submariner projects to the new admiral version are merged:\n  On your releases repository fork, either reuse the previous branch or create a new one.\n  Edit the release yaml file (v0.8.0.yaml). Update the status field to projects and add the submariner and lighthouse components with their latest commit ID hashes:\n-status: admiral +status: projects  components: shipyard: \u0026lt;hash goes here\u0026gt; admiral: \u0026lt;hash goes here\u0026gt; + lighthouse: \u0026lt;hash goes here\u0026gt; + submariner: \u0026lt;hash goes here\u0026gt;   Commit the modified file, create a new pull request, and have it reviewed and merged.\n  Once the pull request is merged, it will trigger a CI job to create lighthouse and submariner releases and a pull request to pin the consuming submariner-operator project to the new version.\nOn successful completion, the new image versions (0.8.0) should be available on Quay.\nFor submariner:\n https://quay.io/repository/submariner/submariner?tab=tags https://quay.io/repository/submariner/submariner-route-agent?tab=tags https://quay.io/repository/submariner/submariner-globalnet?tab=tags https://quay.io/repository/submariner/submariner-networkplugin-syncer?tab=tags\n For lighthouse:\n https://quay.io/repository/submariner/lighthouse-agent?tab=tags https://quay.io/repository/submariner/lighthouse-coredns?tab=tags\n Step 4: Create the product release Once the pull request to pin the submariner-operator has been merged, we can create the final product release:\n  On your releases repository fork, either reuse the previous branch or create a new one.\n  Edit the release yaml file (v0.8.0.yaml). Update the status field to released and add the submariner-operator and submariner-charts components with their latest commit ID hashes:\n-status: projects +status: released  components: shipyard: \u0026lt;hash goes here\u0026gt; admiral: \u0026lt;hash goes here\u0026gt; lighthouse: \u0026lt;hash goes here\u0026gt; submariner: \u0026lt;hash goes here\u0026gt; + submariner-charts: \u0026lt;hash goes here\u0026gt; + submariner-operator: \u0026lt;hash goes here\u0026gt;   Commit the modified file, create a new pull request, and have it reviewed and merged.\n  Once the pull request is merged, it will trigger a CI job to generate and tag the submariner-operator image which should be made available on Quay here:\n https://quay.io/repository/submariner/submariner-operator?tab=tags\n The final product release will be created on the releases repository with a job triggered to publish the subctl binaries for the various platforms. These should be listed under the Assets section for the new release. If not then the job failed so correct the issue and re-run the job.\nIf the release wasn\u0026rsquo;t marked as a pre-release, the release job will also create pull requests in each consuming project to unpin the shipyard Dapper base image version, that is set it back to devel. For ongoing development we want each project to automatically pick up the latest changes to the base image.\nStep 5: Add release notes If this is a final release, add a section for it on this website\u0026rsquo;s release notes page.\n  Clone the submariner-website project.\n  Open src/content/releases/_index.en.md and make changes.\n  Commit the changes, create a pull request, and have it reviewed and merged.\n  Alternatively you can edit the file and create a pull request directly on GitHub here\nStep 6: Verify the release You can follow any of the quick start guides.\nStep 7: Update the Submariner Operator on OperatorHub.io The community-operators Git repository is the source for sharing Kubernetes Operators with the broader community. This repository is split into two sections:\n Operators for deployment to a vanilla Kubernetes environment (upstream-community-operators). These are shared with the Kubernetes community via OperatorHub.io. Operators for deployment to OpenShift (community-operators)  Here we are going to update the Submariner Operator on OperatorHub.io. OpenShift users will find the Operator in the official Red Hat catalog.\nTo publish the Submariner Operator to the community, perform the following steps:\n  Clone the submariner-operator project\n  Make sure you have operator-sdk v1 installed on your machine otherwise follow this guide\n  Generate new package manifests by running the command:\nmake packagemanifests VERSION=${new_version} FROM_VERSION=${previous_version} CHANNEL=${channel} the generated package output should be located in /packagemanifests/${VERSION}/\n  Fork and clone community-operators project.\n  Update the Kubernetes Operator:\n copy the generated package from step 3 into upstream-community-operators/submariner copy the generated package definition /packagemanifests/submariner.package.yaml into upstream-community-operators/submariner/ test the Operator by running the command: make operator.test OP_PATH=upstream-community-operators/submariner preview the Operator on OperatorHub.io once everything is fine, review this checklist and create a new PR on community-operators    Step 8: Announce the release Via E-Mail  https://bit.ly/submariner-dev https://bit.ly/submariner-users  Via Twitter  https://twitter.com/submarinerio  "
},
{
	"uri": "/getting-started/architecture/service-discovery/",
	"title": "Service Discovery",
	"tags": [],
	"description": "",
	"content": "The Lighthouse project provides DNS discovery for Kubernetes clusters connected by Submariner in multi-cluster environments. Lighthouse implements the Kubernetes Multi-Cluster Service APIs.\nArchitecture The below diagram shows the basic Lighthouse architecture:\nLighthouse Agent The Lighthouse Agent runs in every cluster and accesses the Kubernetes API server running in the Broker cluster to exchange service metadata information with other clusters. Local Service information is exported to the Broker and Service information from other clusters is imported.\nAgent Workflow The workflow is as follows:\n Lighthouse Agent connects to the Broker\u0026rsquo;s Kubernetes API server. For every Service in the local cluster for which a ServiceExport has been created, the Agent creates a corresponding ServiceImport resource and exports it to the Broker to be consumed by other clusters. For every ServiceImport resource in the Broker exported from another cluster, it creates a copy of it in the local cluster.  Lighthouse DNS Server The Lighthouse DNS server runs as an external DNS server which owns the domain clusterset.local. CoreDNS is configured to forward any request sent to clusterset.local to the Lighthouse DNS server, which uses the ServiceImport resources that are distributed by the controller for DNS resolution.\nWhen a single Service is deployed to multiple clusters, Lighthouse DNS server prefers the local cluster first before routing the traffic to other remote clusters in a round-robin fashion.\n Server Workflow The workflow is as follows:\n A Pod tries to resolve a Service name using the domain name clusterset.local. CoreDNS forwards the request to the Lighthouse DNS server. The Lighthouse DNS server will use its ServiceImport cache to try to resolve the request. If a record exists it will be returned, else an NXDomain error will be returned.  "
},
{
	"uri": "/getting-started/quickstart/openshift/vsphere-aws/",
	"title": "Hybrid vSphere and AWS",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two OpenShift Container Platform (OCP) clusters: one on VMware vSphere with user provisioned infrastructure (UPI) and the other one on AWS with full stack automation, also known as installer-provisioned infrastructure (IPI). Once the OpenShift clusters are deployed, we deploy Submariner with Service Discovery to interconnect the two clusters.\nPrerequisites Before we begin, the following tools need to be downloaded and added to your $PATH:\n OpenShift installer, pull secret, and command line interface. All can be downloaded from here. AWS CLI which can be downloaded from here.  Please ensure that the tools you downloaded above are compatible with your OpenShift Container Platform version. For more information, please refer to the official OpenShift documentation.\n Create and Deploy cluster-a on vSphere (On-Prem) In this step you will deploy cluster-a using the default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    Before you deploy an OpenShift Container Platform cluster that uses user-provisioned infrastructure, you must create the underlying infrastructure. Follow the OpenShift documenation for installation instructions on supported versions of vSphere.\nSubmariner Gateway nodes need to be able to accept IPsec traffic. For on-premises clusters behind corporate firewalls, the default IPsec UDP ports might be blocked. To overcome this, Submariner supports NAT Traversal (NAT-T) with the option to set custom non-standard ports. In this example, we use UDP 4501 and UDP 501. Ensure that those ports are allowed on the gateway node and on the corporate firewall.\nSubmariner also uses VXLAN to encapsulate traffic from the worker and master nodes to the Gateway nodes. Ensure that firewall configuration on the vSphere cluster allows 4800/UDP across all nodes in the cluster in both directions.\n   Protocol Port Description     UDP 4800 Overlay network for inter-cluster traffic   UDP 4501 IPsec traffic   UDP 501 IPsec traffic    When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nCreate and Deploy cluster-b on AWS Setup Your AWS Profile Configure the AWS CLI with the settings required to interact with AWS. These include your security credentials, the default AWS Region, and the default output format:\n$ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text Create and Deploy cluster-b In this step you will deploy cluster-b, modifying the default IP CIDRs to avoid IP address conflicts with cluster-a. You can change the IP addresses block and prefix based on your requirements. For more information on IPv4 CIDR conversion, please check this page.\nIn this example, we will use the following IP ranges:\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the Pod network CIDR from 10.128.0.0/14 to 10.132.0.0/14:\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the Service network CIDR from 172.30.0.0/16 to 172.31.0.0/16:\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy the cluster:\nopenshift-install create cluster --dir cluster-b When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nPrepare AWS Cluster for Submariner Submariner Gateway nodes need to be able to accept traffic over UDP ports (4500 and 500 by default) when using IPsec. Submariner also uses UDP port 4800 to encapsulate traffic from the worker and master nodes to the Gateway nodes, and TCP port 8080 to retrieve metrics from the Gateway nodes. Additionally, the default OpenShift deployment does not allow assigning an elastic public IP to existing worker nodes, which may be necessary on one end of the IPsec connection.\nprep_for_subm is a script designed to update your OpenShift installer provisioned AWS infrastructure for Submariner deployments, handling the requirements specified above.\n Download the prep_for_subm.sh script and set permissions:  curl https://raw.githubusercontent.com/submariner-io/submariner/devel/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh  Please note that oc, aws-cli, terraform, and wget need to be installed before the prep_for_subm.sh script can be run. Also note that the script is known to be working with Terraform version 0.12. Maximum compatible version is 0.12.12.\n The script deploys an m5n.large EC2 instance type by default, optimized for improved network throughput and packet rate performance, for the Submariner gateway node. Please ensure that the AWS Region you deploy to support this instance type. Alternatively, you can customize the AWS instance type as shown below.\n  Modify the IPsec UDP ports and run the prep_for_subm.sh script for cluster-b:  export IPSEC_NATT_PORT=4501 export IPSEC_IKE_PORT=501 ./prep_for_subm.sh cluster-b # respond \u0026#34;yes\u0026#34; when Terraform asks for approval, or otherwise add the -auto-approve flag Submariner Installation Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    If you have Go installed, you can use that instead: go install github.com/submariner-io/submariner-operator/pkg/subctl    (and ensure your go/bin directory is on your PATH). Install Submariner with Service Discovery To install Submariner with multi-cluster service discovery, follow the steps below:\nUse cluster-b (AWS) as Broker with Service Discovery enabled subctl deploy-broker --kubeconfig cluster-b/auth/kubeconfig --service-discovery Join cluster-b (AWS) and cluster-a (vSphere) to the Broker subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --ikeport 501 --nattport 4501 subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --ikeport 501 --nattport 4501  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-b.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the Headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification This will perform automated verifications between the clusters.\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/development/",
	"title": "Development",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/operations/cleanup/",
	"title": "Uninstalling Submariner",
	"tags": [],
	"description": "",
	"content": "To properly uninstall Submariner from a cluster, follow the steps below:\nMake sure KUBECONFIG for all participating clusters is exported and all participating clusters are accessible via kubectl.\n   Delete Submariner-related namespaces\nFor each participating cluster, issue the following command:\nkubectl delete namespace submariner-operator For the Broker cluster, issue the following command:\nkubectl delete namespace submariner-k8s-broker For submariner version 0.9 and above, also delete submariner-operator namespace from the Broker cluster by issuing the following command:\nkubectl delete namespace submariner-operator   Delete the Submariner CRDs\nFor each participating cluster, issue the following command:\nfor CRD in `kubectl get crds | grep -iE \u0026#39;submariner|multicluster.x-k8s.io\u0026#39;| awk \u0026#39;{print $1}\u0026#39;`; do kubectl delete crd $CRD; done   Delete Submariner\u0026rsquo;s ClusterRoles and ClusterRoleBindings\nFor each participating cluster, issue the following command:\nroles=\u0026#34;submariner-operator submariner-operator-globalnet submariner-lighthouse submariner-networkplugin-syncer\u0026#34; kubectl delete clusterrole,clusterrolebinding $roles --ignore-not-found   Remove the Submariner gateway labels\nFor each participating cluster, issue the following command:\nkubectl label --all node submariner.io/gateway-   For OpenShift deployments, delete Lighthouse entry from default DNS.\nFor each participating cluster, issue the following command:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: operator.openshift.io/v1 kind: DNS metadata: finalizers: - dns.operator.openshift.io/dns-controller name: default spec: servers: [] EOF This deletes the lighthouse entry from the Data section in Corefile of the configmap.\n#lighthouse-start AUTO-GENERATED SECTION. DO NOT EDIT clusterset.local:53 { forward . 100.3.185.93 } #lighthouse-end Verify that the lighthouse entry is deleted from Corefile of dns-default configmap by running following command on an OpenShift cluster\nkubectl describe configmap dns-default -n openshift-dns For Kubernetes deployments, manually edit the Corefile of coredns configmap and delete the lighthouse entry by issuing below commands\nkubectl edit cm coredns -n kube-system This will also restart the coredns. Below command can also be issued to manually restart coredns.\nkubectl rollout restart -n kube-system deployment/coredns Verify that the lighthouse entry is deleted from Data section in Corefile of dns-default config map by running following command on a Kubernetes cluster\nkubectl describe configmap coredns -n kube-system  Following commands need to be executed from inside the cluster nodes.\n   Remove Submariner\u0026rsquo;s iptables chains\nOn all nodes in each participating cluster, issue the following commands:\niptables --flush SUBMARINER-INPUT iptables -D INPUT $(iptables -L INPUT --line-numbers | grep SUBMARINER-INPUT | awk \u0026#39;{print $1}\u0026#39;) iptables --delete-chain SUBMARINER-INPUT iptables -t nat --flush SUBMARINER-POSTROUTING iptables -t nat -D POSTROUTING $(iptables -t nat -L POSTROUTING --line-numbers | grep SUBMARINER-POSTROUTING | awk \u0026#39;{print $1}\u0026#39;) iptables -t nat --delete-chain SUBMARINER-POSTROUTING If Globalnet is enabled in the setup, additionally issue the following commands on gateway nodes:\niptables -t nat --flush SUBMARINER-GN-INGRESS iptables -t nat -D PREROUTING $(iptables -t nat -L PREROUTING --line-numbers | grep SUBMARINER-GN-INGRESS | awk \u0026#39;{print $1}\u0026#39;) iptables -t nat --delete-chain SUBMARINER-GN-INGRESS iptables -t nat --flush SUBMARINER-GN-EGRESS iptables -t nat --delete-chain SUBMARINER-GN-EGRESS iptables -t nat -t nat --flush SUBMARINER-GN-MARK iptables -t nat --delete-chain SUBMARINER-GN-MARK   Delete the vx-submariner interface\nOn all nodes in each participating cluster, issue the following command:\nip link delete vx-submariner   "
},
{
	"uri": "/other-resources/",
	"title": "Other Resources",
	"tags": [],
	"description": "",
	"content": "There are multiple presentations and demo recordings about Submariner available online.\nConference Presentations  Computing on the Edge with Kubernetes 2020 video recording KubeCon China 2019 slides and video recording KubeCon North America 2019 slides and video recording  Demo Recordings  Submariner in 60s Deploying Submariner with subctl Connecting hybrid Kubernetes clusters using Submariner Cross-cluster service discovery in Submariner using Lighthouse  Blogs  Geographically Distributed Stateful Workloads Part One: Cluster Preparation Geographically Distributed Stateful Workloads Part Two: CockroachDB Multicluster Service Discovery in OpenShift with Submariner and Lighthouse (Part 1) Multicluster Service Discovery in OpenShift with Submariner and Lighthouse (Part 2) Kubernetes Multi-Cloud and Multi-Cluster Connectivity with Submariner  SIG Presentations  SIG Multicluster demo of Submariner\u0026rsquo;s KEP1645 Multicluster Services implementation (2020/09/22) SIG Multicluster demo of Submariner\u0026rsquo;s multicluster networking deployed by Submariner\u0026rsquo;s Operator and subctl (2019/12/17)  Academic Papers  Kubernetes and the Edge?  If you find additional material that isn\u0026rsquo;t listed here, please feel free to add it to this page by editing it. The website contributing guide is here.\n"
},
{
	"uri": "/getting-started/architecture/globalnet/",
	"title": "Globalnet Controller",
	"tags": [],
	"description": "",
	"content": "Introduction Submariner is a tool built to connect overlay networks of different Kubernetes clusters. These clusters can be on different public clouds or on-premises. An important use case for Submariner is to connect disparate independent clusters into a ClusterSet.\nHowever, by default, a limitation of Submariner is that it doesn\u0026rsquo;t handle overlapping CIDRs (ServiceCIDR and ClusterCIDR) across clusters. Each cluster must use distinct CIDRs that don\u0026rsquo;t conflict or overlap with any other cluster that is going to be part of the ClusterSet.\nThis is largely problematic because most actual deployments use the default CIDRs for a cluster so every cluster ends up using the same CIDRs. Changing CIDRs on existing clusters is a very disruptive process and requires a cluster restart. So Submariner needs a way to allow clusters with overlapping CIDRs to connect together.\nArchitecture To support overlapping CIDRs in connected clusters, Submariner has a component called Global Private Network, Globalnet (globalnet). This Globalnet is a virtual network specifically to support Submariner\u0026rsquo;s multi-cluster solution with a global CIDR. Each cluster is given a subnet from this virtual Global Private Network, configured as new cluster parameter GlobalCIDR (e.g. 169.254.0.0/16) which is configurable at time of deployment. User can also manually specify GlobalCIDR for each cluster that is joined to the Broker using the flag globalnet-cidr passed to subctl join command. If Globalnet is not enabled in the Broker or if a GlobalCIDR is preconfigured in the cluster, the supplied globalnet-cidr will be ignored.\nOnce configured, each exported Service and Pod that requires cross-cluster access is allocated an IP, named globalIp, from this GlobalCIDR that is annotated on the Pod/Service object. This globalIp is used for all cross-cluster communication to and from a Pod and the globalIp of a remote Service. Routing and iptable rules are configured to use the globalIp for ingress and egress. All address translations occur on the Gateway node.\nUnlike vanilla Submariner, where Pod to Pod connectivity is also supported, Globalnet only supports Pod to remote Service connectivity using globalIps.\n submariner-globalnet Submariner Globalnet is a component that provides cross-cluster connectivity from Pods to remote Services using their globalIps. Compiled as binary submariner-globalnet, it is responsible for maintaining a pool of global IPs, allocating IPs from the GlobalIp pool to pods and services, annotating Services and Pods with their globalIp, and configuring the required rules on the gateway node to provide cross-cluster connectivity using globalIps. Globalnet also supports connectivity from the Nodes (including Pods that use HostNetworking) to globalIp of remote Services. It mainly consists of two key components: the IP Address Manager and Globalnet.\nIP Address Manager (IPAM) The IP Address Manager (IPAM) component does the following:\n Creates a pool of IP addresses based on the GlobalCIDR configured on cluster. Allocates a globalIp from the GlobalIp pool on creation of a Pod, Service, or ServiceExport. Annotates the Pod or exported Service with submariner.io/globalIp=\u0026lt;global-ip\u0026gt;. On deletion of a Pod, Service, or ServiceExport, releases its globalIp back to the pool.  Globalnet This component is responsible for programming the routing entries, iptable rules and does the following:\n Creates initial iptables chains for Globalnet rules. Whenever a Pod is annotated with a globalIp, creates an egress SNAT rule to convert the source Ip from the Pod\u0026rsquo;s Ip to the Pod\u0026rsquo;s globalIp on the Gateway Node. Whenever a Service is annotated with a globalIp, creates an ingress rule to direct all traffic destined to the Service\u0026rsquo;s globalIp to the Service\u0026rsquo;s kube-proxy iptables chain which in turn directs traffic to Service\u0026rsquo;s backend Pods. Clean up the rules from the gateway node on the deletion of a Pod, Service, or ServiceExport.  Globalnet currently relies on kube-proxy and thus will only work with deployments that use kube-proxy.\nService Discovery - Lighthouse Connectivity is only part of the solution as pods still need to know the IPs of services on remote clusters.\nThis is achieved by enhancing lighthouse with support for Globalnet. The Lighthouse controller adds the service\u0026rsquo;s globalIp to the ServiceImport object that is distributed to all clusters. The lighthouse plugin then uses the Service\u0026rsquo;s globalIp when replying to DNS queries for the Service.\nBuilding Nothing extra needs to be done to build submariner-globalnet as it is built with the standard Submariner build.\n"
},
{
	"uri": "/community/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "Submariner organizes all current and upcoming work using GitHub Issues, Projects, and Milestones.\nPlanning Process In preparation for sprint planning meetings (see Submariner\u0026rsquo;s Community Calendar), GitHub Issues should be raised for work that is to be a part of a sprint. Issues targeted for a sprint should be added to the upcoming Project\u0026rsquo;s \u0026ldquo;Backlog\u0026rdquo; column. During sprint planning meetings, Issues will be discussed and moved to the \u0026ldquo;TODO\u0026rdquo; column and the milestone will be set to the targeted release. As contributors make progress during sprints, Issues should be moved through the \u0026ldquo;In Progress\u0026rdquo;/\u0026ldquo;Review/Verify\u0026rdquo;/\u0026ldquo;Done\u0026rdquo; columns of the Project. If an Issue is implemented during a release but additional work (like an ACK-fixed verification) tracked by the relevant Issue is necessary, the Issue can be carried over to the next Project but the Milestone should reflect where the code was shipped for accurate release note creation.\nCurrent Work Current and near-future work is tracked by Submariner\u0026rsquo;s open Projects.\nFuture Work Some high-level goals are summarized here, but the primary source for tracking future work are Submariner\u0026rsquo;s GitHub Issues.\n Auto detecting NAT vs non-NAT scenarios (https://github.com/submariner-io/submariner/issues/300) Support different IPsec ports for each cluster Network Policy across clusters (Coastguard) Support for finer-grained connectivity policies (https://github.com/submariner-io/submariner/issues/533) Globalnet: annotating Global IPs per namespaces (https://github.com/submariner-io/submariner/issues/528) More tunnel encapsulation options Dynamic routing with BGP to support multi-path forwarding across gateways Testing with multi-cluster Istio  Suggesting Work If we are missing something that would make Submariner more useful to you, please let us know. The best way is to file an Issue and include information on how you intend to use Submariner with that feature.\n"
},
{
	"uri": "/development/security/",
	"title": "Security Reporting",
	"tags": [],
	"description": "",
	"content": "Submariner welcomes and appreciates responsible disclosure of security vulnerabilities.\nIf you know of a security issue with Submariner, please report it to submariner-security@googlegroups.com.\nSubmariner aspires to follow the Kubernetes security reporting process, but is far too small of a project to implement those practices. Where applicable, Submariner will follow the principles of the Kubernetes process.\n"
},
{
	"uri": "/development/shipyard/",
	"title": "Working with Shipyard",
	"tags": [],
	"description": "",
	"content": "Overview The Shipyard project provides common tooling for creating Kubernetes clusters with kind (Kubernetes in Docker) and provides a common Go framework for creating end to end tests. Shipyard contains common functionality shared by other projects. Any project specific functionality should be part of that project.\nA base image quay.io/submariner/shipyard-dapper-base is created from Shipyard and contains all the tooling to build other projects and run tests in a consistent environment.\nShipyard has several folders at the root of the project:\n package: Contains the ingredients to build the base image. scripts: Contains general scripts for Shipyard make targets.  shared: Contains all the shared scripts that projects can consume. These are copied into the base image under $SCRIPTS_DIR.  lib: Library functions that shared scripts, or consuming projects, can use. resources: Resource files to be used by the shared scripts.     test: Test library to be used by other projects.  Shipyard ships with some Makefile targets which can be used by consuming projects and are used by Shipyard\u0026rsquo;s CI to test and validate itself. It also has some specific Makefile targets which are used by the project itself.\nUsage Add Shipyard to a Project To enable usage of Shipyard\u0026rsquo;s functionality, please see Adding Shipyard to a Project.\nUse Shipyard in Your Project Once Shipyard has been added to a project, you can use any of the Makefile targets that it provides.\nAny variables that you need to pass to these targets should be specified in your Dockerfile.dapper so they\u0026rsquo;re available in the Dapper environment. For example:\nENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT CLUSTERS_ARGS DEPLOY_ARGS\u0026#34;Have Shipyard Targets Depend on Your Project\u0026rsquo;s Targets Having any of the Shipyard Makefile targets rely on your project\u0026rsquo;s specific targets can be done easily by adding the dependency in your project\u0026rsquo;s Makefile. For example:\nclusters: build images Use an Updated Shipyard Image in Your Project If you\u0026rsquo;ve made changes to Shipyard\u0026rsquo;s base image and need to test them in your project, run:\nmake dapper-image in the Shipyard directory. This creates a local image with your changes available for consumption in other projects.\nShared Makefile Targets Shipyard ships a Makefile.inc file which defines these basic targets:\n clusters: Creates the kind-based cluster environment. deploy : Deploys submariner components in the cluster environment (depends on clusters). cleanup: Deletes the kind environment (if it exists) and any residual resources. release: Uploads the requested image(s) to Quay.io. vendor/modules.txt: Populates go modules (in case go.mod exists in the root directory).  If your project uses Shipyard then it has all these targets and supports all the variables these targets support.\nAny variables supported by these targets can be either declared as environment variables or assigned on the make command line (takes precedence over environment variables).\nClusters A Make target that creates a kind-based multi-cluster environment with just the default Kubernetes deployment:\nmake clusters Respected variables:\n CLUSTERS_ARGS: Any arguments (flags and/or values) to be sent to the clusters.sh script. To get a list of available arguments, run: scripts/shared/clusters.sh --help  Deploy A Make target that deploys Submariner components in a kind-based cluster environment (if one isn\u0026rsquo;t created yet, this target will first invoke the clusters target to do so):\nmake deploy Respected variables:\n Any variable from clusters target (only if it wasn\u0026rsquo;t created). DEPLOY_ARGS: Any arguments (flags and/or values) to be sent to the deploy.sh script. To get a list of available arguments, run: scripts/shared/deploy.sh --help  Cleanup To clean up all the kind clusters deployed in any of the previous steps, use:\nmake cleanup This command will remove the clusters and any resources that might\u0026rsquo;ve been left in docker that are not needed any more (images, volumes, etc).\nRelease Uploads the built images to Quay.io:\nmake release release_images=\u0026#34;\u0026lt;image name\u0026gt;\u0026#34; Respected variables:\n QUAY_USERNAME, QUAY_PASSWORD: Needed in order to log in to Quay. release_images: One or more image names to release separated by spaces. release_tag: A tag to use for the release (default is latest). repo: The Quay repo to use (default is quay.io/submariner).  Specific Makefile Targets Shipyard has some project-specific targets which are used to build parts of the projects:\n dapper-image: Builds the base image that can be used by other projects. validate: Validates the go code that Shipyard provides, and the shared shell scripts.  Dapper-Image Builds the basic image which is then used by other projects to build the code and run tests:\nmake dapper-image Respected variables:\n dapper_image_flags: Any additional flags and values to be sent to the build_image.sh script.  "
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Submariner Submariner enables direct networking between Pods and Services in different Kubernetes clusters, either on-premises or in the cloud.\nWhy Submariner As Kubernetes gains adoption, teams are finding they must deploy and manage multiple clusters to facilitate features like geo-redundancy, scale, and fault isolation for their applications. With Submariner, your applications and services can span multiple cloud providers, data centers, and regions.\nSubmariner is completely open source, and designed to be network plugin (CNI) agnostic.\nWhat Submariner Provides  Cross-cluster L3 connectivity using encrypted VPN tunnels Service Discovery across clusters subctl, a friendly deployment tool Support for interconnecting clusters with overlapping CIDRs  A few requirements need to be met before you can begin. Check the Prerequisites section for more information.\n Check the Quickstart Guides section for deployment instructions.\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/getting-started/architecture/networkplugin-syncer/",
	"title": "Network Plugin Syncer",
	"tags": [],
	"description": "",
	"content": "The Network Plugin Syncer provides a framework for components to interface with the configured Kubernetes Container Network Interface (CNI) plugin to perform any API/database tasks necessary to facilitate routing cross-cluster traffic, like creating API objects that the CNI plugin will process or working with the specific CNI databases.\nThe detected CNI plugin implementation configured for the cluster is received by the Network Plugin Syncer, and executes the appropriate plugin handler component, if any.\nThe following table highlights the differences with the Route Agent:\n    Route Agent Network Plugin Syncer     Configures the CNI plugin  x   Configures low level network elements on the host x    Runs as a Kubernetes Deployment  x   Runs as a Kubernetes Daemonset on every host x     This component is only necessary for specific Kubernetes CNI plugins like OVN Kubernetes.\n "
},
{
	"uri": "/getting-started/architecture/networkplugin-syncer/ovn-kubernetes/",
	"title": "OVN Kubernetes",
	"tags": [],
	"description": "",
	"content": "A specific handler component is deployed for the OVN Kubernetes CNI plugin.\nOVN is a project that builds on top of Open vSwitch providing a rich high level API for describing virtual network components like Logical Routers, Logical Switches, Load balancers, Logical Ports. OVN Kubernetes is a Cloud Management System Plugin (CMS plugin) in terms of the OVN project.\nThe OVN Kubernetes handler watches for Submariner Endpoints and Kubernetes Nodes and interfaces with the OVN databases (OVN NorthDB and SouthDB) to store and maintain information necessary for Submariner, including:\n  A logical router named submariner_router that handles the communication to remote clusters and has a leg on the network which can talk to the ovn-k8s-gw0 interface on the Gateway node. This router is pinned to the active Gateway chassis.\n  Routing policies added to the existing ovn_cluster_router which redirect traffic targeted for remote routers through the submariner_router.\n  A submariner_join logical switch that connects the submariner_router with the ovn_cluster_router.\n  The handler architecture The following diagram illustrates the OVN Kubernetes handler architecture where the blue elements represent the OVN Kubernetes native network elements and the yellow elements are introduced by Submariner.\n"
},
{
	"uri": "/getting-started/architecture/route-agent/",
	"title": "Route Agent",
	"tags": [],
	"description": "",
	"content": "The Route Agent component runs on every node in each participating cluster. It is responsible for setting up the necessary host network elements on top of the existing Kubernetes CNI plugin.\nThe Route Agent receives the detected CNI plugin as part of its configuration.\nkube-proxy iptables For CNI plugins that utilize kube-proxy in iptables mode, the Route Agent is responsible for setting up VXLAN tunnels and routing the cross cluster traffic from the node to the cluster’s active Gateway Engine which subsequently sends the traffic to the destination cluster.\nWhen running on the same node as the active Gateway Engine, Route Agent creates a VXLAN VTEP interface to which Route Agent instances running on the other worker nodes in the local cluster connect by establishing a VXLAN tunnel with the VTEP of the active Gateway Engine node. The MTU of the VXLAN tunnel is configured based on the MTU of the default interface on the host minus the VXLAN overhead.\nRoute Agents use Endpoint resources synced from other clusters to configure routes and to program the necessary iptables rules to enable full cross-cluster connectivity.\nWhen the active Gateway Engine fails and a new Gateway Engine takes over, Route Agents will automatically update the route tables on each node to point to the new active Gateway Engine node.\nOVN Kubernetes For the OVN Kubernetes CNI plugin, host network routing is configured on all nodes and, on the active Gateway node, IP forwarding is configured between the ovn-k8s-gw0 and cable interfaces.\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]