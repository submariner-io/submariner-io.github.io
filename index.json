[
{
	"uri": "/contributing/website/style_guide/",
	"title": "Docs Style Guide",
	"tags": [],
	"description": "",
	"content": "Documentation Style Guide This guide is meant to help keep our documentation consistent and ease the contribution and review process.\nSubmariner follows the Kubernetes Documentation Style Guide wherever relevant. This is a Submariner-specific extension of those practices.\nSubmariner.io word list A list of Submariner-specific terms and words to be used consistently across the site.\n   Term Usage     Submariner The project name Submariner should always be capitalized.   Admiral The project name Admiral should always be capitalized.   Lighthouse The project name Lighthouse should always be capitalized.   Coastguard The project name Coastguard should always be capitalized.   Shipyard The project name Shipyard should always be capitalized.   subctl The artifact subctl should not be capitalized and should be formatted in code style.    "
},
{
	"uri": "/contributing/website/",
	"title": "Contributing to the Website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on hugo, grav, and the hugo-learn-theme, and is written in Markdown format.\nIf you want to contribute, we recommend reading the hugo-learn-theme documentation.\nYou can always click the Edit this page link at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on GitHub.\n  Checkout your copy locally:\n  git clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git cd submariner-website make server  An instance of the website is now running locally on your machine and is accessible at http://localhost:1313\n  Edit files in src. The browser should automatically reload so you can test your changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the GitHub workflow here.\n  "
},
{
	"uri": "/architecture/globalnet/",
	"title": "Globalnet Controller",
	"tags": [],
	"description": "",
	"content": "Introduction Submariner is a tool built to connect overlay networks of different Kubernetes clusters. These clusters can be on different public clouds or on-premise. An important use case for Submariner is to connect disparate independent clusters into a single cohesive multi-cluster fleet.\nHowever, by default, a limitation of submariner is that it doesn\u0026rsquo;t handle overlapping CIDRs (ServiceCIDR and ClusterCIDR) across these clusters. Each cluster must use distinct CIDRs that don\u0026rsquo;t conflict or overlap with any other cluster that is going to be part of cluster fleet.\nThis is largely problematic because most actual deployments use the default CIDRs for a cluster so every cluster ends up using the same CIDRs. Changing CIDRs on existing clusters is a very disruptive process and requires a cluster restart. So submariner needs a way to allow clusters with overlapping CIDRs to connect together.\nArchitecture To support overlapping CIDRs in clusters connected through submariner, submariner has a component called Global Private Network, GlobalNet (globalnet). This GlobalNet is a virtual network specifically to support submariner\u0026rsquo;s multi-cluster solution with a Global CIDR. Each cluster is given a subnet from this Global Private Network, configured as new cluster parameter GlobalCIDR (e.g. 169.254.0.0/16) which is configurable at time of deployment\nOnce configured, each Service and Pod that requires cross-cluster access is allocated an IP, named globalIp, from this GlobalCIDR that is annotated on the Pod/Service object. This globalIp is used for all cross-cluster communication to and from a Pod and the globalIp of a remote Service. Routing and IPTable rules are configured to use the globalIp for ingress and egress. All address translations occur on the Gateway node.\nUnlike vanilla Submariner, where Pod to Pod connectivity is also supported, GlobalNet only supports Pod to remote Service connectivity using globalIps.\n submariner-globalnet Submariner GlobalNet is a component that provides cross-cluster connectivity from Pods to remote Services using their globalIps. Compiled as binary submariner-globalnet, it is responsible for maintaining a pool of global IPs, allocating IPs from the GlobalIp pool to pods and services, annotating Services and Pods with their globalIp, and configuring the required rules on the gateway node to provide cross-cluster connectivity using globalIps. Globalnet also supports connectivity from the Nodes (including Pods that use HostNetworking) to globalIp of remote Services. It mainly consists of two key components: the IP Address Manager and Globalnet.\nIP Address Manager (IPAM) The IP Address Manager (IPAM) component does the following:\n Creates a pool of IP addresses based on the GlobalCIDR configured on cluster. On creation of a Pod/Service, allocates a globalIp from the GlobalIp pool. Annotates the Pod/Service with submariner.io/globalIp=\u0026lt;global-ip\u0026gt;. On deletion of a Pod/Service, releases its globalIp back to the pool.  Globalnet This component is responsible for programming the routing entries, IPTable rules and does the following:\n Creates initial IPTables chains for Globalnet rules. Whenever a Pod is annotated with a globalIp, creates an egress SNAT rule to convert the source Ip from the Pod\u0026rsquo;s Ip to the Pod\u0026rsquo;s globalIp on the Gateway Node. Whenever a Service is annotated with a globalIp, creates an ingress rule to direct all traffic destined to the Service\u0026rsquo;s globalIp to the Service\u0026rsquo;s kube-proxy IPTables chain which in turn directs traffic to Service\u0026rsquo;s backend Pods. On deletion of pod/service, clean up the rules from the gateway node.  Globalnet currently relies on kube-proxy and thus will only work with deployments that use kube-proxy.\nService Discovery - Lighthouse Connectivity is only part of the solution as pods still need to know the IPs of services on remote clusters.\nThis is achieved by enhancing lighthouse with support for GlobalNet. The Lighthouse controller adds the service\u0026rsquo;s globalIp to the MultiClusterService object that is distributed to all clusters. The lighthouse plugin then uses the Service\u0026rsquo;s globalIp when replying to DNS queries for the Service.\nBuilding Nothing extra needs to be done to build submariner-globalnet as it is built with the standard submariner build.\n"
},
{
	"uri": "/architecture/network/",
	"title": "Network",
	"tags": [],
	"description": "",
	"content": "Lorem Ipsum.\n"
},
{
	"uri": "/quickstart/",
	"title": "Quickstart Guide",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner has two main core pieces (the broker and submariner), more information about this topic can be found in the Architecture section.\nThe Broker The broker is an API to which all participating clusters are given access and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a Cluster, and the reachable cluster IPs from the endpoint.  The broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe Submariner Deployment on a Cluster Once submariner is deployed on a cluster with the proper credentials to the broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n  At least 2 Kubernetes clusters, one of which is designated to serve as the central broker that is accessible by all of your connected clusters; this can be one of your connected clusters, but comes with the limitation that the cluster is required to be up to facilitate interconnectivity/negotiation.\n  Different service/pod CIDRs between clusters. This is to prevent routing conflicts.\n    Direct IP connectivity between the gateway nodes through the internet (or on the same network if not running Submariner over the internet). Submariner supports 1:1 NAT setups but has a few caveats/provider-specific configuration instructions in this configuration.  Knowledge of each cluster\u0026rsquo;s network configuration.\n  Worker node IPs on all the clusters must be outside of the cluster/service CIDR ranges.\n  An example of three clusters configured to use with Submariner would look like the following:\n   Cluster Name Provider Pods CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east OnPrem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Deployment The available methods for deployment are:\n subctl (+ submariner-operator). helm charts.  The community recommends the use of subctl, because it simplifies most of the manual steps required for deployment, as well as verification of connectivity between the clusters. In the future it may provide additional capabilities like:\n Detection of possible conflicts Upgrade management Status inspection of the deployment Configuration updates Maintenance and debugs tasks Wrapping of logs for support tasks.  To deploy submariner with subctl please follow the deployment guide. If helm fits better your deployment methodologies, please find the details here\n"
},
{
	"uri": "/contributing/shipyard/first_time/",
	"title": "Adding Shipyard to a Project",
	"tags": [],
	"description": "",
	"content": "To use Shipyard in your project, it\u0026rsquo;s easiest to use Dapper and Make. To use Dapper you\u0026rsquo;ll need a specific Dockerfile that Dapper consumes to create a consistent environment based upon Shipyard\u0026rsquo;s base image. To use Make you\u0026rsquo;ll need some commands to enable Dapper and also include the targets which ship in the base image.\nDockerfile.dapper The project should have a Dockerfile.dapper Dockerfile which builds upon quay.io/submariner/shipyard-dapper-base.\nFor example:\nFROMquay.io/submariner/shipyard-dapper-baseENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT\u0026#34; \\  DAPPER_SOURCE=\u0026lt;your source directory\u0026gt; DAPPER_DOCKER_SOCKET=trueENV DAPPER_OUTPUT=${DAPPER_SOURCE}/outputWORKDIR${DAPPER_SOURCE}ENTRYPOINT [\u0026#34;./scripts/entry\u0026#34;]CMD [\u0026#34;ci\u0026#34;]You can also refer to the project\u0026rsquo;s own Dockerfile.dapper as an example.\nBuilding The Base Image To build the base container image used in the shared developer and CI enviroment, simply run:\nmake dapper-image Makefile The project\u0026rsquo;s Makefile should include targets to run everything in Dapper. These are defined in Shipyard\u0026rsquo;s Makefile.dapper which can be copied as is into your project and included in the Makefile. To use Shipyard\u0026rsquo;s built-in targets available in the base Dapper image, include the Makefile.inc file in the project\u0026rsquo;s Makefile within the section where the Dapper environment is detected.\nThe simplest Makefile would look like this:\nifneq (,$(DAPPER_HOST_ARCH)) # Running in Dapper  include $(SHIPYARD_DIR)/Makefile.inc else # Not running in Dapper  include Makefile.dapper endif # Disable rebuilding Makefile Makefile Makefile.dapper Makefile.inc: ; You can also refer to the project\u0026rsquo;s own Makefile as an example.\n"
},
{
	"uri": "/contributing/shipyard/",
	"title": "Working With Shipyard",
	"tags": [],
	"description": "",
	"content": "Overview The Shipyard project provides common tooling for creating K8s clusters with KIND (K8s in Docker) and provides a common Go framework for creating end to end tests. Shipyard contains common functionality shared by other projects. Any project specific functionality should be part of that project.\nA base image quay.io/submariner/shipyard-dapper-base is created from Shipyard and contains all the tooling to build other projects and run tests in a consistent environment.\nShipyard has several folders at the root of the project:\n package: Contains the ingredients to build the base image. scripts: Contains general scripts for Shipyard make targets.  shared: Contains all the shared scripts that projects can consume. These are copied into the base image under $SCRIPTS_DIR.  lib: Library functions that shared scripts, or consuming projects, can use. resources: Resource files to be used by the shared scripts.     test: Test library to be used by other projects.  Shipyard ships with some Makefile targets which can be used by consuming projects and are used by Shipyard\u0026rsquo;s CI to test and validate itself. It also has some specific Makefile targets which are used by the project itself.\nUsage Add Shipyard to a Project To enable usage of Shipyard\u0026rsquo;s functionality, please see Adding Shipyard to a Project.\nUse Shipyard in Your Project Once Shipyard has been added to a project, you can use any of the Makefile targets that it provides.\nAny variables that you need to pass to these targets should be specified in your Dockerfile.dapper so they\u0026rsquo;re available in the Dapper environment. For example:\nENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT CLUSTERS_ARGS DEPLOY_ARGS\u0026#34;Have Shipyard Targets Depend on Your Project\u0026rsquo;s Targets Having any of the Shipyard Makefile targets rely on your project\u0026rsquo;s specific targets can be done easily by adding the dependency in your project\u0026rsquo;s Makefile. For example:\nclusters: build images Use an Updated Shipyard Image in Your Project If you\u0026rsquo;ve made changes to Shipyard\u0026rsquo;s base image and need to test them in your project, run:\nmake dapper-image in the Shipyard directory. This creates a local image with your changes available for consumption in other projects.\nShared Makefile Targets Shipyard ships a Makefile.inc file which defines these basic targets:\n clusters: Creates the KIND -based cluster environment. deploy : Deploys submariner components in the cluster environment (depends on clusters). cleanup: Deletes the KIND environment (if it exists) and any residual resources. release: Uploads the requested image(s) to Quay.io. vendor/modules.txt: Populates go modules (in case go.mod exists in the root directory).  If your project uses Shipyard then it has all these targets and supports all the variables these targets support.\nAny variables supported by these targets can be either declared as environment variables or assigned on the make command line (takes precedence over environment variables).\nClusters A Make target that creates a KIND-based multi-cluster environment with just the default K8s deployment:\nmake clusters Respected variables:\n CLUSTERS_ARGS: Any arguments (flags and/or values) to be sent to the clusters.sh script. To get a list of available arguments, run: scripts/shared/clusters.sh --help  Deploy A Make target that deploys Submariner components in a KIND-based cluster environment (if one isn\u0026rsquo;t created yet, this target will first invoke the clusters target to do so):\nmake deploy Respected variables:\n Any variable from clusters target (only if it wasn\u0026rsquo;t created). DEPLOY_ARGS: Any arguments (flags and/or values) to be sent to the deploy.sh script. To get a list of available arguments, run: scripts/shared/deploy.sh --help  Cleanup To clean up all the KIND clusters deployed in any of the previous steps, use:\nmake cleanup This command will remove the clusters and any resources that might\u0026rsquo;ve been left in docker that are not needed any more (images, volumes, etc).\nRelease Uploads the built images to Quay.io:\nmake release release_images=\u0026quot;\u0026lt;image name\u0026gt;\u0026quot; Respected variables:\n QUAY_USERNAME, QUAY_PASSWORD: Needed in order to log in to Quay. release_images: One or more image names to release separated by spaces. release_tag: A tag to use for the release (default is latest). repo: The Quay repo to use (default is quay.io/submariner).  Specific Makefile Targets Shipyard has some project-specific targets which are used to build parts of the projects:\n dapper-image: Builds the base image that can be used by other projects. validate: Validates the go code that Shipyard provides, and the shared shell scripts.  Dapper-Image Builds the basic image which is then used by other projects to build the code and run tests:\nmake dapper-image Respected variables:\n dapper_image_flags: Any additional flags and values to be sent to the build_image.sh script.  "
},
{
	"uri": "/contributing/building_testing/",
	"title": "Building and Testing",
	"tags": [],
	"description": "",
	"content": "Submariner strives to be an open, welcoming community for developers. Substantial tooling is provided to ease the contribution experience.\nStandard Development Environment Submariner provides a standard, shared development environment suitable for all local work. The same environment is used in CI.\nThe submariner-io/shipyard project contains the logic to build the base container images used across all submariner-io repositories.\nLearn more about working with Shipyard here.\nPrescribed Tasks via Make Targets Make targets are provided to further ease the process of using the shared development environment. The specific make targets available might differ by repository. For any submariner-io repository, see the Makefile at the root of the repository for the supported targets and the .travis.yml file for the targets actually used in CI.\nCommon Build and Testing Targets All submariner-io/* repositories provide a standard set of Make targets for similar building and testing actions.\nLinting To run static Go linting (goimports, golangci-lint):\nmake validate Unit tests To run Go unit tests:\nmake test Multi-Cluster KIND Based Environment Shipyard provides a basic target that creates a KIND based multi-cluster environment, without any special deployment (apart from the default K8s):\nmake clusters Find out more about Shipyard\u0026rsquo;s clusters target.\nMulti-Cluster Submariner Deployment Shipyard provides a basic target that deploys submariner on a KIND based multi-cluster environment (if one isn\u0026rsquo;t yet created, this target will first invoke the clusters target to do so): as well):\nmake deploy Find out more about Shipyard\u0026rsquo;s deploy target.\nEnd-to-End Tests To run functional end-to-end tests with a full multi-cluster deployment (if one isn\u0026rsquo;t yet deployed, this target will first invoke the deploy target to do so):\nmake e2e Optionally, you can specify flags to control the execution of the end-to-end testing and deployment (if it wasn\u0026rsquo;t run separately). Currently these flags are project-specific, so consult the project\u0026rsquo;s Makefile to learn which flags are supported. The flags can be combined or used separately, or not at all (in which case default values apply).\nFor example, here\u0026rsquo;s a flag variation used in submariner-io/submariner CI:\nmake e2e deploytool=helm globalnet=true Environment Clean Up To clean up all the KIND clusters deployed in any of the previous steps, use:\nmake cleanup Learn more about Shipyard\u0026rsquo;s cleanup target here.\nsubmariner-io/submariner Building Engine, Routeagent, and Globalnet Go binaries To build the submariner-route-agent, submariner-engine, and submariner-globalnet Go binaries, in the submariner-io/submariner repository:\nmake build There is an optional flag to build with debug flags set:\nmake build --build_debug=true Building Engine, Routeagent, and Globalnet container images To build the submariner/submariner, submariner/submariner-route-agent, and submariner/submariner-globalnet container images, in the submariner-io/submariner repository:\nmake images submariner-io/submariner-operator Building the Operator and subctl To build the submariner-operator container image and the subctl Go binary, in the submariner-io/submariner-operator repository:\nmake build submariner-io/lighthouse Building Lighthouse Controller, CoreDNS and DNSServer container images To build the lighthouse-agent and lighthouse-coredns container images, in the submariner-io/lighthouse repository:\nmake build-agent build-coredns submariner-io/shipyard Building dapper-base container image To build the base container image used in the shared developer and CI enviroment, in the submariner-io/shipyard:\nmake dapper-image "
},
{
	"uri": "/contributing/release-process/",
	"title": "Release Process",
	"tags": [],
	"description": "",
	"content": "This section explains the necessary steps to make a submariner release. It is assumed that you are familiar with the submariner project and the various repositories.\nStep 1: Create a Submariner Release Assuming that you have an existing submariner git directory, the following steps create a release named \u0026ldquo;Globalnet Overlapping IP support RC0\u0026rdquo; with version v0.2.0-rc0 based on the master branch.\ncd submariner git stash git remote add upstream ssh://git@github.com/submariner-io/submariner git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner?tab=tags\n Step 2: Create a Lighthouse Release To create lighthouse release artifacts follow the steps below.\nBuild Lighthouse Controller Assuming that you have an existing lighthouse git directory, run the following steps .\ncd lighthouse git stash git remote add upstream ssh://git@github.com/submariner-io/lighthouse git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here\n https://github.com/submariner-io/lighthouse/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here\n https://travis-ci.com/github/submariner-io/lighthouse/branches\n For this example the build can be found here.\nVerify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available on quay.io at:\n https://quay.io/repository/submariner/lighthouse-controller?tab=tags https://quay.io/repository/submariner/lighthouse-coredns?tab=tags\n Build Lighthouse CoreDNS  Get the coredns repository and change to the folder  go get github.com/openshift/coredns cd $GOPATH/src/github.com/openshift/coredns Build the image and push  COREDNS_VERSION=\u0026lt;VERSION-NO\u0026gt; LH_COREDNS_VERSION=\u0026lt;LH-VERSION-NO\u0026gt; COREDNS_IMAGE=\u0026#34;lighthouse-coredns:${LH_COREDNS_VERSION}\u0026#34; git checkout -b ${COREDNS_VERSION} sed -i \u0026#39;/^kubernetes:kubernetes/a lighthouse:github.com/submariner-io/lighthouse/plugin/lighthouse\u0026#39; plugin.cfg sed -i \u0026#39;/^github.com/aws/aws-sdk-go/a github.com/submariner-io/lighthouse v0.2.0\u0026#39; go.mod sed -i \u0026#39;$a replace\\ k8s.io\\/apimachinery\\ =\u0026gt;\\ k8s.io\\/apimachinery\\ v0.0.0-20190313205120-d7deff9243b1\u0026#39; go.mod sed -i \u0026#39;$a replace\\ github.com\\/openzipkin-contrib\\/zipkin-go-opentracing\\ =\u0026gt;\\ github.com\\/openzipkin-contrib\\/zipkin-go-opentracing\\ v0.3.5\u0026#39; go.mod docker build -f Dockerfile.openshift -t quay.io/submariner/${COREDNS_IMAGE} . docker push quay.io/submariner/${COREDNS_IMAGE} Build Cluster-DNS-Operator  Clone the cluster-dns-operator repository and create a version branch  git clone https://github.com/submariner-io/cluster-dns-operator.git git checkout -b \u0026lt;VERSION-NO\u0026gt; Change the coredns and cluster dns operator versions.  \t- name: dns-operator terminationMessagePolicy: FallbackToLogsOnError image: quay.io/submariner/lighthouse-cluster-dns-operator:\u0026lt;VERSION-NO\u0026gt; command: - dns-operator env: - name: RELEASE_VERSION value: \u0026quot;0.0.1-snapshot\u0026quot; - name: IMAGE value: quay.io/submariner/lighthouse-coredns:\u0026lt;VERSION-NO\u0026gt; For example, https://github.com/submariner-io/cluster-dns-operator/pull/1/files#diff-8c85a5683e17f9599cbfc641cceaa040R33\nBuild a new image and upload it to quay.  REPO=quay.io/submariner/lighthouse-cluster-dns-operator:$VERSION make release-local Commit and push the changes and raise a PR.  git add . git commit -s git push HEAD:\u0026lt;VERSION-NO\u0026gt; Step 3: Update the Operator Version References and Create a Release Once the other builds have finished and you have 0.2.0-rc0 release tags for the submariner and lighthouse projects, you can proceed with changes to the operator.\nChange Referenced Versions Edit the operator versions file and change the project version constants to reference the new release, \u0026ldquo;0.2.0-rc0\u0026rdquo;.\ncd submariner-operator git stash git remote add upstream ssh://git@github.com/submariner-io/submariner-operator git fetch -a -v -t upstream git checkout remotes/upstream/master -B update-references-to-v0.2.0-rc0 vim pkg/versions/versions.go # make sure we are referencing the latest submariner code (we use it for verify-connectivity and the API) GO111MODULES=on go get github.com/submariner-io/submariner@v0.2.0-rc0 git push my-repo update-references-to-v0.2.0-rc0 Create a pull request, wait for the CI job to pass, and get approval/merge. See an example PR here\nCreate a Submariner-Operator Release Assuming you have an existing submariner-operator git directory, run the following steps:\ncd submariner-operator git stash git remote add upstream ssh://git@github.com/submariner-io/submariner-operator git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner-operator/tags\n A build for v0.2.0-rc0 should start and appear under the under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner-operator/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner-operator?tab=tags\n Create the Subctl Binaries Release git stash cd submariner-operator git fetch -a -v -t upstream git checkout v0.2.0-rc0 rm bin/subctl* make build-cross ls -la bin/subctl* At this point, you should see subctl binaries generated and listed for the various platforms under bin. Go to https://github.com/submariner-io/submariner-operator/tags, find the tag for v0.2.0-rc0 and select \u0026ldquo;Edit release\u0026rdquo; to the right. Then upload the generated subctl binaries.\nIf this is a pre-release, mark the checkbox \u0026ldquo;This is a pre-release\u0026rdquo;.\nStep 4: Verify the Version You can follow any of our quickstarts, for example this one\nStep 5: Announce Via E-Mail  https://bit.ly/submariner-dev https://bit.ly/submariner-users  Via Twitter  https://twitter.com/submarinerio  "
},
{
	"uri": "/architecture/service-discovery/",
	"title": "Service Discovery",
	"tags": [],
	"description": "",
	"content": "The Lighthouse project provides DNS discovery for Kubernetes clusters connected by Submariner in multi-cluster environments. The solution is compatible with any CNI (Container Network Interfaces) plugin.\nArchitecture The below diagram shows the basic Lighthouse architecture.\nLighthouse Agent The Lighthouse Agent runs in every cluster and accesses the Kubernetes API server running in the broker cluster to exchange service metadata information with other clusters. Local service information is exported to the broker and service information from other clusters is imported.\nWorkFlow The workflow is as follows:\n Lighthouse agent connects to the broker\u0026rsquo;s K8s API server. For every service in the local cluster, it creates a corresponding MultiClusterService resource and exports it to the broker to be consumed by other clusters. For every MultiClusterService resource in the broker exported from another cluster, it creates a copy of it in the local cluster.  Lighthouse Plugin Lighthouse plugin can be installed as an external plugin for CoreDNS, and will work along with the default Kubernetes plugin. It uses the MultiClusterService resources that are distributed by the controller for DNS resolution. The below diagram indicates a high-level architecture.\nWorkFlow The workflow is as follows.\n A pod tries to resolve a Service name. The Kubernetes plugin in CoreDNS will first try to resolve the request. If no records are present the request will be sent to Lighthouse plugin. The Lighthouse plugin will use its MultiClusterService cache to try to resolve the request. If a record exists it will be returned, otherwise the plugin will pass the request to the next plugin registered in CoreDNS.  "
},
{
	"uri": "/quickstart/openshift/",
	"title": "OpenShift (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0-rc2/subctl-v0.3.0-rc2-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Use cluster-a as broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster pods and services\nsubctl verify-connectivity cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/deployment/subctl/",
	"title": "Using Subctl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "The inner details of deploying a broker and connecting clusters to the broker are complicated, subctl automates and simplifies most of those details eliminating human error as much as possible. This is why subctl is the recommended deployment method, you can find a complete guide to the subctl tool here: subctl in detail. If you still believe helm works better for you, please go here.\nInstalling subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0-rc2/subctl-v0.3.0-rc2-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Deployment of broker Please remember, this cluster\u0026rsquo;s API should be accessible from all other clusters:\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; this will create:\n The submariner-k8s-broker namespace The Cluster and Endpoint (.submariner.io) CRDs in the cluster A service account in the namespace for subsequent subctl access. A random IPSEC PSK which will be stored only in the broker-info.subm file.  And generate a broker-info.subm file which contains the following elements:\n The API endpoint A CA certificate to for the API endpoint The service account token for accessing the API endpoint / submariner-k8s-broker namespace.  This cluster can also participate in the dataplane connectivity with the other clusters, but it will need to be joined (see following step)\n Joining clusters For each cluster you want to join:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm subctl will discover as much as it can, and ask you for any necessary detail it can\u0026rsquo;t figure out like the cluster ID which has to be different between all clusters.\nDiscovery Service discovery (via DNS and lighthouse project) is an experimental feature (developer preview), and the instructions to deploy with discovery can be found here\n"
},
{
	"uri": "/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.3.0 Lighthouse Service Discovery without KubeFed  This release is focused on removing the KubeFed dependency from Lighthouse, improving the user experience and adding experimental WireGuard support as an alternative to IPsec\n  Lighthouse no longer depends KubeFed. All metadata exchange is handled over the Broker as MultiClusterService CRs. Experimental Wireguard support has been added as a pluggable CableDriver option in addition to the current default IPsec. Submariner reports the active and passive gateways as a gateway.submariner.io CRD. The Submariner Operator reports a detailed status of the deployment. The gateway redundancy/failover tests are now enabled and stable in CI. Globalnet hostNetwork to remote globalIP is now supported. Previously, when a pod used hostNetworking it was unable to connect to a remote service via globalIP. A globalCIDR can be manually specified when joining a cluster with globalnet enabled. This enables CI speed optimizations via better parallelism. Operator and subctl are more robust via standard retries on updates. Subctl creates a new individual access token for every new joined cluster.  v0.2.0 Overlapping CIDR support  This release is focused on overlapping CIDR support between clusters\n  Support for Overlapping CIDRs between clusters (globalnet) Enhanced e2e scripts, which will be shared between repositories in the shipyard project (ongoing work) Improved e2e deployment by using a local registry. Refactoring to support pluggable drivers (in preparation for WireGuard support)  v0.1.1 Submariner with more light  This release has focused on stability for the Lighthouse support\n  Cleaner logging for the submariner-engine Cleaner logging for the submariner-route-agent Fixed issue with wrong token stored in subm file #244 Added flag to disable the OpenShift CVO #235 Fixed several service-discovery related bugs #194 , #167 Fixed several panics on nil network discovery Added checks to ensure the CIDRs for joining cluster don\u0026rsquo;t overlap with an existing ones. Fix context handling related to service-discovery / kubefed #180 Use the right CoreDNS image for OpenShift.  v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making https://github.com/submariner.io/lighthouse available as developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (#346, #348, #332) Migrated to Daemonsets for submariner gateway deployment Added support for hostNetwork to remote pod/service connectivity (#288) Auto detection and configuration of MTU for vx-submariner, jumbo frames support (#301) Support for updated strongswan (#288) Better iptables detection for some hosts (#227)   subctl and the submariner operator have the following improvements\n  support to verify-connectivity between two connected clusters deployment of submariner gateways based in daemonsets instead of deployments renaming submariner pods to \u0026ldquo;submariner-gateway\u0026rdquo; pods for clarity print version details on crash (subctl) stop storing IPSEC key on broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file version command for subctl nicer spinners during deployment (thanks to kind)  v0.0.3 \u0026ndash; KubeCon NA 2019  Submariner has been greatly enhanced to allow operators to deploy into Kubernetes clusters without the necessity for layer-2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). Subctl was created to make deployment of submariner easier.\n v0.0.1 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/quickstart/openshiftsd/",
	"title": "OpenShift with Service Discovery (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0-rc2/subctl-v0.3.0-rc2-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery follow the steps below.\nUse cluster-a as broker with service discovery enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --service-discovery Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b Verify Deployment To verify the deployment follow the steps below.\nkubectl --kubeconfig cluster-b/auth/kubeconfig create deployment nginx --image=nginx kubectl --kubeconfig cluster-b/auth/kubeconfig expose deployment nginx --port=80 kubectl --kubeconfig cluster-a/auth/kubeconfig run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx "
},
{
	"uri": "/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/reading_material/",
	"title": "Online Resources",
	"tags": [],
	"description": "",
	"content": "There are multiple presentations/demo recordings on Submariner available online.\nConference Presentations  KubeCon China 2019 slides and video recording KubeCon North America 2019 slides and video recording  Demo Recordings  Deploying Submariner with subctl Connecting hybrid Kubernetes clusters using Submariner Cross-cluster service discovery in Submariner using Lighthouse  Blogs  Kubernetes Multi-Cloud and Multi-Cluster Connectivity with Submariner  If you find additional material that isn\u0026rsquo;t listed here, please feel free to add it to this page by editing it. The full contributing guide can be found here.\n"
},
{
	"uri": "/quickstart/openshiftgn/",
	"title": "OpenShift with Service Discovery and Globalnet (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create cluster A This step will create a cluster named \u0026ldquo;cluster-a\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a This may take some time to complete so you can move on to the next section in parallel if you wish.\nCreate cluster B This step will create a cluster named \u0026ldquo;cluster-b\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-b And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0-rc2/subctl-v0.3.0-rc2-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery and Globalnet The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery and support for overlapping CIDRs follow the steps below.\nUse cluster-a as broker with service discovery and globalnet enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --service-discovery --globalnet Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid west subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid east Verify Deployment To verify the deployment follow the steps below.\nkubectl --kubeconfig cluster-b/auth/kubeconfig create deployment nginx --image=nginx kubectl --kubeconfig cluster-b/auth/kubeconfig expose deployment nginx --port=80 kubectl --kubeconfig cluster-a/auth/kubeconfig run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx "
},
{
	"uri": "/contributing/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": "Submariner Community Code of Conduct Submariner follows the CNCF Code of Conduct.\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting one or more of the Submariner Committers or Owners.\n"
},
{
	"uri": "/contributing/community-membership/",
	"title": "Community Membership",
	"tags": [],
	"description": "",
	"content": "This is a stripped-down version of the Kubernetes Community Membership process.\nAlthough we aspire to follow the K8s process, some parts are not currently relevant to our structure or possible with our tooling:\n The SIG and subproject abstraction layers don\u0026rsquo;t apply to Submariner. Submariner is treated as a single project with file-based commit rights, not a \u0026ldquo;project\u0026rdquo; per repository. We hope to eventually move to K8s OWNERS and Prow, but until we do so we can\u0026rsquo;t support advanced role-based automation (reviewers vs approvers; PR workflow commands like /okay-to-test, /lgtm, /approved).   This doc outlines the various responsibilities of contributor roles in Submariner.\n   Role Responsibilities Requirements Defined by     Member Active contributor in the community Sponsored by 2 committers, multiple contributions to the project Submariner GitHub org member   Committer Approve contributions from other members History of review and authorship CODEOWNERS file entry   Owner Set direction and priorities for the project Demonstrated responsibility and excellent technical judgement for the project Submariner-owners GitHub team member    New Contributors New contributors should be welcomed to the community by existing members, helped with PR workflow, and directed to relevant documentation and communication channels.\nEstablished Community Members Established community members are expected to demonstrate their adherence to the principles in this document, familiarity with project organization, roles, policies, procedures, conventions, etc., and technical and/or writing ability. Role-specific expectations, responsibilities, and requirements are enumerated below.\nMember Members are continuously active contributors in the community. They can have issues and PRs assigned to them and participate through GitHub teams. Members are expected to remain active contributors to the community.\nDefined by: Member of the Submariner GitHub organization.\nRequirements  Enabled two-factor authentication on their GitHub account Have made multiple contributions to the project or community. Contribution may include, but is not limited to:  Authoring or reviewing PRs on GitHub Filing or commenting on issues on GitHub Contributing to community discussions (e.g. meetings, Slack, email discussion forums, Stack Overflow)   Subscribed to submariner-dev@googlegroups.com Have read the contributor guide Actively contributing Sponsored by 2 committers. Note the following requirements for sponsors:  Sponsors must have close interactions with the prospective member - e.g. code/design/proposal review, coordinating on issues, etc. Sponsors must be committers in at least 1 CODEOWNERS file either in any repo in the Submariner org   Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the member template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Responsibilities and Privileges  Responsive to issues and PRs assigned to them Responsive to mentions of teams they are members of Active owner of code they have contributed (unless ownership is explicitly transferred)  Code is well tested Tests consistently pass Addresses bugs or issues discovered after code is accepted   They can be assigned to issues and PRs, and people can ask members for reviews  Note: Members who frequently contribute code are expected to proactively perform code reviews and work towards becoming a committer.\nCommitters Committers are able to review code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles.\nUntil automation supports approvers vs reviewers: They also review for holistic acceptance of a contribution including: backwards / forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, etc.\nDefined by: Entry in an CODEOWNERS file in a repo owned by the Submariner project.\nCommitter status is scoped to a part of the codebase.\nRequirements The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Member for at least 3 months Primary reviewer for at least 5 PRs to the codebase Reviewed at least 20 substantial PRs to the codebase Knowledgeable about the codebase Sponsored by two committers or project owners  With no objections from other committers or project owners   May either self-nominate or be nominated by a committer/owner Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the committer template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers/owners reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Responsibilities and Privileges The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Responsible for project quality control via code reviews  Focus on code quality and correctness, including testing and factoring Until automation supports approvers vs reviewers: Focus on holistic acceptance of contribution such as dependencies with other features, backwards / forwards compatibility, API and flag definitions, etc   Expected to be responsive to review requests as per community expectations Assigned PRs to review related to project of expertise Assigned test bugs related to project of expertise Granted \u0026ldquo;read access\u0026rdquo; to submariner repo May get a badge on PR and issue comments Demonstrate sound technical judgement Mentor contributors and reviewers  Project Owner Project owners are the technical authority for the Submariner project. They MUST have demonstrated both good judgement and responsibility towards the health the project. Project owners MUST set technical direction and make or approve design decisions for the project - either directly or through delegation of these responsibilities.\nDefined by: Member of the submariner-owners GitHub team.\nRequirements Unlike the roles outlined above, the owners of the project are typically limited to a relatively small group of decision makers and updated as fits the needs of the project.\nThe following apply to people who would be an owner:\n Deep understanding of the technical goals and direction of the project Deep understanding of the technical domain of the project Sustained contributions to design and direction by doing all of:  Authoring and reviewing proposals Initiating, contributing and resolving discussions (emails, GitHub issues, meetings) Identifying subtle or complex issues in designs and implementation PRs   Directly contributed to the project through implementation and / or review  Responsibilities and Privileges The following apply to people who would be an owner:\n Make and approve technical design decisions for the project Set technical direction and priorities for the project Define milestones and releases Mentor and guide committers and contributors to the project Ensure continued health of project  Adequate test coverage to confidently release Tests are passing reliably (i.e. not flaky) and are fixed when they fail   Ensure a healthy process for discussion and decision making is in place Work with other project owners to maintain the project\u0026rsquo;s overall health and success holistically  "
},
{
	"uri": "/deployment/helm/",
	"title": "Using Helm",
	"tags": [],
	"description": "",
	"content": "This is a placeholder, in the meanwhile you can check out https://github.com/submariner-io/submariner-charts\n"
},
{
	"uri": "/quickstart/kind/",
	"title": "KIND (Local Environment)",
	"tags": [],
	"description": "",
	"content": "Locally Testing With KIND KIND is a tool to run local Kubernetes clusters inside Docker container nodes.\nSubmariner provides (via Shipyard) scripts that deploy 3 Kubernetes clusters locally - 1 broker and 2 data clusters with the Submariner dataplane components deployed on all the clusters.\nDocker must be installed and running on your computer.\n Deploying a Basic Environment If you wish to deploy just a basic multi cluster environment, run:\ngit clone https://github.com/submariner-io/shipyard cd shipyard make deploy At the end of the deployment you\u0026rsquo;ll have a very basic environment that you can experiment with.\nDeploying Like a Submariner To deploy the environment like Submariner does it, run:\ngit clone https://github.com/submariner-io/submariner cd submariner make e2e This will deploy 3 Kubernetes clusters and run basic end-to-end tests. The environment will remain available after the tests complete.\nMore details on testing can be found in the testing guide.\n"
},
{
	"uri": "/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "This is a preliminary community roadmap, it\u0026rsquo;s not written in stone, but it can serve as a guideline about what\u0026rsquo;s ahead.\nPlease see details of previous releases here\nv0.4.0  In planning https://github.com/orgs/submariner-io/projects/7  Future Releases  Libreswan support Auto detecting NAT vs non-NAT scenarios. Supporting different ports for IPSEC for each cluster Measuring and improving A/P HA (different scenarios) Support for network policies via coastguard Monitoring and reporting of tunnel endpoints (status of connection, bandwidth, pps, etc..) Monitoring connectivity over port 4800 between routeagent nodes. Support for non-kubeproxy / iptables based implementations, starting with OVN HA Active/Active gateway support (ECMP?) (keep in mind non-iptables-kubeproxy based implementations) Testing with Istio  "
},
{
	"uri": "/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "Talk to Us! We would love to hear from you, how you are using Submariner, and what we can do to make it better.\nGitHub Project Check out the project and consider contributing. Pick up an issue to work on or propose an enhancement by reporting a new issue; once your code is ready to be reviewed, you can propose a pull request. You can find a good guide about the GitHub workflow here.\n#submariner Share your ideas in the #submariner channel in Kubernetes\u0026rsquo; Slack. If you need it, you can request an invite to K8S slack instance.\nCommunity Calendar As a member of the Submariner Community, join any of our community meetings - no registration required. The weekly Submariner Community Meeting (Tuesdays at 5:00pm CET) is a good place to start.\nMailing List Join the developer mailing list.\n"
},
{
	"uri": "/",
	"title": "Submariner",
	"tags": [],
	"description": "",
	"content": "Submariner Submariner enables direct networking between pods and services in different Kubernetes clusters, either on premise or in the cloud. Why Submariner? As Kubernetes gains adoption, teams are finding they must deploy and manage multiple clusters to facilitate features like geo-redundancy, scale, and fault isolation for their applications. With Submariner, your applications and services can span multiple cloud providers, data centers, and regions.\nSubmariner is completely open source, and designed to be network plugin (CNI) agnostic.\nWhat Submariner Provides?  Cross-cluster L3 connectivity using encrypted VPN tunnels Service Discovery across clusters subctl, a friendly deployment tool Support for interconnecting clusters with overlapping CIDRs  Check the Quickstart guide section for deployment instructions.\n "
},
{
	"uri": "/deployment/with-discovery/",
	"title": "With Discovery (experimental)",
	"tags": [],
	"description": "",
	"content": "Deployment with discovery will include the (lighthouse components.\nProject status: The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster may disable some of the operator features.\n The Lighthouse project helps in cross-cluster service discovery. It has the below additional dependencies\n kubectl installed.  Deploying Submariner with Lighthouse Deploy Broker subctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; --service-discovery Join Clusters To join all the other clusters with the broker cluster, run subctl using the broker-info.subm generated in the folder from which the previous step was run.\nsubctl join --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-DATA-CLUSTER\u0026gt; broker-info.subm As for a normal deployment, subctl will try to figure out all necessary information and will ask for anything it can\u0026rsquo;t figure out.\n"
},
{
	"uri": "/quickstart/openshift/create_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b "
},
{
	"uri": "/quickstart/openshift/ready_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n "
},
{
	"uri": "/quickstart/openshift/setup_openshift/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n "
},
{
	"uri": "/architecture/broker/",
	"title": "Broker",
	"tags": [],
	"description": "",
	"content": "Submariner uses a central broker to facilitate the exchange of metadata information between connected clusters. The broker is basically a set of custom resource definitions (CRDs) backed by the kubernetes datastore. The broker is a singleton component that is deployed on one of the clusters whose Kubernetes API must be accessible by all of the connected clusters.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/architecture/gateway-engine/",
	"title": "Gateway Engine",
	"tags": [],
	"description": "",
	"content": "The gateway engine component is deployed in each connected cluster and is responsible for establishing IPsec tunnels to other clusters. Instances of the gateway engine run on specifically designated nodes in a cluster of which there may be more than one for fault tolerance. There is only one active gateway engine instance in a cluster - the others await in standby mode ready to take over should the active instance fail. The active gateway engine communicates with the central broker to advertise its endpoint to the other connected clusters and to learn about the gateway endpoints on the other clusters.\n"
},
{
	"uri": "/architecture/route-agent/",
	"title": "Route Agent",
	"tags": [],
	"description": "",
	"content": "The Route Agent runs on every node in each connected cluster. It is responsible for setting up VxLAN tunnels and routing the cross cluster traffic from the node to the cluster’s active Gateway Engine which subsequently sends the traffic to the destination cluster.\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]