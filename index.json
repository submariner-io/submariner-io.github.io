[
{
	"uri": "/architecture/broker/",
	"title": "Broker",
	"tags": [],
	"description": "",
	"content": "Submariner uses a central Broker component to facilitate the exchange of metadata information between Gateway Engines deployed in participating clusters. The Broker is basically a set of Custom Resource Definitions (CRDs) backed by the Kubernetes datastore. The Broker also defines a ServiceAccount and RBAC components to enable other Submariner components to securely access the Broker\u0026rsquo;s API. There are no Pods or Services deployed with the Broker.\nSubmariner defines two CRDs that are exchanged via the Broker: Endpoint and Cluster. The Endpoint CRD contains the information about the active Gateway Engine in a cluster, such as its IP, needed for clusters to connect to one another. The Cluster CRD contains static information about the originating cluster, such as its Service and Pod CIDRs.\nThe Broker is a singleton component that is deployed on a cluster whose Kubernetes API must be accessible by all of the participating clusters. If there is a mix of on-premises and public clusters, the Broker can be deployed on a public cluster. The Broker cluster may be one of the participating clusters or a standalone cluster without the other Submariner components deployed. The Gateway Engine components deployed in each participating cluster are configured with the information to securely connect to the Broker cluster\u0026rsquo;s API.\n"
},
{
	"uri": "/architecture/gateway-engine/",
	"title": "Gateway Engine",
	"tags": [],
	"description": "",
	"content": "The Gateway Engine component is deployed in each participating cluster and is responsible for establishing secure tunnels to other clusters.\nThe Gateway Engine has a pluggable architecture for the cable engine component that maintains the tunnels. The following implementations are available:\n an IPsec implementation using strongSwan (via the goStrongswanVici library); this is currently the default; an IPsec implementation using Libreswan; an implementation for WireGuard.  Instances of the Gateway Engine run on specifically designated nodes in a cluster of which there may be more than one for fault tolerance. Submariner supports active/passive High Availability for the Gateway Engine component, which means that there is only one active Gateway Engine instance at a time in a cluster. They perform a leader election process to determine the active instance and the others await in standby mode ready to take over should the active instance fail.\nThe Gateway Engine is deployed as a DaemonSet that is configured to only run on nodes labelled with \u0026ldquo;submariner.io/gateway=true\u0026rdquo;.\n The active Gateway Engine communicates with the central Broker to advertise its Endpoint and Cluster resources to the other clusters connected to the Broker, also ensuring that it is the sole Endpoint for its cluster. The Route Agent Pods running in the cluster learn about the local Endpoint and setup the necessary infrastructure to route cross-cluster traffic from all nodes to the active Gateway Engine node. The active Gateway Engine also establishes a watch on the Broker to learn about the active Endpoint and Cluster resources advertised by the other clusters. Once two clusters are aware of each other\u0026rsquo;s Endpoints, they can establish a secure tunnel through which traffic can be routed.\nGateway Failover If the active Gateway Engine fails, another Gateway Engine on one of the other designated nodes will gain leadership and perform reconciliation to advertise its Endpoint and to ensure that it is the sole Endpoint. The remote clusters will learn of the new Endpoint via the Broker and establish a new tunnel. Similarly, the Route Agent Pods running in the local cluster automatically update the route tables on each node to point to the new active Gateway node in the cluster.\nThe impact on datapath for various scenarios in a kind setup are captured in the following spreadsheet.\n"
},
{
	"uri": "/architecture/route-agent/",
	"title": "Route Agent",
	"tags": [],
	"description": "",
	"content": "The Route Agent component runs on every worker node in each participating cluster. It is responsible for setting up VXLAN tunnels and routing the cross cluster traffic from the node to the cluster’s active Gateway Engine which subsequently sends the traffic to the destination cluster.\nWhen running on the same node as the active Gateway Engine, Route Agent creates a VXLAN VTEP interface to which Route Agent instances running on the other worker nodes in the local cluster connect by establishing a VXLAN tunnel with the VTEP of the active Gateway Engine node. The MTU of the VXLAN tunnel is configured based on the MTU of the default interface on the host minus the VXLAN overhead.\nRoute Agents use Endpoint resources synced from other clusters to configure routes and to program the necessary IP table rules to enable full cross-cluster connectivity.\nWhen the active Gateway Engine fails and a new Gateway Engine takes over, Route Agents will automatically update the route tables on each node to point to the new active Gateway Engine node.\n"
},
{
	"uri": "/architecture/globalnet/",
	"title": "Globalnet Controller",
	"tags": [],
	"description": "",
	"content": "Introduction Submariner is a tool built to connect overlay networks of different Kubernetes clusters. These clusters can be on different public clouds or on-premises. An important use case for Submariner is to connect disparate independent clusters into a ClusterSet.\nHowever, by default, a limitation of Submariner is that it doesn\u0026rsquo;t handle overlapping CIDRs (ServiceCIDR and ClusterCIDR) across clusters. Each cluster must use distinct CIDRs that don\u0026rsquo;t conflict or overlap with any other cluster that is going to be part of the ClusterSet.\nThis is largely problematic because most actual deployments use the default CIDRs for a cluster so every cluster ends up using the same CIDRs. Changing CIDRs on existing clusters is a very disruptive process and requires a cluster restart. So Submariner needs a way to allow clusters with overlapping CIDRs to connect together.\nArchitecture To support overlapping CIDRs in connected clusters, Submariner has a component called Global Private Network, Globalnet (globalnet). This Globalnet is a virtual network specifically to support Submariner\u0026rsquo;s multi-cluster solution with a global CIDR. Each cluster is given a subnet from this virtual Global Private Network, configured as new cluster parameter GlobalCIDR (e.g. 169.254.0.0/16) which is configurable at time of deployment. User can also manually specify GlobalCIDR for each cluster that is joined to the Broker using the flag globalnet-cidr passed to subctl join command. If Globalnet is not enabled in the Broker or if a GlobalCIDR is preconfigured in the cluster, the supplied globalnet-cidr will be ignored.\nOnce configured, each Service and Pod that requires cross-cluster access is allocated an IP, named globalIp, from this GlobalCIDR that is annotated on the Pod/Service object. This globalIp is used for all cross-cluster communication to and from a Pod and the globalIp of a remote Service. Routing and iptable rules are configured to use the globalIp for ingress and egress. All address translations occur on the Gateway node.\nUnlike vanilla Submariner, where Pod to Pod connectivity is also supported, Globalnet only supports Pod to remote Service connectivity using globalIps.\n submariner-globalnet Submariner Globalnet is a component that provides cross-cluster connectivity from Pods to remote Services using their globalIps. Compiled as binary submariner-globalnet, it is responsible for maintaining a pool of global IPs, allocating IPs from the GlobalIp pool to pods and services, annotating Services and Pods with their globalIp, and configuring the required rules on the gateway node to provide cross-cluster connectivity using globalIps. Globalnet also supports connectivity from the Nodes (including Pods that use HostNetworking) to globalIp of remote Services. It mainly consists of two key components: the IP Address Manager and Globalnet.\nIP Address Manager (IPAM) The IP Address Manager (IPAM) component does the following:\n Creates a pool of IP addresses based on the GlobalCIDR configured on cluster. On creation of a Pod/Service, allocates a globalIp from the GlobalIp pool. Annotates the Pod/Service with submariner.io/globalIp=\u0026lt;global-ip\u0026gt;. On deletion of a Pod/Service, releases its globalIp back to the pool.  Globalnet This component is responsible for programming the routing entries, iptable rules and does the following:\n Creates initial iptables chains for Globalnet rules. Whenever a Pod is annotated with a globalIp, creates an egress SNAT rule to convert the source Ip from the Pod\u0026rsquo;s Ip to the Pod\u0026rsquo;s globalIp on the Gateway Node. Whenever a Service is annotated with a globalIp, creates an ingress rule to direct all traffic destined to the Service\u0026rsquo;s globalIp to the Service\u0026rsquo;s kube-proxy iptables chain which in turn directs traffic to Service\u0026rsquo;s backend Pods. On deletion of pod/service, clean up the rules from the gateway node.  Globalnet currently relies on kube-proxy and thus will only work with deployments that use kube-proxy.\nService Discovery - Lighthouse Connectivity is only part of the solution as pods still need to know the IPs of services on remote clusters.\nThis is achieved by enhancing lighthouse with support for Globalnet. The Lighthouse controller adds the service\u0026rsquo;s globalIp to the ServiceImport object that is distributed to all clusters. The lighthouse plugin then uses the Service\u0026rsquo;s globalIp when replying to DNS queries for the Service.\nBuilding Nothing extra needs to be done to build submariner-globalnet as it is built with the standard Submariner build.\n"
},
{
	"uri": "/contributing/website/style_guide/",
	"title": "Docs Style Guide",
	"tags": [],
	"description": "",
	"content": "Documentation Style Guide This guide is meant to help keep our documentation consistent and ease the contribution and review process.\nSubmariner follows the Kubernetes Documentation Style Guide wherever relevant. This is a Submariner-specific extension of those practices.\nSubmariner.io Word List A list of Submariner-specific terms and words to be used consistently across the site.\n   Term Usage     Admiral The project name Admiral should always be capitalized.   Broker The design pattern component Broker should always be capitalized.   ClusterSet The Kubernetes object ClusterSet proposed in KEP1645 should always be CamelCase and formatted in code style.   Cluster set The words \u0026ldquo;cluster set\u0026rdquo; should be used as a term for a group of clusters, but not the proposed Kubernetes object.   Coastguard The project name Coastguard should always be capitalized.   Globalnet The feature name Globalnet is one word, and so should always be capitalized and should have a lowercase \u0026ldquo;n\u0026rdquo;.   iptables The application iptables consistently uses all-lowercase. Follow their convention, but avoid starting a sentence with \u0026ldquo;iptables\u0026rdquo;.   K8s The project nickname K8s should typically be expanded to \u0026ldquo;Kubernetes\u0026rdquo;.   kind The tool kind consistently uses all-lowercase. Follow their convention, but avoid starting a sentence with \u0026ldquo;kind\u0026rdquo;.   Lighthouse The project name Lighthouse should always be capitalized.   Operator The design pattern Operator should always be capitalized.   Shipyard The project name Shipyard should always be capitalized.   subctl The artifact subctl should not be capitalized and should be formatted in code style.   Submariner The project name Submariner should always be capitalized.    Pronunciation of \u0026ldquo;Submariner\u0026rdquo; Both the \u0026ldquo;Sub-mariner\u0026rdquo; (\u0026ldquo;Sub-MARE-en-er\u0026rdquo;, like the watch) and \u0026ldquo;Submarine-er\u0026rdquo; (\u0026ldquo;Sub-muh-REEN-er\u0026rdquo;, like the Navy job) pronunciations are okay.\nThe second option, \u0026ldquo;Submarine-er\u0026rdquo;, has historically been more common as Chris Kim (the initial creator) imagined the iconography of the project as related to submarine cables.\n"
},
{
	"uri": "/architecture/service-discovery/",
	"title": "Service Discovery",
	"tags": [],
	"description": "",
	"content": "The Lighthouse project provides DNS discovery for Kubernetes clusters connected by Submariner in multi-cluster environments.\nArchitecture The below diagram shows the basic Lighthouse architecture.\nLighthouse Agent The Lighthouse Agent runs in every cluster and accesses the Kubernetes API server running in the Broker cluster to exchange service metadata information with other clusters. Local service information is exported to the Broker and service information from other clusters is imported.\nAgent Workflow The workflow is as follows:\n Lighthouse agent connects to the Broker\u0026rsquo;s Kubernetes API server. For every Service in the local cluster for which a ServiceExport has been created, the agent creates a corresponding ServiceImport resource and exports it to the Broker to be consumed by other clusters. For every ServiceImport resource in the Broker exported from another cluster, it creates a copy of it in the local cluster.  Lighthouse DNS Server The Lighthouse DNS server runs as an external DNS server which owns the domain clusterset.local. KubeDNS is configured to forward any request sent to clusterset.local to the Lighthouse DNS server, which uses the ServiceImport resources that are distributed by the controller for DNS resolution. The Lighthouse DNS server uses a round robin algorithm for IP selection to distribute the load evenly across the clusters.\nServer Workflow The workflow is as follows.\n A Pod tries to resolve a Service name using the domain name clusterset.local KubeDNS forwards the request to the Lighthouse DNS server. The Lighthouse DNS server will use its ServiceImport cache to try to resolve the request. If a record exists it will be returned, else an NXDomain error will be returned.  "
},
{
	"uri": "/quickstart/",
	"title": "Quickstart Guide",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes clusters. For more information about Submariner\u0026rsquo;s architecture, please refer to the Architecture section.\nThe Broker The Broker is an API to which all participating clusters are given access to, and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a cluster, and the reachable cluster IPs from the endpoint.  The Broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe Submariner Deployment on a Cluster Once Submariner is deployed on a cluster with the proper credentials to the Broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n At least two Kubernetes clusters, one of which is designated to serve as the central Broker that is accessible by all of your connected clusters; this can be one of your connected clusters, or a dedicated cluster. Non-overlapping Pod and Service CIDRs between clusters. This is to prevent routing conflicts. For cases where addresses do overlap, Globalnet can be set up. IP reachability between the gateway nodes. When connecting two clusters, at least one of the clusters should have a publicly routable IP address designated to the Gateway node. This is needed for creating the IPsec tunnel between the clusters. The default ports used by IPsec are 4500/UDP and 500/UDP. For clusters behind corporate firewalls that block the default ports, Submariner also supports NAT Traversal (NAT-T) with the option to set custom non-standard ports like 4501/UDP and 501/UDP. Submariner uses port 4800/UDP to encapsulate traffic from the Worker nodes to the Gateway nodes and ensuring that Pod IP addresses are preserved. Ensure that firewall configuration allows 4800/UDP across all the Worker nodes. Worker node IPs on all connected clusters must be outside of the Pod/Service CIDR ranges.  An example of three clusters configured to use with Submariner (without Globalnet) would look like the following:\n   Cluster Name Provider Pod CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east OnPrem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Support Matrix Submariner is designed to be cloud provider agnostic, and should run in any standard Kubernetes cluster. Presently, Submariner has been tested with the following network (CNI) Plugins that leverage kube-proxy with iptables mode:\n OpenShift-SDN Weave Flannel Canal Calico. Please refer to the following section for deployment instructions.  Deployment The available methods for deployment are:\n subctl Operator Helm  subctl greatly simplifies the deployment of Submariner, and is therefore the recommended deployment method.\n"
},
{
	"uri": "/contributing/shipyard/first_time/",
	"title": "Adding Shipyard to a Project",
	"tags": [],
	"description": "",
	"content": "To use Shipyard in your project, it\u0026rsquo;s easiest to use Dapper and Make. To use Dapper you\u0026rsquo;ll need a specific Dockerfile that Dapper consumes to create a consistent environment based upon Shipyard\u0026rsquo;s base image. To use Make you\u0026rsquo;ll need some commands to enable Dapper and also include the targets which ship in the base image.\nDockerfile.dapper The project should have a Dockerfile.dapper Dockerfile which builds upon quay.io/submariner/shipyard-dapper-base.\nFor example:\nFROMquay.io/submariner/shipyard-dapper-baseENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT\u0026#34; \\  DAPPER_SOURCE=\u0026lt;your source directory\u0026gt; DAPPER_DOCKER_SOCKET=trueENV DAPPER_OUTPUT=${DAPPER_SOURCE}/outputWORKDIR${DAPPER_SOURCE}ENTRYPOINT [\u0026#34;./scripts/entry\u0026#34;]CMD [\u0026#34;ci\u0026#34;]You can also refer to the project\u0026rsquo;s own Dockerfile.dapper as an example.\nBuilding The Base Image To build the base container image used in the shared developer and CI enviroment, simply run:\nmake dapper-image Makefile The project\u0026rsquo;s Makefile should include targets to run everything in Dapper. These are defined in Shipyard\u0026rsquo;s Makefile.dapper which can be copied as is into your project and included in the Makefile. To use Shipyard\u0026rsquo;s built-in targets available in the base Dapper image, include the Makefile.inc file in the project\u0026rsquo;s Makefile within the section where the Dapper environment is detected.\nThe simplest Makefile would look like this:\nifneq (,$(DAPPER_HOST_ARCH)) # Running in Dapper  include $(SHIPYARD_DIR)/Makefile.inc else # Not running in Dapper  include Makefile.dapper endif # Disable rebuilding Makefile Makefile Makefile.dapper Makefile.inc: ; You can also refer to the project\u0026rsquo;s own Makefile as an example.\n"
},
{
	"uri": "/contributing/building_testing/",
	"title": "Building and Testing",
	"tags": [],
	"description": "",
	"content": "Submariner strives to be an open, welcoming community. Substantial tooling is provided to ease the contribution experience.\nStandard Development Environment Submariner provides a standard, shared environment for both development and CI that is maintained in the Shipyard project.\nLearn more about working with Shipyard here.\nBuilding and Testing Submariner provides a set of Make targets for building and testing in the standard development environment.\nLinting To run all linting:\nmake lint There are also Make targets for each type of linting:\nmake gitlint golangci-lint markdownlint yamllint See the linter configuration files at the root of each repository for details about which checks are enabled.\nNote that a few linters only run in CI via GitHub Actions and are not available in the standard development environment.\nUnit Tests To run Go unit tests:\nmake unit Building To build the Go binaries provided by a repository:\nmake build To package those Go binaries into container images:\nmake images Note that Submariner will automatically rebuild binaries and images when they have been modified and are required by tests.\nEnd-to-End Tests To run functional end-to-end tests with a full multi-cluster deployment:\nmake e2e Different types of deployments can be configured with using flags:\nmake e2e using=helm,globalnet See Shipyard\u0026rsquo;s Makefile.inc for the currently-supported using flags.\nTo create a multi-cluster deployment and install Submariner but not run tests:\nmake deploy To create a multi-cluster deployment without Submariner:\nmake clusters To clean up a multi-cluster deployment from one of the previous commands:\nmake cleanup Shell Session in Development Environment To jump into a shell in Submariner\u0026rsquo;s standard development environment:\nmake shell "
},
{
	"uri": "/quickstart/kind/",
	"title": "kind (Local Environment)",
	"tags": [],
	"description": "",
	"content": "Deploy kind+Submariner Locally kind is a tool to run local Kubernetes clusters inside Docker container nodes.\nSubmariner provides (via Shipyard) scripts that deploy 3 Kubernetes clusters locally - 1 Broker and 2 data clusters with the Submariner dataplane components deployed on all the clusters.\nDocker must be installed and running on your computer.\n Deploy Automatically To create kind clusters and deploy Submariner, run:\ngit clone https://github.com/submariner-io/submariner cd submariner make deploy Deploy Manually If you wish to try out Submariner deployment manually, an easy option is to create kind clusters using our scripts and deploy Submariner with subctl.\nCreate kind Clusters To create kind clusters, run:\ngit clone https://github.com/submariner-io/submariner cd submariner make clusters This creates 3 Kubernetes clusters, cluster1, cluster2 and cluster3.\nInstall subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Use cluster1 as Broker subctl deploy-broker --kubeconfig output/kubeconfigs/kind-config-cluster1 --service-discovery Join cluster2 and cluster3 to the Broker subctl join --kubeconfig output/kubeconfigs/kind-config-cluster2 broker-info.subm --clusterid cluster2 --disable-nat subctl join --kubeconfig output/kubeconfigs/kind-config-cluster3 broker-info.subm --clusterid cluster3 --disable-nat You now have a Submariner environment that you can experiment with.\nVerify Deployment Verify Automatically with subctl This will perform automated verifications between the clusters.\nsubctl verify output/kubeconfigs/kind-config-cluster2 output/kubeconfigs/kind-config-cluster3 --only service-discovery,connectivity --verbose Verify Manually To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster3.\nDeploy ClusterIP Service kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 create deployment nginx --image=nginx kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 expose deployment nginx --port=80 subctl export service --kubeconfig output/kubeconfigs/kind-config-cluster3 --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nkubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 create deployment nginx --image=nginx kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 expose deployment nginx --port=80 --cluster-ip=None subctl export service --kubeconfig output/kubeconfigs/kind-config-cluster3 --namespace default nginx Verify Run nettest from cluster2 to access the nginx service:\nkubectl --kubeconfig output/kubeconfigs/kind-config-cluster2 -n default run --generator=run-pod/v1 \\ tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 "
},
{
	"uri": "/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "Submariner provides a Go-based Kubernetes custom controller, called an Operator, that provides easy API-based installation and management. A command line utility, subctl, wraps the Operator to aid users with manual deployments and easy experimentation. subctl greatly simplifies the deployment of Submariner, and is therefore the recommended deployment method. For complete information about subctl, please refer to this page.\nIn addition to Operator and subctl, Submariner also provides Helm Charts.\nInstalling subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Deployment of the Broker The Broker is a set of Custom Resource Definitions (CRDs) backed by the Kubernetes datastore. The Broker must be deployed on a cluster whose Kubernetes API is accessible by all of the participating clusters:\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; This will create:\n The submariner-k8s-broker namespace. The Endpoint and Cluster CRDs in the cluster. A Service Account (SA) in the namespace for subsequent subctl access.  It also generates the broker-info.subm file which contains the following elements:\n The API endpoint. A CA certificate for the API endpoint. The Service Account token for accessing the API endpoint. A random IPsec PSK which will be stored only in this file. Service Discovery settings.  The cluster in which the Broker is deployed can also participate in the dataplane connectivity with other clusters, but it will need to be joined (see following step).\n Joining clusters For each cluster you want to join, issue the following command:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm --clusterid \u0026lt;ID\u0026gt; subctl will automatically discover as much as it can, and prompt the user for any missing necessary information. Note that each cluster must have a unique cluster ID; the cluster ID can be specified, or otherwise is going to be generated by default based on the cluster name in the kubeconfig file.\n"
},
{
	"uri": "/deployment/subctl/",
	"title": "subctl",
	"tags": [],
	"description": "",
	"content": "subctl is a command line utility designed to simplify the deployment and maintenance of Submariner across your clusters.\nsubctl helps to automate the deployment of the Submariner Operator, thereby reducing the possibility of mistakes during the process.\nsubctl connects to specified cluster(s) and performs the requested command.\nSynopsis subctl [command] [--flags] ...\nInstallation Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Installing specific versions By default, https://get.submariner.io will provide the latest release for subctl, and hence Submariner. Specific versions can be requested by using the VERSION environment variable.\nAvalailable options are:\n latest: the latest stable release (default) devel: the master branch code. rc: the latest release candidate. x.x.x (like 0.6.1, 0.5.0, etc)  For example\ncurl https://get.submariner.io | VERSION=devel bash Commands deploy-broker subctl deploy-broker [flags]\nThe deploy-broker command configures the cluster specified by the --kubeconfig flag (or KUBECONFIG env var) and the --kubecontext flag as the Broker. It installs the necessary CRDs and the submariner-k8s-broker namespace.\nIn addition, it generates a broker-info.subm file which can be used with the join command to connect clusters to the Broker. This file contains the following details:\n Encryption PSK key Broker access details for subsequent subctl runs Service Discovery settings  deploy-broker flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default \u0026ldquo;$HOME/.kube/config\u0026rdquo;)   --kubecontext \u0026lt;string\u0026gt; kubeconfig context to use   --service-discovery Enable Multi Cluster Service Discovery (default true)   --globalnet Enable support for overlapping Cluster/Service CIDRs in connecting clusters (default disabled)   --globalnet-cidr-range \u0026lt;string\u0026gt; Global CIDR supernet range for allocating GlobalCIDRs to each cluster (default \u0026ldquo;169.254.0.0/16\u0026rdquo;)   --ipsec-psk-from \u0026lt;string\u0026gt; Import IPsec PSK from existing Submariner broker file, like broker-info.subm (default \u0026ldquo;broker-info.subm\u0026rdquo;)    export export service subctl export service [flags] \u0026lt;name\u0026gt; creates a ServiceExport resource for the given Service name. This makes the corresponding Service discoverable from other clusters in the Submariner deployment.\nexport service flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default \u0026ldquo;$HOME/.kube/config\u0026rdquo;)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use   --namespace \u0026lt;string\u0026gt; Namespace in which the Service to be exported belongs    If no namespace flag is specified, it uses the default namespace from the current context, if present, otherwise it uses default.\njoin subctl join broker-info.subm [flags]\nThe join command deploys the Submariner Operator in a cluster using the settings provided in the broker-info.subm file. The service account credentials needed for the new cluster to access the Broker cluster will be created and provided to the Submariner Operator deployment.\njoin flags (general)    Flag Description     --cable-driver \u0026lt;string\u0026gt; Cable driver implementation (defaults to strongSwan IPsec)   --clusterid \u0026lt;string\u0026gt; Cluster ID used to identify the tunnels. Every cluster needs to have a unique cluster ID. If not provided, one will be generated by default based on the cluster name in the kubeconfig file   --clustercidr \u0026lt;string\u0026gt; Specifies the cluster\u0026rsquo;s CIDR used to generate Pod IP addresses. If not specified, subctl will try to discover it and if unable to do so, it will prompt the user   --no-label Skip gateway labeling. This disables the prompt for a Worker node to use as gateway   --enable-pod-debugging Enable Submariner pod debugging (verbose logging in the deployed pods)    join flags (Globalnet)    Flag Description     --globalnet-cluster-size \u0026lt;value\u0026gt; Cluster size for GlobalCIDR allocated to this cluster (amount of global IPs)   --globalnet-cidr \u0026lt;string\u0026gt; GlobalCIDR to be allocated to the cluster, this setting is exclusive with --globalnet-cluster-size and configures a specific Globalnet CIDR for this cluster    join flags (IPsec)    Flag Description     --disable-nat Disable NAT for IPsec   --ikeport \u0026lt;value\u0026gt; IPsec IKE port (default 500)   --ipsec-debug Enable IPsec debugging (verbose logging)   --nattport \u0026lt;value\u0026gt; IPsec NAT-T port (default 4500)    join flags (images and repositories)    Flag Description     --repository \u0026lt;string\u0026gt; The repository from where the various Submariner images will be sourced (default \u0026ldquo;quay.io/submariner\u0026rdquo;)   --version \u0026lt;string\u0026gt; Image version    show show networks subctl show networks [flags]\nInspects the cluster and reports information about the detected network plugin and detected Cluster and Service CIDRs.\nshow versions subctl show versions [flags]\nShows the version and image repository of each Submariner component in the cluster.\nshow gateways subctl show gateways [flags]\nShows summary information about the Submariner gateways in the cluster.\nshow connections subctl show connections [flags]\nShows information about the Submariner endpoint connections with other clusters.\nshow endpoints subctl show endpoints [flags]\nShows information about the Submariner endpoints in the cluster.\nshow all subctl show all [flags]\nShows the aggregated information from all the other show commands.\nshow flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default \u0026ldquo;$HOME/.kube/config\u0026rdquo;)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use    verify subctl verify \u0026lt;kubeConfig1\u0026gt; \u0026lt;kubeConfig2\u0026gt; [flags]\nThe verify command verifies a Submariner deployment between two clusters is functioning properly. The kubeConfig1 file will be ClusterA in the reports, while kubeConfig2 will be ClusterB in the reports. The --verbose flag is recommended to see what\u0026rsquo;s happening during the tests.\nThere are several suites of verifications that can be performed. By default all verifications are performed. Some verifications are deemed disruptive in that they change some state of the clusters as a side effect. If running the command interactively, you will be prompted for confirmation to perform disruptive verifications unless the --enable-disruptive flag is also specified. If running non-interactively (that is with no stdin), --enable-disruptive must be specified otherwise disruptive verifications are skipped.\nThe connectivity suite verifies dataplane connectivity across the clusters for the following cases:\n Pods (on Gateway nodes) to Services Pods (on non-Gateway nodes) to Services Pods (on Gateway nodes) to Pods Pods (on non-Gateway nodes) to Pods  The service-discovery suite verifies DNS discovery of \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local entries across the clusters.\nThe gateway-failover suite verifies the continuity of cross-cluster dataplane connectivity after a gateway failure in a cluster occurs. This suite requires a single gateway configured on ClusterA and other available Worker nodes capable of serving as gateways. Please note that this verification is disruptive.\nverify flags    Flag Description     --connection-attempts \u0026lt;value\u0026gt; The maximum number of connection attempts (default 2)   --connection-timeout \u0026lt;value\u0026gt; The timeout in seconds per connection attempt (default 60)   --operation-timeout \u0026lt;value\u0026gt; Operation timeout for Kubernetes API calls (default 240)   --report-dir \u0026lt;string\u0026gt; XML report directory (default \u0026ldquo;.\u0026quot;)   --verbose Produce verbose logs during connectivity verification   --only Comma separated list of specific verifications to perform   --enable-disruptive Enable verifications which are potentially disruptive to your deployment    benchmark benchmark throughput subctl benchmark throughput \u0026lt;kubeconfig1\u0026gt; [\u0026lt;kubeconfig2\u0026gt;] [flags]\nThe benchmark throughput command runs a throughput benchmark test between two specified clusters or within a single cluster. It deploys a Pod to run the iperf tool and logs the output to the console. When running benchmark throughput command, two type of tests will be issued:\n Pod to Pod - where both Pods are scheduled on Gateway nodes Pod to Pod - where both Pods are scheduled on non-Gateway nodes  benchmark latency subctl benchmark latency \u0026lt;kubeconfig1\u0026gt; [\u0026lt;kubeconfig2\u0026gt;] [flags]\nThe benchmark latency command runs a latency benchmark test between two specified clusters or within a single cluster. It deploys a Pod to run the netperf tool and logs the output to the console. When running benchmark latency command, two type of tests will be issued:\n Pod to Pod - where both Pods are scheduled on Gateway nodes Pod to Pod - where both Pods are scheduled on non-Gateway nodes  benchmark flags    Flag Description     --intra-cluster Performs the benchmark test within a single cluster between Pods from a Non-Gateway node to a Gateway node    version subctl version\nPrints the version details for the subctl binary.\n"
},
{
	"uri": "/quickstart/openshift/service_discovery/",
	"title": "With Service Discovery",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for Submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default OpenShift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for Submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery To install Submariner with multi-cluster service discovery follow the steps below.\nUse cluster-a as Broker with service discovery enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --service-discovery Join cluster-a and cluster-b to the Broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-a.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification This will perform automated verifications between the clusters.\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "The diagram below illustrates the basic architecture of Submariner:\nSubmariner consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes clusters, both on-premises and on public clouds.\n Gateway Engine: manages the secure tunnels to other clusters. Route Agent: routes cross-cluster traffic from nodes to the active Gateway Engine. Broker: facilitates the exchange of metadata between Gateway Engines enabling them to discover one another.  Submariner has optional components that provide additional functionality.\n Globalnet Controller: handles overlapping CIDRs across clusters. Service Discovery: provides DNS discovery of services across clusters.  "
},
{
	"uri": "/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner provides a number of Prometheus metrics, and sets up ServiceMonitor instances which allow these metrics to be scraped by an in-cluster Prometheus instance.\nThe following metrics are exposed currently:\n  submariner_gateways: the number of gateways in the cluster;\n  submariner_connections: the number of connections to other clusters, with the following labels:\n local_cluster: the local cluster name local_hostname: the local hostname remote_cluster: the remote cluster name remote_hostname: the remote hostname status: the connection status (“connecting”, “connected”, or “error”)    OpenShift setup OpenShift 4.5 or later can automatically discover the Submariner metrics. This currently requires enabling user workload monitoring; see the OpenShift documentation for details.\nPrometheus Operator To start monitoring Submariner using the Prometheus Operator, Prometheus needs to be configured to scrape the Submariner Operator’s namespace (submariner-operator by default). The specifics depend on your Prometheus deployment, but typically, this will require you to:\n  add the Submariner Operator’s namespace to Prometheus’ ClusterRoleBinding;\n  ensure that Prometheus’ configuration doesn’t prevent it from scraping this namespace.\n  A minimal Prometheus object providing access to the Submariner metrics is as follows:\napiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: name: prometheus labels: prometheus: prometheus spec: replicas: 1 serviceAccountName: prometheus serviceMonitorNamespaceSelector: {} serviceMonitorSelector: matchLabels: name: submariner-operator "
},
{
	"uri": "/contributing/shipyard/advanced/",
	"title": "Advanced Features",
	"tags": [],
	"description": "",
	"content": "Shipyard has many advanced features to use in consuming projects.\nTo utilize an advanced feature in a project consuming Shipyard, a good practice is to change the project\u0026rsquo;s Makefile to have the advanced logic that is always needed. Any variable functionality can then be passed as desired in the command line.\nImage Building Helper Script Shipyard ships an image building script build_image.sh which can be used to build the image(s) that you require. The script has built in caching capabilities to speed up local and pull request CI builds, by utilizing docker\u0026rsquo;s ability to reuse layers from a cached image.\nThe script accepts several flags:\n tag: The tag to set for the local image (defaults to dev). repo: The repo portion to use for the image name. image (-i): The image name itself. dockerfile (-f): The Dockerfile to build the image from. [no]cache: Turns the caching on (or off).  For example, to build the submariner image use:\n${SCRIPTS_DIR}/build_image.sh -i submariner -f package/Dockerfile Deployment Scripts Features Per Cluster Settings Shipyard supports specifying different settings for each deployed cluster. A default cluster_settings is supplied in the image, so any custom settings override those. The settings are sent to supporting scripts using a --cluster_settings flag.\nCurrently, the following settings are supported:\n  clusters: An array of the clusters to deploy.\nclusters=(cluster1,cluster2)   cluster_nodes: A map of cluster names to a space separated string, representing a list of nodes to deploy. Supported values are control-plane and worker.\ncluster_nodes[cluster1]=\u0026#34;control-plane worker\u0026#34; cluster_nodes[cluster2]=\u0026#34;control-plane worker worker\u0026#34;   cluster_subm: A map of cluster names to values specifying if Submariner should be installed. Set to true to have Submariner installed, or to false to skip the installation.\ncluster_subm[cluster1]=\u0026#34;false\u0026#34; cluster_subm[cluster2]=\u0026#34;true\u0026#34;   Example: Custom Per Cluster Settings As an example, in order to customize the clusters to have two workers, and no submariner on the 1st cluster, create a cluster_settings file in the project:\ncluster_nodes[\u0026#39;cluster1\u0026#39;]=\u0026#34;control-plane\u0026#34; cluster_nodes[\u0026#39;cluster2\u0026#39;]=\u0026#34;control-plane worker worker\u0026#34; cluster_nodes[\u0026#39;cluster3\u0026#39;]=\u0026#34;control-plane worker worker\u0026#34; cluster_subm[\u0026#39;cluster1\u0026#39;]=\u0026#34;false\u0026#34; Then, to apply these settings, add this snippet to the Makefile:\nCLUSTER_SETTINGS_FLAG = --cluster_settings $(DAPPER_SOURCE)/path/to/cluster_settings CLUSTERS_ARGS += $(CLUSTER_SETTINGS_FLAG) DEPLOY_ARGS += $(CLUSTER_SETTINGS_FLAG) The path to cluster_settings should be specified relative to the project root; this ends up available in the build container in the directory referenced by $DAPPER_SOURCE.\nClusters Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make clusters via a make variable CLUSTERS_ARGS (or an environment variable with the same name). These flags affect how the clusters are deployed (and possibly influence how Submariner works).\nFlags of note:\n  k8s_version: Allows to specify the Kubernetes version that kind will deploy. Available versions can be found here.\nmake clusters CLUSTERS_ARGS=\u0026#39;--k8s_version 1.18.0\u0026#39;   globalnet: When set, deploys the clusters with overlapping Pod \u0026amp; Service CIDRs to simulate this scenario.\nmake clusters CLUSTERS_ARGS=\u0026#39;--globalnet\u0026#39;   Submariner Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make deploy via a make variable DEPLOY_ARGS (or an environment variable with the same name). These flags affect how Submariner is deployed on the clusters.\nSince deploy relies on clusters then effectively you could also specify CLUSTERS_ARGS to control the cluster deployment (provided the cluster hasn\u0026rsquo;t been deployed yet).\nFlags of note:\n  deploytool: Specifies the deployment tool to use: operator (default) or helm.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator\u0026#39;   deploytool_broker_args: Any extra arguments to pass to the deploy tool when deploying the broker.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_broker_args \u0026#34;--service-discovery\u0026#34;\u0026#39;   deploytool_submariner_args: Any extra arguments to pass to the deploy tool when deploying Submariner.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_submariner_args \u0026#34;--cable-driver wireguard\u0026#34;\u0026#39;   globalnet: When set, deploys Submariner with the globalnet controller, and assigns a unique Global CIDR to each cluster.\nmake deploy DEPLOY_ARGS=\u0026#39;--globalnet\u0026#39;   Example: Passing Deployment Variables As an example, in order to deploy with Lighthouse and support both Operator and Helm deployments, one can add this snippet to the Makefile:\nifeq ($(deploytool),operator) DEPLOY_ARGS += --deploytool operator --deploytool_broker_args \u0026#39;--service-discovery\u0026#39; else DEPLOY_ARGS += --deploytool helm --deploytool_broker_args \u0026#39;--set submariner.serviceDiscovery=true\u0026#39; --deploytool_submariner_args \u0026#39;--set submariner.serviceDiscovery=true,lighthouse.image.repository=localhost:5000/lighthouse-agent,serviceAccounts.lighthouse.create=true\u0026#39; endif In such a case, the call to deploy the environment would look like this:\nmake deploy [deploytool=operator] Note that deploytool is a variable used to determine the tool to use, but isn\u0026rsquo;t passed to or used by Shipyard.\n"
},
{
	"uri": "/quickstart/openshift/",
	"title": "OpenShift (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for Submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default OpenShift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for Submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Use cluster-a as Broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the Broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster pods and services\nsubctl verify --only connectivity cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/quickstart/openshift/globalnet/",
	"title": "With Service Discovery and Globalnet",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create cluster A This step will create a cluster named \u0026ldquo;cluster-a\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a This may take some time to complete so you can move on to the next section in parallel if you wish.\nCreate cluster B This step will create a cluster named \u0026ldquo;cluster-b\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-b And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for Submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default OpenShift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for Submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery and Globalnet To install Submariner with multi-cluster service discovery and support for overlapping CIDRs follow the steps below.\nUse cluster-a as Broker with service discovery and globalnet enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --service-discovery --globalnet Join cluster-a and cluster-b to the Broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid west subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid east  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-a.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification This will perform automated verifications between the clusters.\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/contributing/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": "Submariner Community Code of Conduct Submariner follows the CNCF Code of Conduct.\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting one or more of the Submariner Committers or Owners.\n"
},
{
	"uri": "/deployment/helm/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": "Deploying with Helm Installing Helm export KUBECONFIG=\u0026lt;kubeconfig-of-broker\u0026gt; helm repo add submariner-latest https://submariner-io.github.io/submariner-charts/charts kubectl -n kube-system create serviceaccount tiller kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller helm init --service-account tiller kubectl -n kube-system rollout status deploy/tiller-deploy Exporting environment variables needed later export BROKER_NS=submariner-k8s-broker export SUBMARINER_PSK=$(LC_CTYPE=C tr -dc \u0026#39;a-zA-Z0-9\u0026#39; \u0026lt; /dev/urandom | fold -w 64 | head -n 1) export SUBMARINER_NS=submariner Deploying the Broker helm install submariner-latest/submariner-k8s-broker \\  --name \u0026#34;${BROKER_NS}\u0026#34; \\  --namespace \u0026#34;${BROKER_NS}\u0026#34; \\  --set submariner.serviceDiscovery=true Setup more environment variables we will need later for joining clusters.\nexport SUBMARINER_BROKER_URL=$(kubectl -n default get endpoints kubernetes \\  -o jsonpath=\u0026#34;{.subsets[0].addresses[0].ip}:{.subsets[0].ports[?(@.name==\u0026#39;https\u0026#39;)].port}\u0026#34;) export SUBMARINER_BROKER_CA=$(kubectl -n \u0026#34;${BROKER_NS}\u0026#34; get secrets \\  -o jsonpath=\u0026#34;{.items[?(@.metadata.annotations[\u0026#39;kubernetes\\.io/service-account\\.name\u0026#39;]==\u0026#39;${BROKER_NS}-client\u0026#39;)].data[\u0026#39;ca\\.crt\u0026#39;]}\u0026#34;) export SUBMARINER_BROKER_TOKEN=$(kubectl -n \u0026#34;${BROKER_NS}\u0026#34; get secrets \\  -o jsonpath=\u0026#34;{.items[?(@.metadata.annotations[\u0026#39;kubernetes\\.io/service-account\\.name\u0026#39;]==\u0026#39;${BROKER_NS}-client\u0026#39;)].data.token}\u0026#34; \\  | base64 --decode) Joining a cluster This step needs to be repeated for every cluster you want to connect with Submariner.\nexport KUBECONFIG=kubeconfig-of-the-cluster-to-join export CLUSTER_ID=the-id-of-the-cluster export CLUSTER_CIDR=x.x.x.x/x # the cluster\u0026#39;s Pod IP CIDR export SERVICE_CIDR=x.x.x.x/x # the cluster\u0026#39;s Service IP CIDR If your clusters have overlapping IPs (Cluster/Service CIDRs), please set:\nexport GLOBALNET=true export GLOBAL_CIDR=169.254.x.x/x # using an individual non-overlapping # range for each cluster you join. Joining the cluster:\nhelm install submariner-latest/submariner \\  --name submariner \\  --namespace \u0026#34;${SUBMARINER_NS}\u0026#34; \\  --set ipsec.psk=\u0026#34;${SUBMARINER_PSK}\u0026#34; \\  --set broker.server=\u0026#34;${SUBMARINER_BROKER_URL}\u0026#34; \\  --set broker.token=\u0026#34;${SUBMARINER_BROKER_TOKEN}\u0026#34; \\  --set broker.namespace=\u0026#34;${BROKER_NS}\u0026#34; \\  --set broker.ca=\u0026#34;${SUBMARINER_BROKER_CA}\u0026#34; \\  --set submariner.clusterId=\u0026#34;${CLUSTER_ID}\u0026#34; \\  --set submariner.clusterCidr=\u0026#34;${CLUSTER_CIDR}\u0026#34; \\  --set submariner.serviceCidr=\u0026#34;${SERVICE_CIDR}\u0026#34; \\  --set submariner.globalCidr=\u0026#34;${GLOBAL_CIDR}\u0026#34; \\  --set serviceAccounts.globalnet.create=\u0026#34;${GLOBALNET}\u0026#34; \\  --set submariner.natEnabled=\u0026#34;true\u0026#34; \\  # disable this if no NAT will happen between gateways --set crd.create=true \\  --set submariner.serviceDiscovery=true \\  --set serviceAccounts.lighthouse.create=true Some image override settings you could use\n--set globalnet.image.repository=\u0026#34;localhost:5000/submariner-globalnet\u0026#34; \\  --set globalnet.image.tag=\u0026#34;local\u0026#34; \\  --set globalnet.image.pullPolicy=\u0026#34;IfNotPresent\u0026#34; \\  --set engine.image.repository=\u0026#34;localhost:5000/submariner\u0026#34; \\  --set engine.image.tag=\u0026#34;local\u0026#34; \\  --set engine.image.pullPolicy=\u0026#34;IfNotPresent\u0026#34; \\  --set routeAgent.image.repository=\u0026#34;localhost:5000/submariner-route-agent\u0026#34; \\  --set routeAgent.image.tag=\u0026#34;local\u0026#34; \\  --set routeAgent.image.pullPolicy=\u0026#34;IfNotPresent\u0026#34; \\  --set lighthouse.image.repository=localhost:5000/lighthouse-agent If installing on OpenShift, please also add the Submariner service accounts (SAs) to the privileged Security Context Constraint.\noc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-routeagent oc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-engine Perform automated verification Automated verification of the deployment can be performed by using the verification tests embedded in the subctl command line tool via the subctl verify command.\nInstall subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Run the verification subctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/troubleshooting/",
	"title": "Troubleshooting Guide",
	"tags": [],
	"description": "",
	"content": "Overview You have followed steps in Deployment but something has gone wrong. You\u0026rsquo;re not sure what and how to fix it, or what information to collect to raise an issue. Welcome to the Submariner troubleshooting guide where we will help you get your deployment working again.\nBasic familiarity with the Submariner components and architecture will be helpful when troubleshooting so please review the Architecture section.\nThe guide has been broken into different sections for easy navigation.\nPre-requisite Before we begin troubleshooting, run subctl version to obtain which version of the Submariner components you are running.\nRun kubectl get services -n \u0026lt;service-namespace\u0026gt; | grep \u0026lt;service-name\u0026gt; to get information about the service you\u0026rsquo;re trying to access. This will provide you with the Service Name, Namespace and ServiceIP. If Globalnet is enabled, you will also need the globalIp of the service by running\nkubectl get service \u0026lt;service-name\u0026gt; -o jsonpath='{.metadata.annotations.submariner\\.io/globalIp}'\nService Discovery Issues If you are able to connect to remote service by using ServiceIP or globalIp, but not by service name, it is a Service Discovery Issue.\nService Discovery not working This is good time to familiarize yourself with Service Discovery Architecture if you haven\u0026rsquo;t already.\nCheck ServiceExport for your Service For a Service to be accessible across clusters, you must first export the Service via subctl which creates a ServiceExport resource. Ensure the ServiceExport resource exists and check if its status condition indicates `Exported\u0026rsquo;. Otherwise, its status condition will indicate the reason it wasn\u0026rsquo;t exported.\nkubectl get serviceexport -n \u0026lt;service-namespace\u0026gt; \u0026lt;service-name\u0026gt;\nSample output:\napiVersion: lighthouse.submariner.io/v2alpha1 kind: ServiceExport metadata: name: nginx Status: Conditions: Message: Service was successfully synced to the broker Status: True Type: Exported Check Lighthouse CoreDNS Service All cross-cluster service queries are handled by Lighthouse CoreDNS server. First we check if the Lighthouse CoreDNS Service is running properly.\nkubectl -n submariner-operator get service submariner-lighthouse-coredns\nIf it is running fine, note down the ServiceIP for the next steps. If not, check the logs for an error.\nIf the error is due to a wrong image, run kubectl -n submariner-operator get deployment submariner-lighthouse-coredns and make sure Image is set to quay.io/submariner/lighthouse-coredns:\u0026lt;version\u0026gt; and refers to the correct version.\nFor any other errors, capture the information and raise a new issue\nIf there\u0026rsquo;s no error, then check if the Lighthouse CoreDNS server is configured correctly. Run kubectl -n submariner-operator describe configmap submariner-lighthouse-coredns and make sure it has following configuration:\nclusterset.local:53 { lighthouse errors health ready } Check CoreDNS Configuration Submariner requires the CoreDNS deployment to forward requests for the domain clusterset.local to the Lighthouse CoreDNS server in the cluster making the query. Ensure this configuration exists and is correct.\nFirst we check if CoreDNS is configured to forward requests for domain clusterset.local to Lighthouse CoreDNS Server in the cluster making the query.\nkubectl -n kube-system describe configmap coredns\nIn the output look for something like this:\nclusterset.local:53 { forward . \u0026lt;lighthouse-coredns-serviceip\u0026gt; ======\u0026gt; ServiceIP of lighthouse-coredns service as noted in previous section } If the entries highlighted above are missing or ServiceIp is incorrect, it means CoreDNS wasn\u0026rsquo;t configured correctly. It can be fixed by running kubectl edit configmap coredns and making the changes manually. You may need to repeat this step on every cluster.\nCheck submariner-lighthouse-agent Next we check if the submariner-lighthouse-agent is properly running. Run kubectl -n submariner-operator get pods submariner-lighthouse-agent and check the status of Pods.\nIf the status indicates the ImagePullBackOff error, run kubectl -n submariner-operator describe deployment submariner-lighthouse-agent and check if Image is set correctly to quay.io/submariner/lighthouse-agent:\u0026lt;version\u0026gt;. If it is and the same error still occurs, raise an issue here or ping us on the community slack channel.\nIf the status indicates any other error, run kubectl -n submariner-operator get pods to get the name of the lighthouse-agent Pod. Then run kubectl -n submariner-operator logs \u0026lt;lighthouse-agent-pod-name\u0026gt; to get the logs. See if there are any errors in the log. If yes, raise an issue with the log contents, or you can continue reading through this guide to troubleshoot further.\nIf there are no errors, grep the log for the service name that you\u0026rsquo;re trying to query as we may need the log entries later for raising an issue.\nCheck ServiceImport resources If the steps above did not indicate an issue, next we check if the ServiceImport resources were properly created for the service you\u0026rsquo;re trying to access. The format of a ServiceImport resources\u0026rsquo;s name is as follows:\n\u0026lt;service-name\u0026gt;-\u0026lt;service-namespace\u0026gt;-\u0026lt;cluster-id\u0026gt;\nRun kubectl get serviceimports --all-namespaces |grep \u0026lt;your-service-name\u0026gt; on the Broker cluster to check if a resource was created for your service. If not, then check the Lighthouse Agent logs on the cluster where service was created and look for any error or warning messages indicating a failure to create the ServiceImport resource for your service. The most common error is Forbidden if the RBAC wasn\u0026rsquo;t configured correctly. Depending on the deployment method used, \u0026lsquo;subctl\u0026rsquo; or \u0026lsquo;helm\u0026rsquo;, it should\u0026rsquo;ve been done for you. Create an issue with relevant log entries.\nIf the ServiceImport resource was created correctly on the Broker cluster, the next step is to check if it exists on the cluster where you\u0026rsquo;re trying to access the service. Follow the same steps as earlier to get the list of the ServiceImport resources and check if the ServiceImport for your service exists. If not, check the logs of the Lighthouse Agent on the cluster where you are trying to access the service. As described earlier, it will most commonly be an issue with RBAC otherwise create an issue with relevant log entries.\nIf the ServiceImport resource was created properly on the cluster, run kubectl -n submariner-operator describe serviceimport \u0026lt;your-serviceimport-name\u0026gt; and check if it has the correct ClusterID and ServiceIP:\nName: nginx-default-cluster2 Namespace: submariner-operator Labels: \u0026lt;none\u0026gt; Annotations: origin-name: nginx origin-namespace: default API Version: lighthouse.submariner.io/v2alpha1 Kind: ServiceImport Metadata: Creation Timestamp: 2020-07-14T17:27:32Z Generation: 1 Resource Version: 2790 Self Link: /apis/lighthouse.submariner.io/v2alpha1/namespaces/submariner-operator/serviceimports/nginx-default-cluster2 UID: 4cbe1c2b-c5f7-11ea-9bbe-0242ac110009 Spec: Ports: \u0026lt;nil\u0026gt; Session Affinity: Session Affinity Config: \u0026lt;nil\u0026gt; Type: ClusterSetIP Status: Clusters: Cluster: cluster2 ==========\u0026gt; ClusterID of cluster where service is running Ips: 100.92.43.63 ==========\u0026gt; ServiceIP or GlobalIP of service you\u0026#39;re trying to access Events: \u0026lt;none\u0026gt; For headless Service, you need to check EndpointSlice resource.\nIf the data is not correct, you can manually edit the ServiceImport resource to set the correct IP as a workaround and create an issue with relevant information.\nIf the ServiceImport Ips are correct but still not being returned from DNS queries, check the connectivity to the cluster using subctl show endpoint. The Lighthouse CoreDNS Server only returns IPs from connected clusters.\nCheck EndpointSlice resources For a headless Service, next we check if the EndpointSlice resources were properly created for the service you\u0026rsquo;re trying to access. EndpointSlice resources are created in the same namespace as the source Service. The format of a EndpointSlice resource\u0026rsquo;s name is as follows:\n\u0026lt;service-name\u0026gt;--\u0026lt;cluster-id\u0026gt;\nRun kubectl get endpointslices --all-namespaces |grep \u0026lt;your-service-name\u0026gt; on the Broker cluster to check if a resource was created for your Service. If not, then check the Lighthouse Agent logs on the cluster where the Service was created and look for any error or warning messages indicating a failure to create the ServiceImport resource for your Service. The most common error is Forbidden if the RBAC wasn\u0026rsquo;t configured correctly. This is supposed to be done automatically during deployment so please file an issue with the relevant log entries.\nIf the EndpointSlice resource was created correctly on the Broker cluster, the next step is to check if it exists on the cluster where you\u0026rsquo;re trying to access the Service. Follow the same steps as earlier to get the list of the EndpointSlice resources and check if the EndpointSlice for the Service exists. If not, check the logs of the Lighthouse Agent on the cluster where you are trying to access the Service. As described earlier, it will most commonly be an issue with RBAC so create an issue with relevant log entries.\nIf the EndpointSlice resource was created properly on the cluster, run kubectl -n \u0026lt;your-service-namespace\u0026gt; describe endpointslice \u0026lt;your-endpointslice-name\u0026gt; and check if it has the correct endpoint addresses:\nName: nginx-ss-cluster2 Namespace: default Labels: endpointslice.kubernetes.io/managed-by=lighthouse-agent.submariner.io lighthouse.submariner.io/sourceCluster=cluster2 lighthouse.submariner.io/sourceName=nginx-ss lighthouse.submariner.io/sourceNamespace=default multicluster.kubernetes.io/service-name=nginx-ss-default-cluster2 Annotations: \u0026lt;none\u0026gt; AddressType: IPv4 Ports: Name Port Protocol ---- ---- -------- web 80 TCP Endpoints: - Addresses: 10.242.0.5 -----\u0026gt; Pod IP Conditions: Ready: true Hostname: web-0 -----\u0026gt; Pod hostname Topology: kubernetes.io/hostname=cluster2-worker2 - Addresses: 10.242.224.4 Conditions: Ready: true Hostname: web-1 Topology: kubernetes.io/hostname=cluster2-worker Events: \u0026lt;none\u0026gt; If the Addresses are correct but still not being returned from DNS queries, try querying IPs in a specific cluster by prefixing the query with \u0026lt;cluster-id\u0026gt;. If that returns the IPs correctly, then check the connectivity to the cluster using subctl show endpoint. The Lighthouse CoreDNS Server only returns IPs from connected clusters.\nFor errors querying specific Pods of a StatefulSet, check that the Hostname is correct for the endpoint.\nIf still not working, file an issue with relevant log entries.\n"
},
{
	"uri": "/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.6.0 Improved Submariner High Availability and various Lighthouse enhancements  This release mainly focused on support for headless Services in Lighthouse, as well as improving Submariner\u0026rsquo;s High Availability (HA).\n The DNS domains have been updated from \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.supercluster.local to \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local to align with the change in Kubernetes Multicluster Service API. Both domains will be supported for 0.6.0 but 0.7.0 will remove support for supercluster.local. Please update your deployments and applications.\n  Lighthouse has been enhanced to:  Be aware of the local cluster Gateway connectivity so as not to announce the IP addresses for disconnected remote clusters. Support headless Services for non-Globalnet deployments. Support for Globalnet will be available in a future release. Be aware of a Service\u0026rsquo;s backend Pods so as not to announce IP addresses for Services that have no active Pods. Use Round Robin IP resolution for Services available in multiple clusters. Enable service discovery by default for subctl deployments.   subctl auto-detects the cluster ID from the kubeconfig file\u0026rsquo;s information when possible. Submariner\u0026rsquo;s Pods now shut down gracefully and do proper cleanup which reduces downtime during Gateway failover. The Operator now automatically exports Prometheus metrics; these integrate seamlessly with OpenShift Prometheus if user workload monitoring is enabled, and can be included in any other Prometheus setup. Minimum Kubernetes version is now 1.17. HostNetwork to remote Service connectivity fixes for AWS clusters. The project\u0026rsquo;s codebase quality and readability has been improved using various linters.  v0.5.0 Lighthouse service discovery alignment  This release mainly focused on continuing the alignment of Lighthouse\u0026rsquo;s service discovery support with the Kubernetes Multicluster Services KEP.\n  Lighthouse has been modified per the Kubernetes Multicluster Services KEP as follows:  The MultiClusterService resource has been replaced by ServiceImport. The ServiceExport resource is now updated with status information as lifecycle events occur.   Lighthouse now allows a ServiceExport resource to be created prior to the associated Service. Network discovery was moved from subctl to the Submariner Operator. Several new commands were added to subctl: export service, show versions, show connections, show networks, show endpoints, and show gateways. The subctl info command has been removed in lieu of the new show networks command. The Globalnet configuration has been moved from the broker-info.subm file to a ConfigMap resource stored on the Broker cluster. Therefore, the new subctl cannot be used on brownfield Globalnet deployments where this information was stored as part of broker-info.subm. subctl now supports joining multiple clusters in parallel without having to explicitly specify the globalnet-cidr for the cluster to work around this issue. The globalnet-cidr will automatically be allocated by subctl for each cluster. The separate --operator-image parameter has been removed from subctl join and the --repository and --version parameters are now used for all images. The Submariner Operator status now includes Gateway information. Closed technical requirements for Submariner to become a CNCF project, including Developer Certificate of Origin compliance and additional source code linting.  v0.4.0 Libreswan cable driver, Kubernetes multicluster service discovery  This release is mainly focused on Submariner\u0026rsquo;s Libreswan cable driver implementation, as well as standardizing Lighthouse\u0026rsquo;s service discovery support with the Kubernetes Multicluster Services KEP.\n  Libreswan IPsec cable driver is available for testing and is covered in Submariner\u0026rsquo;s CI. Lighthouse has been modified per the Kubernetes Multicluster Services KEP as follows:  A ServiceExport object needs to be created alongside any Service that is intended to be exported to participant clusters. Supercluster services can be accessed with \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local.   Globalnet overlapping CIDR support improvements and bug fixes. Multiple CI improvements implemented from Shipyard. CI tests are now run via GitHub Actions. Submariner\u0026rsquo;s Operator now completely handles the Lighthouse deployment via the ServiceDiscovery CRD. subctl verify is now available for connectivity, service-discovery and gateway-failover.  v0.3.0 Lighthouse Service Discovery without KubeFed  This release is focused on removing the KubeFed dependency from Lighthouse, improving the user experience, and adding experimental WireGuard support as an alternative to IPsec.\n  Lighthouse no longer depends on KubeFed. All metadata exchange is handled over the Broker as MultiClusterService CRs. Experimental WireGuard support has been added as a pluggable CableDriver option in addition to the current default IPsec. Submariner reports the active and passive gateways as a gateway.submariner.io resource. The Submariner Operator reports a detailed status of the deployment. The gateway redundancy/failover tests are now enabled and stable in CI. Globalnet hostNetwork to remote globalIP is now supported. Previously, when a Pod used hostNetworking it was unable to connect to a remote Service via globalIP. A GlobalCIDR can be manually specified when joining a cluster with Globalnet enabled. This enables CI speed optimizations via better parallelism. Operator and subctl are more robust via standard retries on updates. subctl creates a new individual access token for every new joined cluster.  v0.2.0 Overlapping CIDR support  This release is focused on overlapping CIDR support between clusters.\n  Support for overlapping CIDRs between clusters (Globalnet). Enhanced end-to-end scripts, which will be shared between repositories in the Shipyard project (ongoing work). Improved end-to-end deployment by using a local registry. Refactoring to support pluggable drivers (in preparation for WireGuard support).  v0.1.1 Submariner with more light  This release is focused on stability for Lighthouse.\n  Cleaner logging for submariner-engine. Cleaner logging for submariner-route-agent. Fixed issue with wrong token stored in .subm file (submariner-operator#244). Added flag to disable the OpenShift CVO (submariner-operator#235). Fixed several service discovery bugs (submariner-operator#194, submariner-operator#167). Fixed several panics on nil network discovery. Added checks to ensure the CIDRs for joining cluster don\u0026rsquo;t overlap with existing ones. Fix context handling related to service discovery/KubeFed (submariner-operator#180). Use the correct CoreDNS image for OpenShift.  v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making Lighthouse available as a developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (submariner#346, submariner#348, submariner#332). Migrated to DaemonSets for Submariner gateway deployment. Added support for hostNetwork to remote Pod/Service connectivity (submariner#298). Auto detection and configuration of MTU for vx-submariner, jumbo frames support (submariner#301). Support for updated strongSwan (submariner#288). Better iptables detection for some hosts (submariner#227).   subctl and the Submariner Operator have the following improvements:\n  Support for verify-connectivity checks between two connected clusters. Deployment of Submariner gateways based on DaemonSet instead of Deployment. Rename submariner Pods to submariner-gateway Pods for clarity. Print version details on crash (subctl). Stop storing IPsec key on Broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file. Version command for subctl. Nicer spinners during deployment (thanks to kind).  v0.0.3 \u0026ndash; KubeCon NA 2019 Submariner has been greatly enhanced to allow administrators to deploy into Kubernetes clusters without the necessity for Layer 2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). subctl was created to make deployment of Submariner easier.\nv0.0.2 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/contributing/code-review/",
	"title": "Code Review Guide",
	"tags": [],
	"description": "",
	"content": "Code Review Guide This guide is meant to facilitate Submariner code review by sharing norms, best practices, and useful patterns.\nSubmariner follows the Kubernetes Code Review Guide wherever relevant. This guide collects the most important highlights of the Kubernetes process and adds Submariner-specific extensions.\nTwo non-author Committer approvals required Pull Requests to Submariner require two approvals from a Committer to the relevant part of the code base, as defined by the CODEOWNERS file at the root of each repository and the Community Membership/Committers process.\nNo merge commits Kubernetes recommends avoiding merge commits.\nWith our current GitHub setup, pull requests are liable to include merge commits temporarily. Whenever a PR is updated through the UI, GitHub merges the target branch into the PR. However, since we merge PRs by either squashing or rebasing them, those merge commits disappear from the series of commits which ultimately ends up in the target branch.\nSquash/amend commits into discrete steps Kubernetes recommends squashing commits using these guidelines.\nAfter a review, prepare your PR for merging by squashing your commits.\nAll commits left on your branch after a review should represent meaningful milestones or units of work. Use commits to add clarity to the development and review process. Keep in mind that smaller commits are easier to review.\nBefore merging a PR, squash the following kinds of commits:\n Fixes/review feedback Typos Merges and rebases Work in progress Aim to have every commit in a PR compile and pass tests independently if you can, but it\u0026rsquo;s not a requirement.  Address code review feedback with new commits When addressing review comments, as a general rule, push a new commit instead of amending to the prior commit as the former makes it easy for reviewers to determine what changed.\nTo avoid cluttering the git log, squash the review commits into the appropriate commit before merging. The committer can do this in GitHub via the \u0026ldquo;Squash and merge\u0026rdquo; option. However you may want to preserve other commits, in which case squashing will need to be done manually via the Git CLI. To make that simpler, you can commit the review-prompted changes with git commit --fixup with the appropriate commit hash. This will keep them as separate commits, and if you later rebase with the --autosquash option (that is git rebase --autosquash -i) they will automatically be selected for squashing.\nCommit message formatting Kubernetes recommends these commit message practices.\nIn summary:\n Separate subject from body with a blank line Limit the subject line to 50 characters Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs how  GitLint will automatically be run against all commits to try to validate these conventions.\nDismiss reviews after substantial changes If a PR is substantially changed after a code review, the author should dismiss the stale reviews.\nWith the current GitHub configuration, reviews are not automatically dismissed when PRs are updated. This is to cause less drag for the typical cases, like minor merge conflicts. As Submariner grows, it might make sense to trade this low-drag solution for one where only exactly the reviewed code can be merged.\nAddress all -1s before merging If someone requests changes (\u0026ldquo;votes -1\u0026rdquo;) for a PR, a best-effort should be made to address those concerns and achieve a neutral position or approval (0/+1 vote) before the PR is merged.\n"
},
{
	"uri": "/quickstart/vsphere_aws/",
	"title": "OpenShift with Service Discovery (vSphere/AWS)",
	"tags": [],
	"description": "",
	"content": "In this quickstart guide, we shall cover the necessary steps to deploy OpenShift Container Platform (OCP) on vSphere and AWS. Once the OCP clusters are deployed, we shall cover how to deploy Submariner and connect the two clusters.\nOpenShift Prerequisites Before we proceed, the following prerequisites have to be downloaded and added to your $PATH:\n openshift-installer pull secret oc tools aws cli  Please ensure that the tools you downloaded above are compatible with the OpenShift Container Platform version.\n Deploy Cluster on vSphere (OnPrem) Create the necessary infrastructure on vSphere and ensure that your machines have direct internet access before starting the installation. To deploy OCP 4.4, follow the instructions shown here\nAssuming that you deployed the cluster (say, cluster-a) with default network configuration, the Pod and Service CIDRs would be\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    Submariner creates a VXLAN overlay network in the local cluster and uses port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes to preserve the source IP of the inter-cluster traffic. Ensure that firewall configuration on vSphere cluster allows 4800/UDP across all the worker nodes.\n   Protocol Port Description     UDP 4800 overlay network for inter-cluster traffic    Although we are using the default OCP network configuration on vSphere, you can install vSphere with a custom network configuration as shown here\n Deploy Cluster on AWS Configure AWS CLI with appropriate values $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text For more details, follow this link:\n https://docs.openshift.com/container-platform/4.4/installing/installing_aws/installing-aws-account.html\n In this step we shall modify the default Cluster/Service CIDRs and deploy cluster-b on AWS.\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the Pod IP network. Please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the Service IP network. This is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b Make your AWS cluster ready for Submariner Submariner gateway nodes need to be able to accept IPsec traffic. The default ports are 4500/UDP and 500/UDP. However, when you have some on-premises clusters (like vSphere in this example) which are typically inside a corporate network, the firewall configuration on the corporate router may not allow the default IPsec traffic. We can overcome this limitation by using non-standard ports like 4501/UDP and 501/UDP.\nAdditionally, the default OpenShift deployments do not allow assigning an elastic public IP to existing worker nodes, something that\u0026rsquo;s necessary at least on one end of the IPsec connections.\nTo handle these requirements on AWS, we provide a script that will prepare your AWS OpenShift deployment for Submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Currently, prep_for_subm.sh script does not support specifying custom IPsec ports. Until the issue is resolved, execute the following commands to open the necessary ports:\nexport BROKER_IKEPORT=501 export NAT_PORT=4501 sed \u0026#34;s/\\ 500/\\ $BROKER_IKEPORT/g\u0026#34; -i cluster-b/ocp-ipi-aws/ocp-ipi-aws-prep/ec2-resources.tf sed \u0026#34;s/\\ 4500/\\ $NAT_PORT/g\u0026#34; -i cluster-b/ocp-ipi-aws/ocp-ipi-aws-prep/ec2-resources.tf ./prep_for_subm.sh cluster-b # respond yes when terraform asks Submariner Installation Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery To install Submariner with multi-cluster service discovery, follow the steps below.\nUse cluster-b (AWS) as Broker with service discovery enabled subctl deploy-broker --kubeconfig cluster-b/auth/kubeconfig --service-discovery Join cluster-b (AWS) and cluster-a (vSphere) to the Broker subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b --ikeport 501 --nattport 4501 subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a --ikeport 501 --nattport 4501  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-a.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification This will perform automated verifications between the clusters.\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/deployment/calico/",
	"title": "Calico CNI",
	"tags": [],
	"description": "",
	"content": "Typically, the Kubernetes network plugin (based on kube-proxy) programs iptables rules for Pod networking within a cluster. When a Pod in a cluster tries to access an external IP, the plugin performs specific Network Address Translation (NAT) manipulation on the traffic as it does not belong to the local cluster. Similarly, Submariner also programs certain iptables rules and it requires these rules to be applied prior to the ones programmed by the network plugin. Submariner tries to preserve the source IP of the Pods for cross-cluster communication for visibility, ease of debugging, and security purposes.\nOn clusters deployed with Calico as the network plugin, the rules inserted by Calico take precedence over Submariner, causing issues with cross-cluster communication. To make Calico compatible with Submariner, it needs to be configured, via IPPools, not to perform NAT on the subnets associated with the Pod and Service CIDRs of the remote clusters. Once the IPPools are configured in the clusters, Calico will not perform NAT for the configured CIDRs and allows Submariner to support cross-cluster connectivity.\nWhen using Submariner Globalnet with Calico, please avoid the default globalnet-cidr (i.e., 169.254.0.0/16) as its used internally within Calico. You can explicitly specify a non-overlapping globalnet-cidr while deploying Submariner.\n As an example, consider two clusters, East and West, deployed with the Calico network plugin and connected via Submariner. For cluster East, the Service CIDR is 100.93.0.0/16 and the Pod CIDR is 10.243.0.0/16. For cluster West, they are 100.92.0.0/16 and 10.242.0.0/16. The following IPPools should be created:\nOn East Cluster:\n$ cat \u0026gt; svcwestcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: svcwestcluster spec: cidr: 100.92.0.0/16 natOutgoing: false disabled: true EOF cat \u0026gt; podwestcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: podwestcluster spec: cidr: 10.242.0.0/16 natOutgoing: false disabled: true EOF DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-eastcluster.yaml\u0026gt; calicoctl create -f svcwestcluster.yaml DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-eastcluster.yaml\u0026gt; calicoctl create -f podwestcluster.yaml On West Cluster:\ncat \u0026gt; svceastcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: svceastcluster spec: cidr: 100.93.0.0/16 natOutgoing: false disabled: true EOF cat \u0026gt; podeastcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: podeastcluster spec: cidr: 10.243.0.0/16 natOutgoing: false disabled: true EOF DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-westcluster.yaml\u0026gt; calicoctl create -f svceastcluster.yaml DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-westcluster.yaml\u0026gt; calicoctl create -f podeastcluster.yaml "
},
{
	"uri": "/reading_material/",
	"title": "Online Resources",
	"tags": [],
	"description": "",
	"content": "There are multiple presentations and demo recordings about Submariner available online.\nConference Presentations  KubeCon China 2019 slides and video recording KubeCon North America 2019 slides and video recording  Demo Recordings  Submariner in 60s Deploying Submariner with subctl Connecting hybrid Kubernetes clusters using Submariner Cross-cluster service discovery in Submariner using Lighthouse  Blogs  Kubernetes Multi-Cloud and Multi-Cluster Connectivity with Submariner Multicluster Service Discovery in OpenShift with Submariner and Lighthouse (Part 1) Multicluster Service Discovery in OpenShift with Submariner and Lighthouse (Part 2)  SIG Presentations  SIG Multicluster demo of Submariner\u0026rsquo;s KEP1645 Multicluster Services implementation (2020/09/22) SIG Multicluster demo of Submariner\u0026rsquo;s multicluster networking deployed by Submariner\u0026rsquo;s Operator and subctl (2019/12/17)  If you find additional material that isn\u0026rsquo;t listed here, please feel free to add it to this page by editing it. The website contributing guide is here.\n"
},
{
	"uri": "/quickstart/rancher/",
	"title": "Rancher 2.x",
	"tags": [],
	"description": "",
	"content": " Prerequisites These instructions were developed with Rancher v2.4.x\nMake sure you are familiar with Rancher, and creating clusters. You can create either node driver clusters or Custom clusters, as long as your designated gateway nodes can communicate with each other.\nCreate and Deploy Cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.42.0.0/16 10.43.0.0/16    Use the Rancher UI to create a cluster, leaving the default options selected.\nMake sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nCreate and Deploy Cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.44.0.0/16 10.45.0.0/16    Create your cluster, but select Edit as YAML in the cluster creation UI. Edit the services stanza to reflect the options below, while making sure to keep the options that were already defined.\nservices: kube-api: service_cluster_ip_range: 10.45.0.0/16 kube-controller: cluster_cidr: 10.44.0.0/16 service_cluster_ip_range: 10.45.0.0/16 kubelet: cluster_domain: cluster.local cluster_dns_server: 10.45.0.10 Make sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nOnce you have done this, you can deploy your cluster.\nInstall subctl Download the subctl binary and make it available on your PATH. curl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin  ~/.profile    Obtain the kubeconfig files from the Rancher UI for each of your clusters, placing them in the respective kubeconfigs.\n   Cluster Kubeconfig File Name     Cluster A kubeconfig-cluster-a   Cluster B kubeconfig-cluster-b    Use cluster-a as Broker subctl deploy-broker --kubeconfig kubeconfig-cluster-a Join cluster-a and cluster-b to the Broker subctl join --kubeconfig kubeconfig-cluster-a broker-info.subm --clusterid cluster-a subctl join --kubeconfig kubeconfig-cluster-b broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster Pods and Services\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only connectivity --verbose "
},
{
	"uri": "/contributing/community-membership/",
	"title": "Community Membership",
	"tags": [],
	"description": "",
	"content": "This is a stripped-down version of the Kubernetes Community Membership process.\nAlthough we aspire to follow the Kubernetes process, some parts are not currently relevant to our structure or possible with our tooling:\n The SIG and subproject abstraction layers don\u0026rsquo;t apply to Submariner. Submariner is treated as a single project with file-based commit rights, not a \u0026ldquo;project\u0026rdquo; per repository. We hope to eventually move to Kubernetes OWNERS and Prow, but until we do so we can\u0026rsquo;t support advanced role-based automation (reviewers vs approvers; PR workflow commands like /okay-to-test, /lgtm, /approved).   This doc outlines the various responsibilities of contributor roles in Submariner.\n   Role Responsibilities Requirements Defined by     Member Active contributor in the community Sponsored by 2 committers, multiple contributions to the project Submariner GitHub org member   Committer Approve contributions from other members History of review and authorship CODEOWNERS file entry   Owner Set direction and priorities for the project Demonstrated responsibility and excellent technical judgement for the project Submariner-owners GitHub team member    New Contributors New contributors should be welcomed to the community by existing members, helped with PR workflow, and directed to relevant documentation and communication channels.\nWe require every contributor to certify that they are legally permitted to contribute to our project. A contributor expresses this by consciously signing their commits, and by this act expressing that they comply with the Developer Certificate Of Origin.\nEstablished Community Members Established community members are expected to demonstrate their adherence to the principles in this document, familiarity with project organization, roles, policies, procedures, conventions, etc., and technical and/or writing ability. Role-specific expectations, responsibilities, and requirements are enumerated below.\nMember Members are continuously active contributors in the community. They can have issues and PRs assigned to them and participate through GitHub teams. Members are expected to remain active contributors to the community.\nDefined by: Member of the Submariner GitHub organization.\nMember Requirements  Enabled two-factor authentication on their GitHub account Have made multiple contributions to the project or community. Contribution may include, but is not limited to:  Authoring or reviewing PRs on GitHub Filing or commenting on issues on GitHub Contributing to community discussions (e.g. meetings, Slack, email discussion forums, Stack Overflow)   Subscribed to submariner-dev@googlegroups.com Have read the contributor guide Actively contributing Sponsored by 2 committers. Note the following requirements for sponsors:  Sponsors must have close interactions with the prospective member - e.g. code/design/proposal review, coordinating on issues, etc. Sponsors must be committers in at least 1 CODEOWNERS file either in any repo in the Submariner org   Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the member template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Member Responsibilities and Privileges  Responsive to issues and PRs assigned to them Responsive to mentions of teams they are members of Active owner of code they have contributed (unless ownership is explicitly transferred)  Code is well tested Tests consistently pass Addresses bugs or issues discovered after code is accepted   They can be assigned to issues and PRs, and people can ask members for reviews  Note: Members who frequently contribute code are expected to proactively perform code reviews and work towards becoming a committer.\nCommitters Committers are able to review code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles.\nUntil automation supports approvers vs reviewers: They also review for holistic acceptance of a contribution including: backwards / forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, etc.\nDefined by: Entry in an CODEOWNERS file in a repo owned by the Submariner project.\nCommitter status is scoped to a part of the codebase.\nCommitter Requirements The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Member for at least 3 months Primary reviewer for at least 5 PRs to the codebase Reviewed at least 20 substantial PRs to the codebase Knowledgeable about the codebase Sponsored by two committers or project owners  With no objections from other committers or project owners   May either self-nominate or be nominated by a committer/owner Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the committer template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers/owners reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Committer Responsibilities and Privileges The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Responsible for project quality control via code reviews  Focus on code quality and correctness, including testing and factoring Until automation supports approvers vs reviewers: Focus on holistic acceptance of contribution such as dependencies with other features, backwards / forwards compatibility, API and flag definitions, etc   Expected to be responsive to review requests as per community expectations Assigned PRs to review related to project of expertise Assigned test bugs related to project of expertise Granted \u0026ldquo;read access\u0026rdquo; to submariner repo May get a badge on PR and issue comments Demonstrate sound technical judgement Mentor contributors and reviewers  Project Owner Project owners are the technical authority for the Submariner project. They MUST have demonstrated both good judgement and responsibility towards the health the project. Project owners MUST set technical direction and make or approve design decisions for the project - either directly or through delegation of these responsibilities.\nDefined by: Member of the submariner-owners GitHub team.\nOwner Requirements Unlike the roles outlined above, the owners of the project are typically limited to a relatively small group of decision makers and updated as fits the needs of the project.\nThe following apply to people who would be an owner:\n Deep understanding of the technical goals and direction of the project Deep understanding of the technical domain of the project Sustained contributions to design and direction by doing all of:  Authoring and reviewing proposals Initiating, contributing and resolving discussions (emails, GitHub issues, meetings) Identifying subtle or complex issues in designs and implementation PRs   Directly contributed to the project through implementation and / or review  Owner Responsibilities and Privileges The following apply to people who would be an owner:\n Make and approve technical design decisions for the project Set technical direction and priorities for the project Define milestones and releases Mentor and guide committers and contributors to the project Ensure continued health of project  Adequate test coverage to confidently release Tests are passing reliably (i.e. not flaky) and are fixed when they fail   Ensure a healthy process for discussion and decision making is in place Work with other project owners to maintain the project\u0026rsquo;s overall health and success holistically  "
},
{
	"uri": "/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "Submariner organizes all current and upcoming work using GitHub Issues, Projects, and Milestones.\nPlanning Process In preparation for sprint planning meetings (see Submariner\u0026rsquo;s Community Calendar), GitHub Issues should be raised for work that is to be a part of a sprint. Issues targeted for a sprint should be added to the upcoming Project\u0026rsquo;s \u0026ldquo;Backlog\u0026rdquo; column. During sprint planning meetings, Issues will be discussed and moved to the \u0026ldquo;TODO\u0026rdquo; column and the milestone will be set to the targeted release. As contributors make progress during sprints, Issues should be moved through the \u0026ldquo;In Progress\u0026rdquo;/\u0026ldquo;Review/Verify\u0026rdquo;/\u0026ldquo;Done\u0026rdquo; columns of the Project. If an Issue is implemented during a release but additional work (like an ACK-fixed verification) tracked by the relevant Issue is necessary, the Issue can be carried over to the next Project but the Milestone should reflect where the code was shipped for accurate release note creation.\nCurrent Work Current and near-future work is tracked by Submariner\u0026rsquo;s open Projects.\nFuture Work Some high-level goals are summarized here, but the primary source for tracking future work are Submariner\u0026rsquo;s GitHub Issues.\n Auto detecting NAT vs non-NAT scenarios (https://github.com/submariner-io/submariner/issues/300) Support different IPsec ports for each cluster Network Policy across clusters (Coastguard) Support for finer-grained connectivity policies (https://github.com/submariner-io/submariner/issues/533) Support for OVN-based clusters Globalnet: annotating Global IPs per namespaces (https://github.com/submariner-io/submariner/issues/528) Globalnet: only annotate Services for which a ServiceExport has been created (https://github.com/submariner-io/submariner/issues/652) More tunnel encapsulation options Dynamic routing with BGP to support multi-path forwarding across gateways Testing with multi-cluster Istio  Suggesting Work If we are missing something that would make Submariner more useful to you, please let us know. The best way is to file an Issue and include information on how you intend to use Submariner with that feature.\n"
},
{
	"uri": "/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "Talk to Us We would love to hear from you, how you are using Submariner, and what we can do to make it better.\nGitHub Project Check out the project and consider contributing. Pick up an issue to work on or propose an enhancement by reporting a new issue; once your code is ready to be reviewed, you can propose a pull request. You can find a good guide about the GitHub workflow here.\n#submariner Share your ideas in the #submariner channel in Kubernetes\u0026rsquo; Slack. If you need it, you can request an invite to Kubernetes Slack instance.\nCommunity Calendar As a member of the Submariner Community, join any of our community meetings - no registration required. The weekly Submariner Community Meeting (Tuesdays at 5:00pm CET) is a good place to start.\nMailing List Join the developer mailing list.\n"
},
{
	"uri": "/contributing/website/",
	"title": "Contributing to the Website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on Hugo, Grav, Hugo-theme-learn, and is written in Markdown format.\nYou can always click the Edit this page link at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on GitHub.\n  Check out your copy locally:\ngit clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git cd submariner-website make server   An instance of the website is now running locally on your machine and is accessible at http://localhost:1313.\n  Edit files in src. The browser should automatically reload so you can test your changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the GitHub workflow here.\n  "
},
{
	"uri": "/contributing/release-process/",
	"title": "Release Process",
	"tags": [],
	"description": "",
	"content": "This section describes how to create a Submariner product release. It is assumed that you are familiar with the various Submariner projects and their repositories and are familiar with Git and GitHub.\nProject dependencies The Submariner projects have a dependency hierarchy with respect to their Go libraries and Docker images. Therefore the releases for each individual project must be created in a specific order.\nThe Go dependency hierarchy flow is as follows:\nshipyard \u0026lt;- admiral \u0026lt;- [submariner, lighthouse] \u0026lt;- submariner-operator\nNote that the submariner and lighthouse projects are siblings and thus do not depend on one another. Also the submariner-operator components expect that lighthouse and submariner are aligned on the same exact version.\nThe Docker image dependency hierarchy flow is as follows:\nsubctl binary \u0026lt;- shipyard dapper base image \u0026lt;- [admiral, submariner, lighthouse, submariner-operator]\nThe Dapper base image that is provided by shipyard for building and E2E tests in all of the other projects pulls in the subctl binary.\nRelease versions The vx.x.x version format is used for the git projects while x.x.x is used for Docker images.\nThe typical workflow is to first create release candidate(s) for testing before creating the final release. The suggested naming convention is to append -rcN to the final version, for example v0.5.0-rc0, v0.5.0-rc1 etc.\nSometimes you may want to create a specific project release for testing prior to creating a release candidate. In this case, the suggested naming convention is to append -preN.\nCreate Submariner product release The following sections outline the steps to be taken in order to create a full Submariner product release. As an example, we\u0026rsquo;ll use version v0.5.0.\nProject releases can either be created via the git CLI or the GitHub UI. This guide uses the GitHub UI as it is simpler. Here are general steps for creating a new release in a project:\n  Navigate to the project\u0026rsquo;s main releases page.\n  Click Draft a new release\n  Fill in the Tag version field with the new version.\n  Fill in the Release title field as appropriate. Typically this is just the version (sans the leading v) as a release usually contains various changes. But if it\u0026rsquo;s a targeted release then the Release title can reflect that.\n  Enter information to describe the release. This is optional for pre-releases but should be filled in for a final release.\n  If this is not a final release, mark the checkbox This is a pre-release. This includes release candidates. This is important so it is not labeled as the Latest release in GitHub.\n  Click Publish release.\n  For most projects a GitHub action job will be initiated to build release artifacts and publish to Quay. This will take several minutes. You can monitor the progress from the project\u0026rsquo;s main page. In the branches/tags pull-down above the file listing heading, select the tag for the new version. A small yellow circle icon should be present to the right of the file listing heading which indicates a job is in progress. You can click it to see details. There may be several checks for the job listed but the important one is Release Images. When complete, the indicator icon will change to either a green check mark on success or a red X on failure. A failure likely means the artifacts were not published to Quay, in which case select the failed check, inspect the logs, correct the issue and re-run the job.\nStep 0: Create a subctl pre-release, optional Since subctl is provided by shipyard's Dapper base image, if there are specific changes to subctl which other projects require, you will need to create a pre-release (for example v0.5.0-pre0) of the submariner-operator project:\n  Navigate to the releases page and create the release with the new version.\n  In the shipyard project, edit package/Dockerfile.shipyard-dapper-base and set SUBCTL_VERSION to the new version.\n  Commit the change and create a pull request with the test-projects label so it runs the E2E tests on the consuming projects as well. After the tests successfully complete, have it merged.\n  Step 1: Create release for the shipyard project Navigate to the releases page and create the release with the new version. This will initiate a job to build the Dapper base image. Once successfully completed, the generated image version (0.5.0) should be available on Quay here:\n https://quay.io/repository/submariner/shipyard-dapper-base?tab=tags\n Step 2: Create release for the admiral project   Pin the shipyard Dapper base image to the new version. Edit Dockerfile.dapper and, on the first line, change the shipyard-dapper-base image version from devel to the new version (0.5.0).\n  Update the go.mod and go.sum references for shipyard to the new version:\nmake shell go get github.com/submariner-io/shipyard@v0.5.0 go mod vendor go mod tidy exit   Commit the changes, create a pull request, and have it merged.\n  Navigate to releases and create the release for the new version.\n  Step 3: Create releases for the submariner and lighthouse projects These can be done in any order or in parallel and the process is the same.\n  Pin the shipyard Dapper base image to the new version. Edit Dockerfile.dapper and, on the first line, change the shipyard-dapper-base image version from devel to the new version (0.5.0).\n  Update the go.mod and go.sum references for the dependent projects to the new version:\nmake shell go get github.com/submariner-io/shipyard@v0.5.0 go get github.com/submariner-io/admiral@v0.5.0 go mod vendor go mod tidy   Commit the changes, create a pull request, and have it merged.\n  Navigate to the project\u0026rsquo;s releases page and create the release for the new version.\n  Wait for the project\u0026rsquo;s new images to be generated and published to Quay.\nFor submariner:\n https://quay.io/repository/submariner/submariner?tab=tags https://quay.io/repository/submariner/submariner-route-agent?tab=tags https://quay.io/repository/submariner/submariner-globalnet?tab=tags\n For lighthouse:\n https://quay.io/repository/submariner/lighthouse-agent?tab=tags https://quay.io/repository/submariner/lighthouse-coredns?tab=tags\n Step 4: Create release for the submariner-operator project   Pin the shipyard Dapper base image to the new version. Edit Dockerfile.dapper and, on the first line, change the shipyard-dapper-base image version from devel to the new version (0.5.0).\n  Update the go.mod and go.sum references for the dependent projects to the new version:\nmake shell go get github.com/submariner-io/shipyard@v0.5.0 go get github.com/submariner-io/submariner@v0.5.0 go get github.com/submariner-io/lighthouse@v0.5.0 go mod vendor go mod tidy   Edit pkg/versions/versions.go and update the *Version constants to the new version:\nDefaultSubmarinerOperatorVersion = \u0026#34;0.5.0\u0026#34; DefaultSubmarinerVersion = \u0026#34;0.5.0\u0026#34; DefaultLighthouseVersion = \u0026#34;0.5.0\u0026#34;   Commit the changes, create a pull request, and have it merged.\n  Navigate to releases and create the release for the new version.\n  Once the image release job successfully completes, the generated image version (0.5.0) should be available on Quay here:\n https://quay.io/repository/submariner/submariner-operator?tab=tags\n There is a separate job to publish the subctl binaries for the various platforms. These should be listed under the Assets section for the new release on the main releases page. If not then the job failed so correct the issue and re-run the job.\nNote that either job could fail while the other succeeds so it is important to verify both.\nStep 5: Update subctl version in shipyard Update the Dapper base image to pull in the latest subctl binary:\n  In the shipyard project, edit package/Dockerfile.shipyard-dapper-base and set SUBCTL_VERSION to the new version.\n  Commit the change and create a pull request with the test-projects label so it runs the E2E tests on the consuming projects as well. After the tests successfully complete, have it merged.\n  Step 6: Unpin the shipyard Dapper base image version At this point the new product release has been successfully created however we don\u0026rsquo;t want to leave each downstream project pinned to the new shipyard Dapper base image version. For ongoing development we want each project to automatically pick up the latest changes to the base image.\nFor each project, admiral, lighthouse, submariner, and submariner-operator:\n  Edit Dockerfile.dapper and, on the first line, change the shipyard-dapper-base image version back to devel.\n  Commit the changes, create a pull request, and have it merged.\n  Step 7: Add release notes If this is a final release, add a section for it on this website\u0026rsquo;s release notes page.\n  Clone the submariner-website project.\n  Open src/content/releases/_index.en.md and make changes.\n  Commit the changes, create a pull request, and have it reviewed and merged.\n  Alternatively you can edit the file and create a pull request directly on GitHub here\nStep 8: Verify the release You can follow any of the quick start guides.\nStep 9: Update the Submariner Operator on OperatorHub.io The community-operators Git repository is the source for sharing Kubernetes Operators with the broader community. This repository is split into two sections:\n Operators for deployment to a vanilla Kubernetes environment (upstream-community-operators). These are shared with the Kubernetes community via OperatorHub.io. Operators for deployment to OpenShift (community-operators)  To publish the Submariner Operator to the community, perform the following steps:\n  Clone the submariner-operator project\n  Make sure you have the operator-sdk v0-18-x installed on your machine otherwise follow this guide\n  Generate a new CSV file by running the command:\noperator-sdk generate csv \\  --csv-version=${OPERATOR_VERSION} \\  --csv-channel=alpha \\  --default-channel=true \\  --operator-name=submariner \\  --update-crds \\  --make-manifests=false \\  --interactive=false the generated package output should be located in deploy/olm-catalog/submariner\n  Fork and clone community-operators project.\n  Update the Kubernetes Operator:\n copy the generated package from step 3 into upstream-community-operators/submariner compare the new CSV file with the previous one and update the missing fields (e.g spec.description) update the version in upstream-community-operators/submariner/submariner.package.yaml test the Operator by running the command: make operator.test OP_PATH=upstream-community-operators/submariner preview the Operator on OperatorHub.io once everything is fine, review this checklist and create a new PR on community-operators    Update the OpenShift Operator:\n re-run step 5 on the community-operators/submariner directory.    Step 10: Announce the release Via E-Mail  https://bit.ly/submariner-dev https://bit.ly/submariner-users  Via Twitter  https://twitter.com/submarinerio  "
},
{
	"uri": "/contributing/shipyard/",
	"title": "Working With Shipyard",
	"tags": [],
	"description": "",
	"content": "Overview The Shipyard project provides common tooling for creating Kubernetes clusters with kind (Kubernetes in Docker) and provides a common Go framework for creating end to end tests. Shipyard contains common functionality shared by other projects. Any project specific functionality should be part of that project.\nA base image quay.io/submariner/shipyard-dapper-base is created from Shipyard and contains all the tooling to build other projects and run tests in a consistent environment.\nShipyard has several folders at the root of the project:\n package: Contains the ingredients to build the base image. scripts: Contains general scripts for Shipyard make targets.  shared: Contains all the shared scripts that projects can consume. These are copied into the base image under $SCRIPTS_DIR.  lib: Library functions that shared scripts, or consuming projects, can use. resources: Resource files to be used by the shared scripts.     test: Test library to be used by other projects.  Shipyard ships with some Makefile targets which can be used by consuming projects and are used by Shipyard\u0026rsquo;s CI to test and validate itself. It also has some specific Makefile targets which are used by the project itself.\nUsage Add Shipyard to a Project To enable usage of Shipyard\u0026rsquo;s functionality, please see Adding Shipyard to a Project.\nUse Shipyard in Your Project Once Shipyard has been added to a project, you can use any of the Makefile targets that it provides.\nAny variables that you need to pass to these targets should be specified in your Dockerfile.dapper so they\u0026rsquo;re available in the Dapper environment. For example:\nENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT CLUSTERS_ARGS DEPLOY_ARGS\u0026#34;Have Shipyard Targets Depend on Your Project\u0026rsquo;s Targets Having any of the Shipyard Makefile targets rely on your project\u0026rsquo;s specific targets can be done easily by adding the dependency in your project\u0026rsquo;s Makefile. For example:\nclusters: build images Use an Updated Shipyard Image in Your Project If you\u0026rsquo;ve made changes to Shipyard\u0026rsquo;s base image and need to test them in your project, run:\nmake dapper-image in the Shipyard directory. This creates a local image with your changes available for consumption in other projects.\nShared Makefile Targets Shipyard ships a Makefile.inc file which defines these basic targets:\n clusters: Creates the kind-based cluster environment. deploy : Deploys submariner components in the cluster environment (depends on clusters). cleanup: Deletes the kind environment (if it exists) and any residual resources. release: Uploads the requested image(s) to Quay.io. vendor/modules.txt: Populates go modules (in case go.mod exists in the root directory).  If your project uses Shipyard then it has all these targets and supports all the variables these targets support.\nAny variables supported by these targets can be either declared as environment variables or assigned on the make command line (takes precedence over environment variables).\nClusters A Make target that creates a kind-based multi-cluster environment with just the default Kubernetes deployment:\nmake clusters Respected variables:\n CLUSTERS_ARGS: Any arguments (flags and/or values) to be sent to the clusters.sh script. To get a list of available arguments, run: scripts/shared/clusters.sh --help  Deploy A Make target that deploys Submariner components in a kind-based cluster environment (if one isn\u0026rsquo;t created yet, this target will first invoke the clusters target to do so):\nmake deploy Respected variables:\n Any variable from clusters target (only if it wasn\u0026rsquo;t created). DEPLOY_ARGS: Any arguments (flags and/or values) to be sent to the deploy.sh script. To get a list of available arguments, run: scripts/shared/deploy.sh --help  Cleanup To clean up all the kind clusters deployed in any of the previous steps, use:\nmake cleanup This command will remove the clusters and any resources that might\u0026rsquo;ve been left in docker that are not needed any more (images, volumes, etc).\nRelease Uploads the built images to Quay.io:\nmake release release_images=\u0026#34;\u0026lt;image name\u0026gt;\u0026#34; Respected variables:\n QUAY_USERNAME, QUAY_PASSWORD: Needed in order to log in to Quay. release_images: One or more image names to release separated by spaces. release_tag: A tag to use for the release (default is latest). repo: The Quay repo to use (default is quay.io/submariner).  Specific Makefile Targets Shipyard has some project-specific targets which are used to build parts of the projects:\n dapper-image: Builds the base image that can be used by other projects. validate: Validates the go code that Shipyard provides, and the shared shell scripts.  Dapper-Image Builds the basic image which is then used by other projects to build the code and run tests:\nmake dapper-image Respected variables:\n dapper_image_flags: Any additional flags and values to be sent to the build_image.sh script.  "
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Submariner Submariner enables direct networking between Pods and Services in different Kubernetes clusters, either on-premises or in the cloud.\nWhy Submariner As Kubernetes gains adoption, teams are finding they must deploy and manage multiple clusters to facilitate features like geo-redundancy, scale, and fault isolation for their applications. With Submariner, your applications and services can span multiple cloud providers, data centers, and regions.\nSubmariner is completely open source, and designed to be network plugin (CNI) agnostic.\nWhat Submariner Provides  Cross-cluster L3 connectivity using encrypted VPN tunnels Service Discovery across clusters subctl, a friendly deployment tool Support for interconnecting clusters with overlapping CIDRs  Check the Quickstart guide section for deployment instructions.\n "
},
{
	"uri": "/quickstart/openshift/create_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b "
},
{
	"uri": "/quickstart/openshift/ready_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Make your clusters ready for Submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default OpenShift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for Submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n "
},
{
	"uri": "/quickstart/openshift/setup_openshift/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n "
},
{
	"uri": "/quickstart/rancher/create_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Create and Deploy Cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.42.0.0/16 10.43.0.0/16    Use the Rancher UI to create a cluster, leaving the default options selected.\nMake sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nCreate and Deploy Cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.44.0.0/16 10.45.0.0/16    Create your cluster, but select Edit as YAML in the cluster creation UI. Edit the services stanza to reflect the options below, while making sure to keep the options that were already defined.\nservices: kube-api: service_cluster_ip_range: 10.45.0.0/16 kube-controller: cluster_cidr: 10.44.0.0/16 service_cluster_ip_range: 10.45.0.0/16 kubelet: cluster_domain: cluster.local cluster_dns_server: 10.45.0.10 Make sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nOnce you have done this, you can deploy your cluster.\n"
},
{
	"uri": "/quickstart/rancher/prerequisites/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Prerequisites These instructions were developed with Rancher v2.4.x\nMake sure you are familiar with Rancher, and creating clusters. You can create either node driver clusters or Custom clusters, as long as your designated gateway nodes can communicate with each other.\n"
},
{
	"uri": "/quickstart/verify_with_discovery/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service Note that headless Services can only be exported on non-globalnet deployments.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-a.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification This will perform automated verifications between the clusters.\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]