[
{
	"uri": "/architecture/broker/",
	"title": "Broker",
	"tags": [],
	"description": "",
	"content": "Submariner uses a central Broker component to facilitate the exchange of metadata information between Gateway Engines deployed in participating clusters. The Broker is basically a set of Custom Resource Definitions (CRDs) backed by the Kubernetes datastore. The Broker also defines a ServiceAccount and RBAC components to enable other Submariner components to securely access the Broker\u0026rsquo;s API. There are no Pods or Services deployed with the Broker.\nSubmariner defines two CRDs that are exchanged via the Broker: Endpoint and Cluster. The Endpoint CRD contains the information about the active Gateway Engine in a cluster, such as its IP, needed for clusters to connect to one another. The Cluster CRD contains static information about the originating cluster, such as its Service and Pod CIDRs.\nThe Broker is a singleton component that is deployed on a cluster whose Kubernetes API must be accessible by all of the participating clusters. If there is a mix of on-premise and public clusters, the Broker can be deployed on a public cluster. The Broker cluster may be one of the participating clusters or a standalone cluster without the other Submariner components deployed. The Gateway Engine components deployed in each participating cluster are configured with the information to securely connect to the Broker cluster\u0026rsquo;s API.\n"
},
{
	"uri": "/architecture/gateway-engine/",
	"title": "Gateway Engine",
	"tags": [],
	"description": "",
	"content": "The Gateway Engine component is deployed in each participating cluster and is responsible for establishing secure tunnels to other clusters.\nInstances of the Gateway Engine run on specifically designated nodes in a cluster of which there may be more than one for fault tolerance. There is only one active Gateway Engine instance at a time in a cluster. They perform a leader election process to determine the active instance and the others await in standby mode ready to take over should the active instance fail.\nThe active Gateway Engine communicates with the central Broker to advertise its Endpoint and Cluster resources to the other clusters connected to the Broker, also ensuring that it is the sole Endpoint for its cluster. The active Gateway Engine also establishes a watch on the Broker to learn about the active Endpoint and Cluster resources advertised by the other clusters. Once two clusters are aware of each other\u0026rsquo;s Endpoints, they can establish a secure tunnel through which traffic can be routed.\nIf the active Gateway Engine fails, another Gateway Engine on one of the other designated nodes will gain leadership and perform reconciliation to advertise its Endpoint and to ensure that it is the sole Endpoint. The remote clusters will learn of the new Endpoint via the Broker and establish a new tunnel.\nThe Gateway Engine is deployed as a DaemonSet that is configured to only run on nodes labelled with submariner.io/gateway=true.\nThe Gateway Engine has a pluggable architecture for the cable engine component that maintains the tunnels. The default implementation uses the Strongswan IPsec library. An implementation for WireGuard is also available.\n"
},
{
	"uri": "/architecture/route-agent/",
	"title": "Route Agent",
	"tags": [],
	"description": "",
	"content": "The Route Agent component runs on every worker node in each participating cluster. It is responsible for setting up VxLAN tunnels and routing the cross cluster traffic from the node to the cluster’s active Gateway Engine which subsequently sends the traffic to the destination cluster.\nWhen running on the same node as the active Gateway Engine, the Route Agent creates a VxLAN VTEP interface to which the Route Agent instances running on the other worker nodes in the local cluster connect by establishing a VXLAN tunnel with the VTEP of the active Gateway Engine node. The MTU of VxLAN tunnel is configured based on the MTU of the default interface on the host minus the VxLAN overhead.\nThe Route Agent uses Endpoint resources synced from other clusters to configure routes and to program the necessary IP table rules to enable full cross-cluster connectivity.\nWhen the active Gateway Engine fails and a new Gateway Engine takes over, the Route Agent will automatically update the route tables on each node to point to the new active Gateway Engine node.\n"
},
{
	"uri": "/architecture/globalnet/",
	"title": "Globalnet Controller",
	"tags": [],
	"description": "",
	"content": "Introduction Submariner is a tool built to connect overlay networks of different Kubernetes clusters. These clusters can be on different public clouds or on-premise. An important use case for Submariner is to connect disparate independent clusters into a single cohesive multi-cluster fleet.\nHowever, by default, a limitation of submariner is that it doesn\u0026rsquo;t handle overlapping CIDRs (ServiceCIDR and ClusterCIDR) across these clusters. Each cluster must use distinct CIDRs that don\u0026rsquo;t conflict or overlap with any other cluster that is going to be part of cluster fleet.\nThis is largely problematic because most actual deployments use the default CIDRs for a cluster so every cluster ends up using the same CIDRs. Changing CIDRs on existing clusters is a very disruptive process and requires a cluster restart. So submariner needs a way to allow clusters with overlapping CIDRs to connect together.\nArchitecture To support overlapping CIDRs in clusters connected through submariner, submariner has a component called Global Private Network, GlobalNet (globalnet). This GlobalNet is a virtual network specifically to support submariner\u0026rsquo;s multi-cluster solution with a global CIDR. Each cluster is given a subnet from this virtual Global Private Network, configured as new cluster parameter GlobalCIDR (e.g. 169.254.0.0/16) which is configurable at time of deployment. User can also manually specify GlobalCIDR for each cluster that is joined to the broker using the flag globalnet-cidr passed to subctl join command. If Globalnet is not enabled in the broker or if a GlobalCIDR is preconfigured in the cluster, the supplied globalnet-cidr will be ignored.\nOnce configured, each Service and Pod that requires cross-cluster access is allocated an IP, named globalIp, from this GlobalCIDR that is annotated on the Pod/Service object. This globalIp is used for all cross-cluster communication to and from a Pod and the globalIp of a remote Service. Routing and IPTable rules are configured to use the globalIp for ingress and egress. All address translations occur on the Gateway node.\nUnlike vanilla Submariner, where Pod to Pod connectivity is also supported, GlobalNet only supports Pod to remote Service connectivity using globalIps.\n submariner-globalnet Submariner GlobalNet is a component that provides cross-cluster connectivity from Pods to remote Services using their globalIps. Compiled as binary submariner-globalnet, it is responsible for maintaining a pool of global IPs, allocating IPs from the GlobalIp pool to pods and services, annotating Services and Pods with their globalIp, and configuring the required rules on the gateway node to provide cross-cluster connectivity using globalIps. Globalnet also supports connectivity from the Nodes (including Pods that use HostNetworking) to globalIp of remote Services. It mainly consists of two key components: the IP Address Manager and Globalnet.\nIP Address Manager (IPAM) The IP Address Manager (IPAM) component does the following:\n Creates a pool of IP addresses based on the GlobalCIDR configured on cluster. On creation of a Pod/Service, allocates a globalIp from the GlobalIp pool. Annotates the Pod/Service with submariner.io/globalIp=\u0026lt;global-ip\u0026gt;. On deletion of a Pod/Service, releases its globalIp back to the pool.  Globalnet This component is responsible for programming the routing entries, IPTable rules and does the following:\n Creates initial IPTables chains for Globalnet rules. Whenever a Pod is annotated with a globalIp, creates an egress SNAT rule to convert the source Ip from the Pod\u0026rsquo;s Ip to the Pod\u0026rsquo;s globalIp on the Gateway Node. Whenever a Service is annotated with a globalIp, creates an ingress rule to direct all traffic destined to the Service\u0026rsquo;s globalIp to the Service\u0026rsquo;s kube-proxy IPTables chain which in turn directs traffic to Service\u0026rsquo;s backend Pods. On deletion of pod/service, clean up the rules from the gateway node.  Globalnet currently relies on kube-proxy and thus will only work with deployments that use kube-proxy.\nService Discovery - Lighthouse Connectivity is only part of the solution as pods still need to know the IPs of services on remote clusters.\nThis is achieved by enhancing lighthouse with support for GlobalNet. The Lighthouse controller adds the service\u0026rsquo;s globalIp to the MultiClusterService object that is distributed to all clusters. The lighthouse plugin then uses the Service\u0026rsquo;s globalIp when replying to DNS queries for the Service.\nBuilding Nothing extra needs to be done to build submariner-globalnet as it is built with the standard submariner build.\n"
},
{
	"uri": "/contributing/code-review/",
	"title": "Code Review Guide",
	"tags": [],
	"description": "",
	"content": "Code Review Guide This guide is meant to facilitate Submariner code review by sharing norms, best practices, and useful patterns.\nSubmariner follows the Kubernetes Code Review Guide wherever relevant. This guide collects the most important highlights of the K8s process and adds Submariner-specific extensions.\nTwo non-author approvals required Pull Requests to Submariner require two non-author code review approvals.\nAt least one approval must be from a Committer to the relevant part of the code base, as defined by the CODEOWNERS file at the root of the repository.\nNo merge commits Kubernetes recommends avoiding merge commits.\nUse git fetch and git rebase to avoid them.\nSquash/amend commits into discrete steps Kubernetes recommends squashing commits using these guidelines.\nAfter a review, prepare your PR for merging by squashing your commits.\nAll commits left on your branch after a review should represent meaningful milestones or units of work. Use commits to add clarity to the development and review process.\nBefore merging a PR, squash the following kinds of commits:\n Fixes/review feedback Typos Merges and rebases Work in progress Aim to have every commit in a PR compile and pass tests independently if you can, but it\u0026rsquo;s not a requirement.  Commit message formatting Kubernetes recommends these commit message practices.\nIn summary:\n Separate subject from body with a blank line Limit the subject line to 50 characters Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs how  "
},
{
	"uri": "/contributing/website/style_guide/",
	"title": "Docs Style Guide",
	"tags": [],
	"description": "",
	"content": "Documentation Style Guide This guide is meant to help keep our documentation consistent and ease the contribution and review process.\nSubmariner follows the Kubernetes Documentation Style Guide wherever relevant. This is a Submariner-specific extension of those practices.\nSubmariner.io word list A list of Submariner-specific terms and words to be used consistently across the site.\n   Term Usage     Submariner The project name Submariner should always be capitalized.   Admiral The project name Admiral should always be capitalized.   Lighthouse The project name Lighthouse should always be capitalized.   Coastguard The project name Coastguard should always be capitalized.   Shipyard The project name Shipyard should always be capitalized.   subctl The artifact subctl should not be capitalized and should be formatted in code style.    "
},
{
	"uri": "/architecture/service-discovery/",
	"title": "Service Discovery",
	"tags": [],
	"description": "",
	"content": "The Lighthouse project provides DNS discovery for Kubernetes clusters connected by Submariner in multi-cluster environments. The solution is compatible with any CNI (Container Network Interfaces) plugin.\nArchitecture The below diagram shows the basic Lighthouse architecture.\nLighthouse Agent The Lighthouse Agent runs in every cluster and accesses the Kubernetes API server running in the broker cluster to exchange service metadata information with other clusters. Local service information is exported to the broker and service information from other clusters is imported.\nWorkFlow The workflow is as follows:\n Lighthouse agent connects to the broker\u0026rsquo;s K8s API server. For every service in the local cluster, it creates a corresponding MultiClusterService resource and exports it to the broker to be consumed by other clusters. For every MultiClusterService resource in the broker exported from another cluster, it creates a copy of it in the local cluster.  Lighthouse Plugin Lighthouse plugin can be installed as an external plugin for CoreDNS, and will work along with the default Kubernetes plugin. It uses the MultiClusterService resources that are distributed by the controller for DNS resolution. The below diagram indicates a high-level architecture.\nWorkFlow The workflow is as follows.\n A pod tries to resolve a Service name. The Kubernetes plugin in CoreDNS will first try to resolve the request. If no records are present the request will be sent to Lighthouse plugin. The Lighthouse plugin will use its MultiClusterService cache to try to resolve the request. If a record exists it will be returned, otherwise the plugin will pass the request to the next plugin registered in CoreDNS.  "
},
{
	"uri": "/contributing/website/",
	"title": "Contributing to the Website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on hugo, grav, and the hugo-learn-theme, and is written in Markdown format.\nIf you want to contribute, we recommend reading the hugo-learn-theme documentation.\nYou can always click the Edit this page link at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on GitHub.\n  Checkout your copy locally:\n  git clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git cd submariner-website make server  An instance of the website is now running locally on your machine and is accessible at http://localhost:1313\n  Edit files in src. The browser should automatically reload so you can test your changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the GitHub workflow here.\n  "
},
{
	"uri": "/quickstart/",
	"title": "Quickstart Guide",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner has two main core pieces (the broker and submariner), more information about this topic can be found in the Architecture section.\nThe Broker The broker is an API to which all participating clusters are given access and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a Cluster, and the reachable cluster IPs from the endpoint.  The broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe Submariner Deployment on a Cluster Once submariner is deployed on a cluster with the proper credentials to the broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n  At least 2 Kubernetes clusters, one of which is designated to serve as the central broker that is accessible by all of your connected clusters; this can be one of your connected clusters, but comes with the limitation that the cluster is required to be up to facilitate interconnectivity/negotiation.\n  Non-overlapping Service and Pod CIDRs between clusters. This is to prevent routing conflicts. For cases where addresses do overlap, GlobalNet can be set up.\n    IP reachability between the gateway nodes. Also, when connecting two clusters, at least one of the clusters should have a publicly routable IP address designated to the gateway node. This is needed for creating the IPsec tunnel between the clusters. The default ports used by IPsec are 4500/UDP and 500/UDP. For clusters behind corporate firewalls that block the default ports, Submariner also supports NAT Traversal (NAT-T) with the option to set custom non-standard ports like 4501/UDP and 501/UDP.\n  Knowledge of each cluster\u0026rsquo;s network configuration.\n  Worker node IPs on all connected clusters must be outside of the Pods/Service CIDR ranges.\n  An example of three clusters configured to use with Submariner (without GlobalNet) would look like the following:\n   Cluster Name Provider Pods CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east OnPrem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Support Matrix Submariner is designed to be cloud provider agnostic, and should run in any standard Kubernetes cluster. Submariner has been tested with and known to be working properly with the following cloud environments:\nAre you using Submariner in an environment which is not described here? Please let us know so that we can update this document.\n Cloud Providers  AWS VMware vSphere OpenStack GCP  Kubernetes CNI Plugins Presently, Submariner has been tested with the following CNI Plugins that leverage kube-proxy with iptables mode:\n openshift-sdn Weave Flannel Canal  Deployment The available methods for deployment are:\n subctl (+ submariner-operator). helm charts.  The community recommends the use of subctl, because it simplifies most of the manual steps required for deployment, as well as verification of connectivity between the clusters. In the future it may provide additional capabilities like:\n Detection of possible conflicts Upgrade management Status inspection of the deployment Configuration updates Maintenance and debugs tasks Wrapping of logs for support tasks.  To deploy submariner with subctl please follow the deployment guide. If helm fits better your deployment methodologies, please find the details here\n"
},
{
	"uri": "/quickstart/rancher/",
	"title": "Rancher 2.x",
	"tags": [],
	"description": "",
	"content": " Prerequisites These instructions were developed with Rancher v2.4.x\nMake sure you are familiar with Rancher, and creating clusters. You can create either node driver clusters or Custom clusters, as long as your designated gateway nodes can communicate with each other.\nCreate and Deploy Cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.42.0.0/16 10.43.0.0/16    Use the Rancher UI to create a cluster, leaving the default options selected.\nMake sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nCreate and Deploy Cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.44.0.0/16 10.45.0.0/16    Create your cluster, but select Edit as YAML in the cluster creation UI. Edit the services stanza to reflect the options below, while making sure to keep the options that were already defined.\nservices: kube-api: service_cluster_ip_range: 10.45.0.0/16 kube-controller: cluster_cidr: 10.44.0.0/16 service_cluster_ip_range: 10.45.0.0/16 kubelet: cluster_domain: cluster.local cluster_dns_server: 10.45.0.10 Make sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nOnce you have done this, you can deploy your cluster.\nInstall subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Obtain the kubeconfig files from the Rancher UI for each of your clusters, placing them in the respective kubeconfigs.\n   Cluster Kubeconfig File Name     Cluster A kubeconfig-cluster-a   Cluster B kubeconfig-cluster-b    Use cluster-a as broker subctl deploy-broker --kubeconfig kubeconfig-cluster-a Join cluster-a and cluster-b to the broker subctl join --kubeconfig kubeconfig-cluster-a broker-info.subm --clusterid cluster-a subctl join --kubeconfig kubeconfig-cluster-b broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster Pods and Services\nsubctl verify-connectivity kubeconfig-cluster-a kubeconfig-cluster-b --verbose "
},
{
	"uri": "/contributing/shipyard/first_time/",
	"title": "Adding Shipyard to a Project",
	"tags": [],
	"description": "",
	"content": "To use Shipyard in your project, it\u0026rsquo;s easiest to use Dapper and Make. To use Dapper you\u0026rsquo;ll need a specific Dockerfile that Dapper consumes to create a consistent environment based upon Shipyard\u0026rsquo;s base image. To use Make you\u0026rsquo;ll need some commands to enable Dapper and also include the targets which ship in the base image.\nDockerfile.dapper The project should have a Dockerfile.dapper Dockerfile which builds upon quay.io/submariner/shipyard-dapper-base.\nFor example:\nFROMquay.io/submariner/shipyard-dapper-baseENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT\u0026#34; \\  DAPPER_SOURCE=\u0026lt;your source directory\u0026gt; DAPPER_DOCKER_SOCKET=trueENV DAPPER_OUTPUT=${DAPPER_SOURCE}/outputWORKDIR${DAPPER_SOURCE}ENTRYPOINT [\u0026#34;./scripts/entry\u0026#34;]CMD [\u0026#34;ci\u0026#34;]You can also refer to the project\u0026rsquo;s own Dockerfile.dapper as an example.\nBuilding The Base Image To build the base container image used in the shared developer and CI enviroment, simply run:\nmake dapper-image Makefile The project\u0026rsquo;s Makefile should include targets to run everything in Dapper. These are defined in Shipyard\u0026rsquo;s Makefile.dapper which can be copied as is into your project and included in the Makefile. To use Shipyard\u0026rsquo;s built-in targets available in the base Dapper image, include the Makefile.inc file in the project\u0026rsquo;s Makefile within the section where the Dapper environment is detected.\nThe simplest Makefile would look like this:\nifneq (,$(DAPPER_HOST_ARCH)) # Running in Dapper  include $(SHIPYARD_DIR)/Makefile.inc else # Not running in Dapper  include Makefile.dapper endif # Disable rebuilding Makefile Makefile Makefile.dapper Makefile.inc: ; You can also refer to the project\u0026rsquo;s own Makefile as an example.\n"
},
{
	"uri": "/contributing/lighthouse/",
	"title": "Contributing to Lighthouse",
	"tags": [],
	"description": "",
	"content": "The Lighthouse Project contains the code for the service discovery component of Submariner. It mainly consists of an agent and a coreDNS plugin. Learn more about the architecture here.\nFork and Clone the Repository First log into your GitHub account and create a fork of the Lighthouse repository. Then clone your forked repository locally and switch to the lighthouse folder. You will need to create a branch in order to raise a PR.\ngit clone https://github.com/\u0026lt;your-github-username\u0026gt;/lighthouse.git cd lighthouse git checkout -b \u0026lt;BRANCH-NAME\u0026gt; Contributing to Lighthouse Agent The Lighthouse Agent code is located under the pkg/agent directory. The Controller is the main component that performs bi-directional syncing of Service and MultiClusterService resources. After making code changes, please refer to building and testing to build and verify your changes.\nRunning make e2e will create a 3-cluster deployment which you can use to manually verify changes if necessary. The Lighthouse Agent will be deployed in the submariner-operator namespace in each cluster. You can use kubectl commands to check the pod\u0026rsquo;s logs and status.\nContributing to Lighthouse Plugin The Lighthouse plugin code is located under the plugin/lighthouse directory and uses the structure as required by a standard CoreDNS plugin. The plugin answers DNS requests by using the MultiClusterService resources populated by the Lighthouse Agent. After making code changes, please refer to building and testing to build and verify your changes.\nSimilar to the agent, you can use the deployment created using make e2e to manually test and verify as well. The Lighthouse plugin will be running as a part of the CoreDNS in the kube-system\nnamespace and the CoreDNS config map will be changed to use it. The usual kubectl commands can be used here as well. You can add the debug entry in the CoreDNS config map if you require more logging.\nCreating a Pull Request You can commit your changes using git commit and push the changes to your fork branch. Then you will see an option to create a PR in the GitHub UI in your browser when you go to your forked repository.\ngit commit -s #Give a commit message git push -f origin HEAD:\u0026lt;Your-Branch-Name\u0026gt; Creating a PR will trigger CI to verify the changes.\n"
},
{
	"uri": "/contributing/shipyard/",
	"title": "Working With Shipyard",
	"tags": [],
	"description": "",
	"content": "Overview The Shipyard project provides common tooling for creating K8s clusters with KIND (K8s in Docker) and provides a common Go framework for creating end to end tests. Shipyard contains common functionality shared by other projects. Any project specific functionality should be part of that project.\nA base image quay.io/submariner/shipyard-dapper-base is created from Shipyard and contains all the tooling to build other projects and run tests in a consistent environment.\nShipyard has several folders at the root of the project:\n package: Contains the ingredients to build the base image. scripts: Contains general scripts for Shipyard make targets.  shared: Contains all the shared scripts that projects can consume. These are copied into the base image under $SCRIPTS_DIR.  lib: Library functions that shared scripts, or consuming projects, can use. resources: Resource files to be used by the shared scripts.     test: Test library to be used by other projects.  Shipyard ships with some Makefile targets which can be used by consuming projects and are used by Shipyard\u0026rsquo;s CI to test and validate itself. It also has some specific Makefile targets which are used by the project itself.\nUsage Add Shipyard to a Project To enable usage of Shipyard\u0026rsquo;s functionality, please see Adding Shipyard to a Project.\nUse Shipyard in Your Project Once Shipyard has been added to a project, you can use any of the Makefile targets that it provides.\nAny variables that you need to pass to these targets should be specified in your Dockerfile.dapper so they\u0026rsquo;re available in the Dapper environment. For example:\nENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT CLUSTERS_ARGS DEPLOY_ARGS\u0026#34;Have Shipyard Targets Depend on Your Project\u0026rsquo;s Targets Having any of the Shipyard Makefile targets rely on your project\u0026rsquo;s specific targets can be done easily by adding the dependency in your project\u0026rsquo;s Makefile. For example:\nclusters: build images Use an Updated Shipyard Image in Your Project If you\u0026rsquo;ve made changes to Shipyard\u0026rsquo;s base image and need to test them in your project, run:\nmake dapper-image in the Shipyard directory. This creates a local image with your changes available for consumption in other projects.\nShared Makefile Targets Shipyard ships a Makefile.inc file which defines these basic targets:\n clusters: Creates the KIND -based cluster environment. deploy : Deploys submariner components in the cluster environment (depends on clusters). cleanup: Deletes the KIND environment (if it exists) and any residual resources. release: Uploads the requested image(s) to Quay.io. vendor/modules.txt: Populates go modules (in case go.mod exists in the root directory).  If your project uses Shipyard then it has all these targets and supports all the variables these targets support.\nAny variables supported by these targets can be either declared as environment variables or assigned on the make command line (takes precedence over environment variables).\nClusters A Make target that creates a KIND-based multi-cluster environment with just the default K8s deployment:\nmake clusters Respected variables:\n CLUSTERS_ARGS: Any arguments (flags and/or values) to be sent to the clusters.sh script. To get a list of available arguments, run: scripts/shared/clusters.sh --help  Deploy A Make target that deploys Submariner components in a KIND-based cluster environment (if one isn\u0026rsquo;t created yet, this target will first invoke the clusters target to do so):\nmake deploy Respected variables:\n Any variable from clusters target (only if it wasn\u0026rsquo;t created). DEPLOY_ARGS: Any arguments (flags and/or values) to be sent to the deploy.sh script. To get a list of available arguments, run: scripts/shared/deploy.sh --help  Cleanup To clean up all the KIND clusters deployed in any of the previous steps, use:\nmake cleanup This command will remove the clusters and any resources that might\u0026rsquo;ve been left in docker that are not needed any more (images, volumes, etc).\nRelease Uploads the built images to Quay.io:\nmake release release_images=\u0026quot;\u0026lt;image name\u0026gt;\u0026quot; Respected variables:\n QUAY_USERNAME, QUAY_PASSWORD: Needed in order to log in to Quay. release_images: One or more image names to release separated by spaces. release_tag: A tag to use for the release (default is latest). repo: The Quay repo to use (default is quay.io/submariner).  Specific Makefile Targets Shipyard has some project-specific targets which are used to build parts of the projects:\n dapper-image: Builds the base image that can be used by other projects. validate: Validates the go code that Shipyard provides, and the shared shell scripts.  Dapper-Image Builds the basic image which is then used by other projects to build the code and run tests:\nmake dapper-image Respected variables:\n dapper_image_flags: Any additional flags and values to be sent to the build_image.sh script.  "
},
{
	"uri": "/contributing/building_testing/",
	"title": "Building and Testing",
	"tags": [],
	"description": "",
	"content": "Submariner strives to be an open, welcoming community for developers. Substantial tooling is provided to ease the contribution experience.\nStandard Development Environment Submariner provides a standard, shared development environment suitable for all local work. The same environment is used in CI.\nThe submariner-io/shipyard project contains the logic to build the base container images used across all submariner-io repositories.\nLearn more about working with Shipyard here.\nPrescribed Tasks via Make Targets Make targets are provided to further ease the process of using the shared development environment. The specific make targets available might differ by repository. For any submariner-io repository, see the Makefile at the root of the repository for the supported targets and the .travis.yml file for the targets actually used in CI.\nCommon Build and Testing Targets All submariner-io/* repositories provide a standard set of Make targets for similar building and testing actions.\nLinting To run static Go linting (goimports, golangci-lint):\nmake validate Unit tests To run Go unit tests:\nmake test Multi-Cluster KIND Based Environment Shipyard provides a basic target that creates a KIND based multi-cluster environment, without any special deployment (apart from the default K8s):\nmake clusters Find out more about Shipyard\u0026rsquo;s clusters target.\nMulti-Cluster Submariner Deployment Shipyard provides a basic target that deploys submariner on a KIND based multi-cluster environment (if one isn\u0026rsquo;t yet created, this target will first invoke the clusters target to do so): as well):\nmake deploy Find out more about Shipyard\u0026rsquo;s deploy target.\nEnd-to-End Tests To run functional end-to-end tests with a full multi-cluster deployment (if one isn\u0026rsquo;t yet deployed, this target will first invoke the deploy target to do so):\nmake e2e Optionally, you can specify arguments to control the execution of the end-to-end testing and deployment (if it wasn\u0026rsquo;t run separately). Currently some arguments are project-specific, while standard ones are supplied by Shipyard. Please consult the project\u0026rsquo;s Makefile to learn which arguments are supported. The arguments can be combined or used separately, or not at all (in which case default values apply).\nLearn more about controlling the deployments via Shipyard\u0026rsquo;s standard arguments.\nFor example, here\u0026rsquo;s a variation used in submariner-io/submariner CI to deploy with globalnet, using helm:\nmake e2e CLUSTERS_ARGS=\u0026#34;--globalnet\u0026#34; DEPLOY_ARGS=\u0026#34;--globalnet --deploytool helm\u0026#34; Environment Clean Up To clean up all the KIND clusters deployed in any of the previous steps, use:\nmake cleanup Learn more about Shipyard\u0026rsquo;s cleanup target here.\nsubmariner-io/submariner Building Engine, Routeagent, and Globalnet Go binaries To build the submariner-route-agent, submariner-engine, and submariner-globalnet Go binaries, in the submariner-io/submariner repository:\nmake build There is an optional flag to build with debug flags set:\nmake build build_debug=true Building Engine, Routeagent, and Globalnet container images To build the submariner/submariner, submariner/submariner-route-agent, and submariner/submariner-globalnet container images, in the submariner-io/submariner repository:\nmake images submariner-io/submariner-operator Building the Operator and subctl To build the submariner-operator container image and the subctl Go binary, in the submariner-io/submariner-operator repository:\nmake build submariner-io/lighthouse Building Lighthouse Controller, CoreDNS and DNSServer container images To build the lighthouse-agent and lighthouse-coredns container images, in the submariner-io/lighthouse repository:\nmake build-agent build-coredns submariner-io/shipyard Building dapper-base container image To build the base container image used in the shared developer and CI enviroment, in the submariner-io/shipyard:\nmake dapper-image "
},
{
	"uri": "/contributing/release-process/",
	"title": "Release Process",
	"tags": [],
	"description": "",
	"content": "This section explains the necessary steps to make a submariner release. It is assumed that you are familiar with the submariner project and the various repositories.\nStep 1: Create a Submariner Release Assuming that you have an existing submariner git directory, the following steps create a release named \u0026ldquo;Globalnet Overlapping IP support RC0\u0026rdquo; with version v0.2.0-rc0 based on the master branch.\ncd submariner git stash git remote add upstream ssh://git@github.com/submariner-io/submariner git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner?tab=tags\n Step 2: Create a Lighthouse Release To create lighthouse release artifacts follow the steps below.\nBuild Lighthouse Controller Assuming that you have an existing lighthouse git directory, run the following steps .\ncd lighthouse git stash git remote add upstream ssh://git@github.com/submariner-io/lighthouse git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here\n https://github.com/submariner-io/lighthouse/tags\n A build for v0.2.0-rc0 should start and appear under the \u0026ldquo;Active branches\u0026rdquo; section here\n https://travis-ci.com/github/submariner-io/lighthouse/branches\n For this example the build can be found here.\nVerify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available on quay.io at:\n https://quay.io/repository/submariner/lighthouse-controller?tab=tags https://quay.io/repository/submariner/lighthouse-coredns?tab=tags\n Build Lighthouse CoreDNS  Get the coredns repository and change to the folder  go get github.com/openshift/coredns cd $GOPATH/src/github.com/openshift/coredns Build the image and push  COREDNS_VERSION=\u0026lt;VERSION-NO\u0026gt; LH_COREDNS_VERSION=\u0026lt;LH-VERSION-NO\u0026gt; COREDNS_IMAGE=\u0026#34;lighthouse-coredns:${LH_COREDNS_VERSION}\u0026#34; git checkout -b ${COREDNS_VERSION} sed -i \u0026#39;/^kubernetes:kubernetes/a lighthouse:github.com/submariner-io/lighthouse/plugin/lighthouse\u0026#39; plugin.cfg sed -i \u0026#39;/^github.com/aws/aws-sdk-go/a github.com/submariner-io/lighthouse v0.2.0\u0026#39; go.mod sed -i \u0026#39;$a replace\\ k8s.io\\/apimachinery\\ =\u0026gt;\\ k8s.io\\/apimachinery\\ v0.0.0-20190313205120-d7deff9243b1\u0026#39; go.mod sed -i \u0026#39;$a replace\\ github.com\\/openzipkin-contrib\\/zipkin-go-opentracing\\ =\u0026gt;\\ github.com\\/openzipkin-contrib\\/zipkin-go-opentracing\\ v0.3.5\u0026#39; go.mod docker build -f Dockerfile.openshift -t quay.io/submariner/${COREDNS_IMAGE} . docker push quay.io/submariner/${COREDNS_IMAGE} Build Cluster-DNS-Operator  Clone the cluster-dns-operator repository and create a version branch  git clone https://github.com/submariner-io/cluster-dns-operator.git git checkout -b \u0026lt;VERSION-NO\u0026gt; Change the coredns and cluster dns operator versions.  \t- name: dns-operator terminationMessagePolicy: FallbackToLogsOnError image: quay.io/submariner/lighthouse-cluster-dns-operator:\u0026lt;VERSION-NO\u0026gt; command: - dns-operator env: - name: RELEASE_VERSION value: \u0026quot;0.0.1-snapshot\u0026quot; - name: IMAGE value: quay.io/submariner/lighthouse-coredns:\u0026lt;VERSION-NO\u0026gt; For example, https://github.com/submariner-io/cluster-dns-operator/pull/1/files#diff-8c85a5683e17f9599cbfc641cceaa040R33\nBuild a new image and upload it to quay.  REPO=quay.io/submariner/lighthouse-cluster-dns-operator:$VERSION make release-local Commit and push the changes and raise a PR.  git add . git commit -s git push HEAD:\u0026lt;VERSION-NO\u0026gt; Step 3: Update the Operator Version References and Create a Release Once the other builds have finished and you have 0.2.0-rc0 release tags for the submariner and lighthouse projects, you can proceed with changes to the operator.\nChange Referenced Versions Edit the operator versions file and change the project version constants to reference the new release, \u0026ldquo;0.2.0-rc0\u0026rdquo;.\ncd submariner-operator git stash git remote add upstream ssh://git@github.com/submariner-io/submariner-operator git fetch -a -v -t upstream git checkout remotes/upstream/master -B update-references-to-v0.2.0-rc0 vim pkg/versions/versions.go # make sure we are referencing the latest submariner code (we use it for verify-connectivity and the API) GO111MODULES=on go get github.com/submariner-io/submariner@v0.2.0-rc0 git push my-repo update-references-to-v0.2.0-rc0 Create a pull request, wait for the CI job to pass, and get approval/merge. See an example PR here\nCreate a Submariner-Operator Release Assuming you have an existing submariner-operator git directory, run the following steps:\ncd submariner-operator git stash git remote add upstream ssh://git@github.com/submariner-io/submariner-operator git fetch -a -v -t upstream git checkout remotes/upstream/master -B master git tag -s -m \u0026#34;Globalnet Overlapping IP support RC0\u0026#34; v0.2.0-rc0 git push upstream v0.2.0-rc0 A tagged release should appear here.\n https://github.com/submariner-io/submariner-operator/tags\n A build for v0.2.0-rc0 should start and appear under the under the \u0026ldquo;Active branches\u0026rdquo; section here.\n https://travis-ci.com/github/submariner-io/submariner-operator/branches\n Verify that the build successfully completes as indicated by a green checkmark at the right. At this point the images tagged with 0.2.0-rc0 will be available here.\n https://quay.io/repository/submariner/submariner-operator?tab=tags\n Verify the Subctl Binaries Release At this point, you should see subctl binaries generated and listed for the various platforms under the release https://github.com/submariner-io/submariner-operator/tags, find the tag for v0.2.0-rc0 , verify that the binaries uploaded, the process needs around 5 minutes.\nUpdate the release notes Go to https://github.com/submariner-io/submariner-operator/tags, find the tag for v0.2.0-rc0 and select \u0026ldquo;Edit release\u0026rdquo; to the right.\nUpdate the release notes.\nIf this is a pre-release, mark the checkbox \u0026ldquo;This is a pre-release\u0026rdquo;.\nIf this is not a pre-release update the release notes on the website.\nStep 4: Verify the Version You can follow any of our quickstarts, for example this one\nStep 5: Announce Via E-Mail  https://bit.ly/submariner-dev https://bit.ly/submariner-users  Via Twitter  https://twitter.com/submarinerio  "
},
{
	"uri": "/quickstart/openshift/",
	"title": "OpenShift (AWS)",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Use cluster-a as broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster pods and services\nsubctl verify-connectivity cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --verbose "
},
{
	"uri": "/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "The inner details of deploying a broker and connecting clusters to the broker are complicated, subctl automates and simplifies most of those details eliminating human error as much as possible. This is why subctl is the recommended deployment method, you can find a complete guide to the subctl tool here: subctl in detail. If you still believe helm works better for you, please go here.\nInstalling subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Deployment of broker Please remember, this cluster\u0026rsquo;s API should be accessible from all other clusters:\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; this will create:\n The submariner-k8s-broker namespace The Cluster and Endpoint (.submariner.io) CRDs in the cluster A service account in the namespace for subsequent subctl access.  And generate a broker-info.subm file which contains the following elements:\n The API endpoint A CA certificate to for the API endpoint The service account token for accessing the API endpoint / submariner-k8s-broker namespace. A random IPsec PSK which will be stored only in this file. Globalnet settings Service discovery settings  This cluster can also participate in the dataplane connectivity with the other clusters, but it will need to be joined (see following step)\n Joining clusters For each cluster you want to join:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm subctl will discover as much as it can, and ask you for any necessary detail it can\u0026rsquo;t figure out like the cluster ID which has to be different between all clusters.\nDiscovery Service discovery (via DNS and lighthouse project) is an experimental feature (developer preview), and the instructions to deploy with discovery can be found here\n"
},
{
	"uri": "/deployment/subctl/",
	"title": "subctl",
	"tags": [],
	"description": "",
	"content": "subctl is a command line utility designed to simplify the deployment and maintenance of Submariner across your clusters.\nSynopsis subctl [command] [--flags] ...\nDescription subctl helps to automate the deployment of the Submariner operator, thereby reducing the possibility of mistakes during the process.\nsubctl connects to specified cluster(s) and performs the requested command.\nCommands deploy-broker subctl deploy-broker [flags]\nThe deploy-broker command configures the cluster specified by the --kubeconfig flag (or KUBECONFIG env var) and the --kubecontext flag as the Broker. It installs the necessary CRDs and the submariner-k8s-broker namespace.\nIn addition, it generates a broker-info.subm file which can be used with the join command to connect clusters to the Broker. This file contains the following details:\n Encryption PSK key Broker access details for subsequent subctl runs. Service discovery settings Globalnet settings  deploy-broker flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default \u0026ldquo;$HOME/.kube/config\u0026rdquo;)   --kubecontext \u0026lt;string\u0026gt; kubeconfig context to use   --service-discovery Enable Multi Cluster Service Discovery   --globalnet Enable support for Overlapping cluster/service CIDRs in connecting clusters (default disabled)   --globalnet-cidr-range \u0026lt;string\u0026gt; Global CIDR supernet range for allocating GlobalCIDRs to each cluster (default \u0026ldquo;169.254.0.0/16\u0026rdquo;)   --ipsec-psk-from \u0026lt;string\u0026gt; Import IPSEC PSK from existing Submariner broker file, like broker-info.subm (default \u0026ldquo;broker-info.subm)   --dataplane Install the Submariner dataplane on the broker. If this flag is enabled, the broker will be joined as if a join command was run right after deploy-broker. Use the join flags too if you use \u0026ndash;dataplane.    join subctl join broker-info.subm [flags]\nThe join command deploys the Submariner operator in a cluster using the settings provided in the broker-info.subm file. The service account credentials needed for the new cluster to access the Broker cluster will be created and provided to the Submariner operator deployment. All the other settings like service discovery enablement and globalnet support are sourced from the broker-info file.\njoin flags (general)    Flag Description     --cable-driver \u0026lt;string\u0026gt; Cable driver implementation (defaults to strongswan -IPSec-)   --clusterid \u0026lt;string\u0026gt; Cluster ID used to identify the tunnels. Every cluster needs to have a unique cluster ID, if not provided subctl will prompt the admin for a cluster ID.   --clustercidr \u0026lt;string\u0026gt; Specifies the cluster\u0026rsquo;s CIDR used to generate Pod IP addresses. If not specified, subctl will try to discover it and, if unable to do so, it will prompt the user.   --no-label  Skip gateway labeling. This disables the prompt for a worker node to use as gateway.   --subm-debug Enable Submariner debugging (verbose logging)   --disable-cvo Disable OpenShift\u0026rsquo;s cluster version operator, if necessary, without prompting as a warning. Currently, Lighthouse modifies the cluster DNS operator in OpenShift to disable CVO.    join flags (globalnet)    Flag Description     --globalnet-cluster-size \u0026lt;value\u0026gt; Cluster size for GlobalCIDR allocated to this cluster (amount of global IPs).   --globalnet-cidr \u0026lt;string\u0026gt; GlobalCIDR to be allocated to the cluster, this setting is exclusive with --globalnet-cluster-size and configures a specific globalnet CIDR for this cluster.    join flags (IPSec)    Flag Description     --disable-nat Disable NAT for IPsec.   --ikeport \u0026lt;value\u0026gt; IPsec IKE port (default 500)   --ipsec-debug Enable IPsec debugging (verbose logging)   --nattport \u0026lt;value\u0026gt; IPsec NATT port (default 4500)    join flags (images and repositories)    Flag Description     --repository \u0026lt;string\u0026gt; The repository from where the various submariner images will be sourced. (default \u0026ldquo;quay.io/submariner\u0026rdquo;)   --version \u0026lt;string\u0026gt; Image version   -o, --operator-image \u0026lt;string\u0026gt; The operator image location (default \u0026ldquo;quay.io/submariner/submariner-operator:$version\u0026rdquo;)   --service-discovery-repo \u0026lt;string\u0026gt; Service Discovery image repository (default \u0026ldquo;quay.io/submariner\u0026rdquo;)   --service-discovery-version \u0026lt;string\u0026gt; Service Discovery image version    info subctl info [flags]\nThe info command inspects the cluster and reports information related to Submariner, like the detected network plugin, and the detected Cluster and Service CIDRs.\ninfo flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default \u0026ldquo;$HOME/.kube/config\u0026rdquo;)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use    verify-connectivity subctl verify-connectivity \u0026lt;kubeConfig1\u0026gt; \u0026lt;kubeConfig2\u0026gt; [flags]\nThe verify-connectivity command verifies dataplane connectivity between two clusters. The kubeConfig1 file will be ClusterA in the reports, while kubeConfig2 will be ClusterB in the reports. The --verbose flag is recommended to see what\u0026rsquo;s happening during the tests.\nDataplane connectivity is verified in multiple ways:\n Pods (on gateways) to Services Pods (on non-gateways) to Services Pods (on gateways) to Pods Pods (on non-gateways) to Pods and between gateway and non-gateway node combinations.  verify-connectivity flags    Flag Description     --connection-attempts \u0026lt;value\u0026gt; The maximum number of connection attempts (default 2)   --connection-timeout \u0026lt;value\u0026gt; The timeout in seconds per connection attempt (default 60)   --operation-timeout \u0026lt;value\u0026gt; Operation timeout for K8s API calls (default 240)   --report-dir \u0026lt;string\u0026gt; XML report directory (default \u0026ldquo;.\u0026quot;)   --verbose Produce verbose logs during connectivity verification    version subctl version\nPrints the version details for the subctl binary.\nInstallation Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    "
},
{
	"uri": "/quickstart/vsphere_aws/",
	"title": "OpenShift with Service Discovery (vSphere/AWS)",
	"tags": [],
	"description": "",
	"content": "In this quickstart guide, we shall cover the necessary steps to deploy OpenShift Container Platform (OCP) on vSphere and AWS. Once the OCP clusters are deployed, we shall cover how to deploy Submariner and connect the two clusters.\nOpenShift Prerequisites Before we proceed, the following prerequisites have to be downloaded and added to your $PATH:\n openshift-installer pull secret oc tools aws cli  Please ensure that the tools you downloaded above are compatible with the OpenShift Container Platform version.\n Deploy Cluster on vSphere (OnPrem) Create the necessary infrastructure on vSphere and ensure that your machines have direct internet access before starting the installation. To deploy OCP 4.4, follow the instructions shown here\nAssuming that you deployed the cluster (say, cluster-a) with default network configuration, the Pod and Service CIDRs would be\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    Submariner creates a VxLAN overlay network in the local cluster and uses port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes to preserve the source IP of the inter-cluster traffic. Ensure that firewall configuration on vSphere cluster allows 4800/UDP across all the worker nodes.\n   Protocol Port Description     UDP 4800 overlay network for inter-cluster traffic    Although we are using the default OCP network configuration on vSphere, you can install vSphere with a custom network configuration as shown here\n Deploy Cluster on AWS Configure AWS CLI with appropriate values $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text For more details, follow this link:\n https://docs.openshift.com/container-platform/4.4/installing/installing_aws/installing-aws-account.html\n In this step we shall modify the default Cluster/Service CIDRs and deploy cluster-b on AWS.\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the Pod IP network. Please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the Service IP network. This is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b Make your AWS cluster ready for Submariner Submariner gateway nodes need to be able to accept IPsec traffic. The default ports are 4500/UDP and 500/UDP. However, when you have some on-premise clusters (like vSphere in this example) which are typically inside a corporate network, the firewall configuration on the corporate router may not allow the default IPsec traffic. We can overcome this limitation by using non-standard ports like 4501/UDP and 501/UDP.\nAdditionally, the default OpenShift deployments do not allow assigning an elastic public IP to existing worker nodes, something that\u0026rsquo;s necessary at least on one end of the IPsec connections.\nTo handle these requirements on AWS, we provide a script that will prepare your AWS OpenShift deployment for Submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Currently, prep_for_subm.sh script does not support specifying custom IPsec ports. Until the issue is resolved, execute the following commands to open the necessary ports:\nexport BROKER_IKEPORT=501 export NAT_PORT=4501 sed \u0026#34;s/\\ 500/\\ $BROKER_IKEPORT/g\u0026#34; -i cluster-b/ocp-ipi-aws/ocp-ipi-aws-prep/ec2-resources.tf sed \u0026#34;s/\\ 4500/\\ $NAT_PORT/g\u0026#34; -i cluster-b/ocp-ipi-aws/ocp-ipi-aws-prep/ec2-resources.tf ./prep_for_subm.sh cluster-b # respond yes when terraform asks Submariner Installation Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery The Lighthouse project is meant only to be used as a development preview. Installing the operator on an OpenShift cluster will disable OpenShift CVO\n To install Submariner with multi-cluster service discovery, follow the steps below.\nUse cluster-b (AWS) as broker with service discovery enabled subctl deploy-broker --kubeconfig cluster-b/auth/kubeconfig --service-discovery Join cluster-b (AWS) and cluster-a (vSphere) to the broker subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b --ikeport 501 --nattport 4501 subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a --ikeport 501 --nattport 4501 Verify Deployment To verify the deployment follow the steps below:\nkubectl --kubeconfig cluster-b/auth/kubeconfig create deployment nginx --image=nginx kubectl --kubeconfig cluster-b/auth/kubeconfig expose deployment nginx --port=80 kubectl --kubeconfig cluster-a/auth/kubeconfig run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx Alternately, you can also run the Submariner test-suite that validates various use-cases:\nsubctl verify-connectivity cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig "
},
{
	"uri": "/quickstart/openshift/service_discovery/",
	"title": "With Service Discovery",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery follow the steps below.\nUse cluster-a as broker with service discovery enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --service-discovery Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b Verify Deployment To verify the deployment follow the steps below.\nexport KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 kubectl -n default apply -f - \u0026lt;\u0026lt;EOF apiVersion: lighthouse.submariner.io/v2alpha1 kind: ServiceExport metadata: name: nginx-demo EOF export KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.supercluster.local:8080 Perform automated verification This will perform all automated verification between your clusters\nsubctl verify cluster-a/auth/kubeconfig cluster-b/auth/kubeconfig --only service-discovery,connectivity --verbose "
},
{
	"uri": "/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "The diagram below illustrates the basic architecture of Submariner:\nSubmariner consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes clusters, both on-premise and on public clouds.\n Gateway Engine: manages the secure tunnels to other clusters. Route Agent: routes cross-cluster traffic from nodes to the active Gateway Engine. Broker: facilitates the exchange of metadata between Gateway Engines enabling them to discover one another.  Submariner has optional components that provide additional functionality.\n Globalnet Controller: handles overlapping CIDRs across clusters. Service Discovery: provides DNS discovery of services across clusters.  "
},
{
	"uri": "/contributing/shipyard/advanced/",
	"title": "Advanced Features",
	"tags": [],
	"description": "",
	"content": "Shipyard has many advanced features to use in consuming projects.\nTo utilize an advanced feature in a project consuming Shipyard, a good practice is to change the project\u0026rsquo;s Makefile to have the advanced logic that is always needed. Any variable functionality can then be passed as desired in the command line.\nImage Building Helper Script Shipyard ships an image building script build_image.sh which can be used to build the image(s) that you require. The script has built in caching capabilities to speed up local and pull request CI builds, by utilizing docker\u0026rsquo;s ability to reuse layers from a cached image.\nThe script accepts several flags:\n tag: The tag to set for the local image (defaults to dev). repo: The repo portion to use for the image name. image (-i): The image name itself. dockerfile (-f): The Dockerfile to build the image from. [no]cache: Turns the caching on (or off).  For example, to build the submariner image use:\n${SCRIPTS_DIR}/build_image.sh -i submariner -f package/Dockerfile Deployment Scripts Features Per Cluster Settings Shipyard supports specifying different settings for each deployed cluster. A default cluster_settings is supplied in the image, so any custom settings override those. The settings are sent to supporting scripts using a --cluster_settings flag.\nCurrently, the following settings are supported:\n clusters: An array of the clusters to deploy. clusters=(cluster1,cluster2)  cluster_nodes: A map of cluster names to a space separated string, representing a list of nodes to deploy. Supported values are control-plane and worker. cluster_nodes[cluster1]=\u0026#34;control-plane worker\u0026#34; cluster_nodes[cluster2]=\u0026#34;control-plane worker worker\u0026#34;  cluster_subm: A map of cluster names to values specifying if Submariner should be installed. Set to true to have Submariner installed, or to false to skip the installation. cluster_subm[cluster1]=\u0026#34;false\u0026#34; cluster_subm[cluster2]=\u0026#34;true\u0026#34;   Example: Custom Per Cluster Settings As an example, in order to customize the clusters to have two workers, and no submariner on the 1st cluster, create a cluster_settings file in the project:\ncluster_nodes[\u0026#39;cluster1\u0026#39;]=\u0026#34;control-plane\u0026#34; cluster_nodes[\u0026#39;cluster2\u0026#39;]=\u0026#34;control-plane worker worker\u0026#34; cluster_nodes[\u0026#39;cluster3\u0026#39;]=\u0026#34;control-plane worker worker\u0026#34; cluster_subm[\u0026#39;cluster1\u0026#39;]=\u0026#34;false\u0026#34; Then, to apply these settings, add this snippet to the Makefile:\nCLUSTER_SETTINGS_FLAG = --cluster_settings $(DAPPER_SOURCE)/path/to/cluster_settings CLUSTERS_ARGS += $(CLUSTER_SETTINGS_FLAG) DEPLOY_ARGS += $(CLUSTER_SETTINGS_FLAG) The path to cluster_settings should be specified relative to the project root; this ends up available in the build container in the directory referenced by $DAPPER_SOURCE.\nClusters Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make clusters via a make variable CLUSTERS_ARGS (or an environment variable with the same name). These flags affect how the clusters are deployed (and possibly influence how Submariner works).\nFlags of note:\n k8s_version: Allows to specify the K8s version that KIND will deploy. Available versions can be found here. make clusters CLUSTERS_ARGS=\u0026#39;--k8s_version 1.18.0\u0026#39;  globalnet: When set, deploys the clusters with overlapping Pod \u0026amp; Service CIDRs to simulate this scenario. make clusters CLUSTERS_ARGS=\u0026#39;--globalnet\u0026#39;   Submariner Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make deploy via a make variable DEPLOY_ARGS (or an environment variable with the same name). These flags affect how Submariner is deployed on the clusters.\nSince deploy relies on clusters then effectively you could also specify CLUSTERS_ARGS to control the cluster deployment (provided the cluster hasn\u0026rsquo;t been deployed yet).\nFlags of note:\n deploytool: Specifies the deployment tool to use: operator (default) or helm. make deploy DEPLOY_ARGS=\u0026#39;--deploytool operator\u0026#39;  deploytool_broker_args: Any extra arguments to pass to the deploy tool when deploying the broker. make deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_broker_args \u0026#34;--service-discovery\u0026#34;\u0026#39;  deploytool_submariner_args: Any extra arguments to pass to the deploy tool when deploying Submariner. make deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_submariner_args \u0026#34;--cable-driver wireguard\u0026#34;\u0026#39;  globalnet: When set, deploys Submariner with the globalnet controller, and assigns a unique Global CIDR to each cluster. make deploy DEPLOY_ARGS=\u0026#39;--globalnet\u0026#39;   Example: Passing Deployment Variables As an example, in order to deploy with Lighthouse and support both Operator and Helm deployments, one can add this snippet to the Makefile:\nifeq ($(deploytool),operator) DEPLOY_ARGS += --deploytool operator --deploytool_broker_args \u0026#39;--service-discovery\u0026#39; else DEPLOY_ARGS += --deploytool helm --deploytool_broker_args \u0026#39;--set submariner.serviceDiscovery=true\u0026#39; --deploytool_submariner_args \u0026#39;--set submariner.serviceDiscovery=true,lighthouse.image.repository=localhost:5000/lighthouse-agent,serviceAccounts.lighthouse.create=true\u0026#39; endif In such a case, the call to deploy the environment would look like this:\nmake deploy [deploytool=operator] Note that deploytool is a variable used to determine the tool to use, but isn\u0026rsquo;t passed to or used by Shipyard.\n"
},
{
	"uri": "/troubleshooting/",
	"title": "Troubleshooting Guide",
	"tags": [],
	"description": "",
	"content": "Overview You have followed steps in Deployment but something has gone wrong. You\u0026rsquo;re not sure what and how to fix it, or what information to collect to raise an issue. Welcome to the Submariner troubleshooting guide where we will help you get your deployment working again.\nBasic familiarity with the Submariner components and architecture will be helpful when troubleshooting so please review the Architecture section.\nThe guide has been broken into different sections for easy navigation.\nPre-requisite Before we begin troubleshooting, run subctl version to obtain which version of the Submariner components you are running.\nRun kubectl get services -n \u0026lt;service-namespace\u0026gt; | grep \u0026lt;service-name\u0026gt; to get information about the service you\u0026rsquo;re trying to access. This will provide you with the Service Name, Namespace and ServiceIP. If GlobalNet is enabled, you will also need the globalIp of the service by running\nkubectl get service \u0026lt;service-name\u0026gt; -o jsonpath='{.metadata.annotations.submariner\\.io/globalIp}'\nService Discovery Issues If you are able to connect to remote service by using ServiceIP or globalIp, but not by service name, it is a Service Discovery Issue.\nService Discovery not working This is good time to familiarize yourself with Service Discovery Architecture if you haven\u0026rsquo;t already.\nCheck CoreDNS Configuration Submariner configures CoreDNS deployment to enable lighthouse plugin. Since CoreDNS is the first point of entry for any DNS queries, that is where we will start first.\nFirst we check if we are running the CoreDNS image with the Lighthouse plugin enabled.\nkubectl -n kube-system kube-dns describe deployment\nFor Openshift use kubectl -n openshift-dns dns describe deployment. Some deployments may use different name and namespace for DNS service e.g. coredns, core-dns etc.\n Make sure that Image is set to quay.io/submariner/lighthouse-coredns:\u0026lt;version\u0026gt; and the version is correct. If not, change it to use the Lighthouse image.\nIf the image is correct, next we check if the Lighthouse plugin is enabled on the CoreDNS in the cluster making the query.\nkubectl describe configmap coredns\nIn the output look for something like this:\n kubernetes cluster2.local in-addr.arpa ip6.arpa { pods insecure upstream #fallthrough in-addr.arpa ip6.arpa fallthrough =====\u0026gt; This tells kubernetes to go to next plugin if query fails locally } lighthouse { ====\u0026gt; Lighthouse plugin will catch any queries that local kubernetes can't resolve fallthrough =====\u0026gt; proceed as usual if lighthouse can't resolve } If the entries highlighted above are missing, it means CoreDNS wasn\u0026rsquo;t configured to enable Lighthouse. It can be enabled by running kubectl edit configmap coredns and making the changes manually. You may need to repeat this step on every cluster.\nCheck submariner-lighthouse-agent Next we check if the submariner-lighthouse-agent is properly running. Run kubectl -n submariner-operator get pods submariner-lighthouse-agent and check the status of Pods.\nIf the status indicates the ImagePullBackOff error, run kubectl -n submariner-operator describe deployment submariner-lighthouse-agent and check if Image is set correctly to quay.io/submariner/lighthouse-agent:\u0026lt;version\u0026gt;. If it is and the same error still occurs, raise an issue here or ping us on the community slack channel.\nIf the status indicates any other error, run kubectl -n submariner-operator get pods to get the name of the lighthouse-agent Pod. Then run kubectl -n submariner-operator logs \u0026lt;lighthouse-agent-pod-name\u0026gt; to get the logs. See if there are any errors in the log. If yes, raise an issue with the log contents, or you can continue reading through this guide to troubleshoot further.\nIf there are no errors, grep the log for the service name that you\u0026rsquo;re trying to query as we may need the log entries later for raising an issue.\nCheck Multiclusterservice resources If the steps above did not indicate an issue, next we check if the Multiclusterservice resources were properly created for the service you\u0026rsquo;re trying to access. The format of a Multiclusterservice resources\u0026rsquo;s name is as follows:\n\u0026lt;service-name\u0026gt;-\u0026lt;service-namespace\u0026gt;-\u0026lt;cluster-id\u0026gt;\nRun kubectl get multiclusterservices --all-namespaces |grep \u0026lt;your-service-name\u0026gt; on the broker cluster to check if a resource was created for your service. If not, then check the Lighthouse Agent logs on the cluster where service was created and look for any error or warning messages indicating a failure to create the Multiclusterservice resource for your service. The most common error is Forbidden if the RBAC wasn\u0026rsquo;t configured correctly. Depending on the deployment method used, \u0026lsquo;subctl\u0026rsquo; or \u0026lsquo;helm\u0026rsquo;, it should\u0026rsquo;ve been done for you. Create an issue with relevant log entries.\nIf the Multiclusterservice resource was created correctly on the broker cluster, the next step is to check if it exists on the cluster where you\u0026rsquo;re trying to access the service. Follow the same steps as earlier to get the list of the Multiclusterservice resources and check if the Multiclusterservice for your service exists. If not, check the logs of the Lighthouse Agent on the cluster where you are trying to access the service. As described earlier, it will most commonly be an issue with RBAC otherwise create an issue with relevant log entries.\nIf the Multiclusterservice resource was created properly on the cluster, run kubectl -n submariner-operator describe multiclusterservice \u0026lt;your-multiclusterservice-name\u0026gt; and check if it has the correct ClusterID and ServiceIP:\nName: nginx-demo-default-cluster2 Namespace: submariner-operator Labels: submariner-io/clusterID=cluster2 Annotations: \u0026lt;none\u0026gt; API Version: lighthouse.submariner.io/v1 Kind: MultiClusterService Metadata: Creation Timestamp: 2020-04-16T15:40:36Z Generation: 1 Resource Version: 6847 Self Link: /apis/lighthouse.submariner.io/v1/namespaces/submariner-operator/multiclusterservices/nginx-demo-ns-default-src-cluster2 UID: 9db7759c-7ff8-11ea-85a3-0242ac110006 Spec: Cluster Service Info: Cluster Domain: Cluster ID: cluster2 ==========\u0026gt; ClusterID of cluster where service is running Service IP: 100.92.243.156 ==========\u0026gt; ServiceIP or GlobalIP of service you're trying to access Events: \u0026lt;none\u0026gt; If the data is not correct, you can manually edit the Multiclusterservice resource to set the correct IP as a workaround and create an issue with relevant information.\nIf it is correct, congratulations - you\u0026rsquo;ve found a new issue.\n"
},
{
	"uri": "/quickstart/openshift/globalnet/",
	"title": "With Service Discovery and Globalnet",
	"tags": [],
	"description": "",
	"content": " AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n Create cluster A This step will create a cluster named \u0026ldquo;cluster-a\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a This may take some time to complete so you can move on to the next section in parallel if you wish.\nCreate cluster B This step will create a cluster named \u0026ldquo;cluster-b\u0026rdquo; with the default IP CIDRs.\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-b And finally deploy\nopenshift-install create cluster --dir cluster-b  Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n Install subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Install Submariner with Service Discovery and Globalnet The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster will disable Openshift CVO.\n To install Submariner with multi-cluster service discovery and support for overlapping CIDRs follow the steps below.\nUse cluster-a as broker with service discovery and globalnet enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --service-discovery --globalnet Join cluster-a and cluster-b to the broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid west subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid east Verify Deployment To verify the deployment follow the steps below.\nkubectl --kubeconfig cluster-b/auth/kubeconfig create deployment nginx --image=nginx kubectl --kubeconfig cluster-b/auth/kubeconfig expose deployment nginx --port=80 kubectl --kubeconfig cluster-a/auth/kubeconfig run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash curl nginx "
},
{
	"uri": "/contributing/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": "Submariner Community Code of Conduct Submariner follows the CNCF Code of Conduct.\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting one or more of the Submariner Committers or Owners.\n"
},
{
	"uri": "/contributing/community-membership/",
	"title": "Community Membership",
	"tags": [],
	"description": "",
	"content": "This is a stripped-down version of the Kubernetes Community Membership process.\nAlthough we aspire to follow the K8s process, some parts are not currently relevant to our structure or possible with our tooling:\n The SIG and subproject abstraction layers don\u0026rsquo;t apply to Submariner. Submariner is treated as a single project with file-based commit rights, not a \u0026ldquo;project\u0026rdquo; per repository. We hope to eventually move to K8s OWNERS and Prow, but until we do so we can\u0026rsquo;t support advanced role-based automation (reviewers vs approvers; PR workflow commands like /okay-to-test, /lgtm, /approved).   This doc outlines the various responsibilities of contributor roles in Submariner.\n   Role Responsibilities Requirements Defined by     Member Active contributor in the community Sponsored by 2 committers, multiple contributions to the project Submariner GitHub org member   Committer Approve contributions from other members History of review and authorship CODEOWNERS file entry   Owner Set direction and priorities for the project Demonstrated responsibility and excellent technical judgement for the project Submariner-owners GitHub team member    New Contributors New contributors should be welcomed to the community by existing members, helped with PR workflow, and directed to relevant documentation and communication channels.\nEstablished Community Members Established community members are expected to demonstrate their adherence to the principles in this document, familiarity with project organization, roles, policies, procedures, conventions, etc., and technical and/or writing ability. Role-specific expectations, responsibilities, and requirements are enumerated below.\nMember Members are continuously active contributors in the community. They can have issues and PRs assigned to them and participate through GitHub teams. Members are expected to remain active contributors to the community.\nDefined by: Member of the Submariner GitHub organization.\nRequirements  Enabled two-factor authentication on their GitHub account Have made multiple contributions to the project or community. Contribution may include, but is not limited to:  Authoring or reviewing PRs on GitHub Filing or commenting on issues on GitHub Contributing to community discussions (e.g. meetings, Slack, email discussion forums, Stack Overflow)   Subscribed to submariner-dev@googlegroups.com Have read the contributor guide Actively contributing Sponsored by 2 committers. Note the following requirements for sponsors:  Sponsors must have close interactions with the prospective member - e.g. code/design/proposal review, coordinating on issues, etc. Sponsors must be committers in at least 1 CODEOWNERS file either in any repo in the Submariner org   Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the member template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Responsibilities and Privileges  Responsive to issues and PRs assigned to them Responsive to mentions of teams they are members of Active owner of code they have contributed (unless ownership is explicitly transferred)  Code is well tested Tests consistently pass Addresses bugs or issues discovered after code is accepted   They can be assigned to issues and PRs, and people can ask members for reviews  Note: Members who frequently contribute code are expected to proactively perform code reviews and work towards becoming a committer.\nCommitters Committers are able to review code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles.\nUntil automation supports approvers vs reviewers: They also review for holistic acceptance of a contribution including: backwards / forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, etc.\nDefined by: Entry in an CODEOWNERS file in a repo owned by the Submariner project.\nCommitter status is scoped to a part of the codebase.\nRequirements The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Member for at least 3 months Primary reviewer for at least 5 PRs to the codebase Reviewed at least 20 substantial PRs to the codebase Knowledgeable about the codebase Sponsored by two committers or project owners  With no objections from other committers or project owners   May either self-nominate or be nominated by a committer/owner Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the committer template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers/owners reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Responsibilities and Privileges The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Responsible for project quality control via code reviews  Focus on code quality and correctness, including testing and factoring Until automation supports approvers vs reviewers: Focus on holistic acceptance of contribution such as dependencies with other features, backwards / forwards compatibility, API and flag definitions, etc   Expected to be responsive to review requests as per community expectations Assigned PRs to review related to project of expertise Assigned test bugs related to project of expertise Granted \u0026ldquo;read access\u0026rdquo; to submariner repo May get a badge on PR and issue comments Demonstrate sound technical judgement Mentor contributors and reviewers  Project Owner Project owners are the technical authority for the Submariner project. They MUST have demonstrated both good judgement and responsibility towards the health the project. Project owners MUST set technical direction and make or approve design decisions for the project - either directly or through delegation of these responsibilities.\nDefined by: Member of the submariner-owners GitHub team.\nRequirements Unlike the roles outlined above, the owners of the project are typically limited to a relatively small group of decision makers and updated as fits the needs of the project.\nThe following apply to people who would be an owner:\n Deep understanding of the technical goals and direction of the project Deep understanding of the technical domain of the project Sustained contributions to design and direction by doing all of:  Authoring and reviewing proposals Initiating, contributing and resolving discussions (emails, GitHub issues, meetings) Identifying subtle or complex issues in designs and implementation PRs   Directly contributed to the project through implementation and / or review  Responsibilities and Privileges The following apply to people who would be an owner:\n Make and approve technical design decisions for the project Set technical direction and priorities for the project Define milestones and releases Mentor and guide committers and contributors to the project Ensure continued health of project  Adequate test coverage to confidently release Tests are passing reliably (i.e. not flaky) and are fixed when they fail   Ensure a healthy process for discussion and decision making is in place Work with other project owners to maintain the project\u0026rsquo;s overall health and success holistically  "
},
{
	"uri": "/deployment/helm/",
	"title": "Using Helm",
	"tags": [],
	"description": "",
	"content": "This is a placeholder, in the meanwhile you can check out https://github.com/submariner-io/submariner-charts\n"
},
{
	"uri": "/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.3.0 Lighthouse Service Discovery without KubeFed  This release is focused on removing the KubeFed dependency from Lighthouse, improving the user experience and adding experimental WireGuard support as an alternative to IPsec\n  Lighthouse no longer depends KubeFed. All metadata exchange is handled over the Broker as MultiClusterService CRs. Experimental Wireguard support has been added as a pluggable CableDriver option in addition to the current default IPsec. Submariner reports the active and passive gateways as a gateway.submariner.io resource. The Submariner Operator reports a detailed status of the deployment. The gateway redundancy/failover tests are now enabled and stable in CI. Globalnet hostNetwork to remote globalIP is now supported. Previously, when a pod used hostNetworking it was unable to connect to a remote service via globalIP. A globalCIDR can be manually specified when joining a cluster with globalnet enabled. This enables CI speed optimizations via better parallelism. Operator and subctl are more robust via standard retries on updates. Subctl creates a new individual access token for every new joined cluster.  v0.2.0 Overlapping CIDR support  This release is focused on overlapping CIDR support between clusters\n  Support for Overlapping CIDRs between clusters (globalnet) Enhanced e2e scripts, which will be shared between repositories in the shipyard project (ongoing work) Improved e2e deployment by using a local registry. Refactoring to support pluggable drivers (in preparation for WireGuard support)  v0.1.1 Submariner with more light  This release has focused on stability for the Lighthouse support\n  Cleaner logging for the submariner-engine Cleaner logging for the submariner-route-agent Fixed issue with wrong token stored in subm file #244 Added flag to disable the OpenShift CVO #235 Fixed several service-discovery related bugs #194 , #167 Fixed several panics on nil network discovery Added checks to ensure the CIDRs for joining cluster don\u0026rsquo;t overlap with an existing ones. Fix context handling related to service-discovery / kubefed #180 Use the right CoreDNS image for OpenShift.  v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making https://github.com/submariner.io/lighthouse available as developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (#346, #348, #332) Migrated to Daemonsets for submariner gateway deployment Added support for hostNetwork to remote pod/service connectivity (#288) Auto detection and configuration of MTU for vx-submariner, jumbo frames support (#301) Support for updated strongswan (#288) Better iptables detection for some hosts (#227)   subctl and the submariner operator have the following improvements\n  support to verify-connectivity between two connected clusters deployment of submariner gateways based in daemonsets instead of deployments renaming submariner pods to \u0026ldquo;submariner-gateway\u0026rdquo; pods for clarity print version details on crash (subctl) stop storing IPSEC key on broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file version command for subctl nicer spinners during deployment (thanks to kind)  v0.0.3 \u0026ndash; KubeCon NA 2019  Submariner has been greatly enhanced to allow operators to deploy into Kubernetes clusters without the necessity for layer-2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). Subctl was created to make deployment of submariner easier.\n v0.0.1 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/reading_material/",
	"title": "Online Resources",
	"tags": [],
	"description": "",
	"content": "There are multiple presentations/demo recordings on Submariner available online.\nConference Presentations  KubeCon China 2019 slides and video recording KubeCon North America 2019 slides and video recording  Demo Recordings  Submariner in 60s Deploying Submariner with subctl Connecting hybrid Kubernetes clusters using Submariner Cross-cluster service discovery in Submariner using Lighthouse  Blogs  Kubernetes Multi-Cloud and Multi-Cluster Connectivity with Submariner  If you find additional material that isn\u0026rsquo;t listed here, please feel free to add it to this page by editing it. The full contributing guide can be found here.\n"
},
{
	"uri": "/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "This is a preliminary community roadmap, it\u0026rsquo;s not written in stone, but it can serve as a guideline about what\u0026rsquo;s ahead.\nPlease see details of previous releases here\nv0.4.0  In planning https://github.com/orgs/submariner-io/projects/7  Future Releases  Libreswan support Auto detecting NAT vs non-NAT scenarios. Supporting different ports for IPSEC for each cluster Measuring and improving A/P HA (different scenarios) Support for network policies via coastguard Monitoring and reporting of tunnel endpoints (status of connection, bandwidth, pps, etc..) Monitoring connectivity over port 4800 between routeagent nodes. Support for non-kubeproxy / iptables based implementations, starting with OVN HA Active/Active gateway support (ECMP?) (keep in mind non-iptables-kubeproxy based implementations) Testing with Istio  "
},
{
	"uri": "/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": "Talk to Us! We would love to hear from you, how you are using Submariner, and what we can do to make it better.\nGitHub Project Check out the project and consider contributing. Pick up an issue to work on or propose an enhancement by reporting a new issue; once your code is ready to be reviewed, you can propose a pull request. You can find a good guide about the GitHub workflow here.\n#submariner Share your ideas in the #submariner channel in Kubernetes\u0026rsquo; Slack. If you need it, you can request an invite to K8S slack instance.\nCommunity Calendar As a member of the Submariner Community, join any of our community meetings - no registration required. The weekly Submariner Community Meeting (Tuesdays at 5:00pm CET) is a good place to start.\nMailing List Join the developer mailing list.\n"
},
{
	"uri": "/quickstart/kind/",
	"title": "KIND (Local Environment)",
	"tags": [],
	"description": "",
	"content": "Locally Testing With KIND KIND is a tool to run local Kubernetes clusters inside Docker container nodes.\nSubmariner provides (via Shipyard) scripts that deploy 3 Kubernetes clusters locally - 1 broker and 2 data clusters with the Submariner dataplane components deployed on all the clusters.\nDocker must be installed and running on your computer.\n Deploying manually If you wish to try out Submariner deployment manually, you can first create KIND clusters using our scripts and subctl.\nCreate KIND clusters To create KIND clusters, run:\ngit clone https://github.com/submariner-io/submariner cd submariner make clusters This creates 3 Kubernetes clusters, cluster1, cluster2 and cluster3.\nInstall subctl Download the subctl binary and make it available on your PATH. mkdir -p ~/.local/bin curl -L -o ~/.local/bin/subctl https://github.com/submariner-io/submariner-operator/releases/download/v0.3.0/subctl-v0.3.0-linux-amd64 chmod a+x ~/.local/bin/subctl export PATH=$PATH:~/.local/bin echo export PATH=$PATH:~/.local/bin  ~/.profile    Use cluster1 as broker subctl deploy-broker --kubeconfig output/kubeconfigs/kind-config-cluster1 --service-discovery Join cluster2 and cluster3 to the broker subctl join --kubeconfig output/kubeconfigs/kind-config-cluster2 broker-info.subm --clusterid cluster2 --disable-nat subctl join --kubeconfig output/kubeconfigs/kind-config-cluster3 broker-info.subm --clusterid cluster3 --disable-nat You now have a Submariner environment that you can experiment with.\nVerify Deployment To manually verify the deployment follow the steps below.\nkubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 create deployment nginx --image=nginx kubectl --kubeconfig output/kubeconfigs/kind-config-cluster3 expose deployment nginx --port=80 kubectl --kubeconfig output/kubeconfigs/kind-config-cluster2 run --generator=run-pod/v1 tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx Alternately, you can also run the Submariner test-suite that validates various use-cases: subctl verify-connectivity\nsubctl verify-connectivity --verbose output/kubeconfigs/kind-config-cluster2 output/kubeconfigs/kind-config-cluster3 "
},
{
	"uri": "/",
	"title": "Submariner",
	"tags": [],
	"description": "",
	"content": "Submariner Submariner enables direct networking between pods and services in different Kubernetes clusters, either on premise or in the cloud. Why Submariner? As Kubernetes gains adoption, teams are finding they must deploy and manage multiple clusters to facilitate features like geo-redundancy, scale, and fault isolation for their applications. With Submariner, your applications and services can span multiple cloud providers, data centers, and regions.\nSubmariner is completely open source, and designed to be network plugin (CNI) agnostic.\nWhat Does Submariner Provide?  Cross-cluster L3 connectivity using encrypted VPN tunnels Service Discovery across clusters subctl, a friendly deployment tool Support for interconnecting clusters with overlapping CIDRs   .mygrid { display: grid; grid-gap: 12px; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); grid-template-rows: repeat(2, 100px); padding-bottom: 5em; } .mygrid h5 { text-align: center; }  Demos       Connecting Pods and Services across Clusters        Deploying Submariner with subctl        Cross-cluster Service Discovery   Check the Quickstart guide section for deployment instructions.\n "
},
{
	"uri": "/deployment/with-discovery/",
	"title": "With Discovery (experimental)",
	"tags": [],
	"description": "",
	"content": "Deployment with discovery will include the (lighthouse components.\nProject status: The Lighthouse project is meant only to be used as a development preview. Installing the operator on an Openshift cluster may disable some of the operator features.\n The Lighthouse project helps in cross-cluster service discovery. It has the below additional dependencies\n kubectl installed.  Deploying Submariner with Lighthouse Deploy Broker subctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; --service-discovery Join Clusters To join all the other clusters with the broker cluster, run subctl using the broker-info.subm generated in the folder from which the previous step was run.\nsubctl join --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-DATA-CLUSTER\u0026gt; broker-info.subm As for a normal deployment, subctl will try to figure out all necessary information and will ask for anything it can\u0026rsquo;t figure out.\n"
},
{
	"uri": "/quickstart/openshift/create_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Create and deploy cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a The create cluster step will take some time, you can create Cluster B in parallel if you wish.\nCreate and deploy cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the POD IP network, please note it’s a /14 range by default so you need to use +4 increments for “128”, for example: 10.132.0.0, 10.136.0.0, 10.140.0.0, \u0026hellip;\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the service IP network, this is a /16 range by default, so just use +1 increments for “30”: for example: 172.31.0.0, 172.32.0.0, 172.33.0.0, \u0026hellip;\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy\nopenshift-install create cluster --dir cluster-b "
},
{
	"uri": "/quickstart/openshift/ready_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Make your clusters ready for submariner Submariner gateway nodes need to be able to accept traffic over ports 4500/UDP and 500/UDP when using IPSEC. In addition we use port 4800/UDP to encapsulate traffic from the worker nodes to the gateway nodes and ensuring that Pod IP addresses are preserved.\nAdditionally, the default Openshift deployments don\u0026rsquo;t allow assigning an elastic public IP to existing worker nodes, something that it\u0026rsquo;s necessary at least on one end of the IPSEC connections.\nTo handle all those details we provide a script that will prepare your AWS OpenShift deployment for submariner, and will create an additional gateway node with an external IP.\ncurl https://raw.githubusercontent.com/submariner-io/submariner/master/tools/openshift/ocp-ipi-aws/prep_for_subm.sh -L -O chmod a+x ./prep_for_subm.sh ./prep_for_subm.sh cluster-a # respond yes when terraform asks ./prep_for_subm.sh cluster-b # respond yes when terraform asks  INFO Please note that oc, aws-cli, terraform, and unzip need to be installed before running the prep_for_subm.sh script.\n "
},
{
	"uri": "/quickstart/openshift/setup_openshift/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "AWS openshift-install and pull-secret Download the openshift-install and oc tools, and copy your pull secret from:\n https://cloud.redhat.com/openshift/install/aws/installer-provisioned\n Find more detailed instructions here:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-default.html\n Make sure the aws cli is properly installed and configured Installation instructions\n https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n $ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text See also for more details:\n https://docs.openshift.com/container-platform/4.3/installing/installing_aws/installing-aws-account.html\n "
},
{
	"uri": "/quickstart/rancher/create_clusters/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Create and Deploy Cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.42.0.0/16 10.43.0.0/16    Use the Rancher UI to create a cluster, leaving the default options selected.\nMake sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nCreate and Deploy Cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.44.0.0/16 10.45.0.0/16    Create your cluster, but select Edit as YAML in the cluster creation UI. Edit the services stanza to reflect the options below, while making sure to keep the options that were already defined.\nservices: kube-api: service_cluster_ip_range: 10.45.0.0/16 kube-controller: cluster_cidr: 10.44.0.0/16 service_cluster_ip_range: 10.45.0.0/16 kubelet: cluster_domain: cluster.local cluster_dns_server: 10.45.0.10 Make sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nOnce you have done this, you can deploy your cluster.\n"
},
{
	"uri": "/quickstart/rancher/prerequisites/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Prerequisites These instructions were developed with Rancher v2.4.x\nMake sure you are familiar with Rancher, and creating clusters. You can create either node driver clusters or Custom clusters, as long as your designated gateway nodes can communicate with each other.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]