[
{
	"uri": "/development/backports/",
	"title": "Backports",
	"tags": [],
	"description": "",
	"content": "Fixes for serious issues or regressions affecting previous releases may be backported to the corresponding branches, to be included in the next release from that branch.\nRequesting a backport Backports can only be requested on fixes made against a later branch, or the devel branch. (This doesn’t mean that bugs can’t be fixed in older branches directly; but where relevant, they should first be fixed on devel.)\nTo request such a backport, identify the relevant pull request, and add the “backport” label to it. You should also add a comment to the pull request explaining why the backport is necessary, and which branch(es) are targeted. Issues should not be labeled, they are liable to be overlooked or lack a one-to-one mapping to a code fix.\nHandling backports Pending backports can be identified using this query, listing all non-archived pull requests with a “backport” label and without a “backport-handled” label.\nBackports should only be handled once the reference pull request is merged. This ensures that commit identifiers will remain stable during the backport process and for later history.\nStandalone pull requests Backporting a pull request (PR) is automated by running:\nmake LOCAL_BUILD=1 backport release=\u0026lt;release-branch\u0026gt; pr=\u0026lt;PR to cherry-pick\u0026gt;\nThe make target runs a script, backport.sh, originally developed by the Kubernetes community.\nThe script does the following:\n Cherry-picks the commits from the PR onto \u0026lt;remote branch\u0026gt;. Creates a PR on \u0026lt;release-branch\u0026gt; with the title Automated backport of \u0026lt;original PR number\u0026gt;: \u0026lt;original PR title\u0026gt;. Adds the backport-handled label to the original PR and the automated-backport label to the backported PR.  The DRY_RUN environment variable can be set to skip creating the PR. When set, it leaves you in a branch containing the commits that were cherry-picked.\nMultiple PRs can be backported together by passing a comma-separated list of PR numbers, eg pr=630,631.\nThe script uses the following environment variables. Please change them according to your setup.\n UPSTREAM_REMOTE: the remote for the upstream repository. Defaults to origin. FORK_REMOTE: the remote for your forked repository. Defaults to GITHUB_USER. GITHUB_USER: needs to be set to your GitHub username.  Pull requests requiring dependent backports Reviewing backports Backports need to go through the same review process as usual. The author of the original pull request should be added as a reviewer.\nChange requests on a backport should only concern changes arising from the specifics of backporting to the target release branch. Any other change which is deemed useful as a result of the review probably also applies to the original pull request and should result in an entirely new pull request, which might not be a backport candidate.\n"
},
{
	"uri": "/development/website/style-guide/",
	"title": "Docs Style Guide",
	"tags": [],
	"description": "",
	"content": "Documentation Style Guide This guide is meant to help keep our documentation consistent and ease the contribution and review process.\nSubmariner follows the Kubernetes Documentation Style Guide wherever relevant. This is a Submariner-specific extension of those practices.\nSubmariner.io Word List A list of Submariner-specific terms and words to be used consistently across the site.\n   Term Usage     Admiral The project name Admiral should always be capitalized.   Broker The design pattern component Broker should always be capitalized.   ClusterSet The Kubernetes object ClusterSet proposed in KEP1645 should always be CamelCase and formatted in code style.   Cluster set The words \u0026ldquo;cluster set\u0026rdquo; should be used as a term for a group of clusters, but not the proposed Kubernetes object.   Coastguard The project name Coastguard should always be capitalized.   Globalnet The feature name Globalnet is one word, and so should always be capitalized and should have a lowercase \u0026ldquo;n\u0026rdquo;.   iptables The application iptables consistently uses all-lowercase. Follow their convention, but avoid starting a sentence with \u0026ldquo;iptables\u0026rdquo;.   K8s The project nickname K8s should typically be expanded to \u0026ldquo;Kubernetes\u0026rdquo;.   kind The tool kind consistently uses all-lowercase. Follow their convention, but avoid starting a sentence with \u0026ldquo;kind\u0026rdquo;.   Lighthouse The project name Lighthouse should always be capitalized.   Operator The design pattern Operator should always be capitalized.   Shipyard The project name Shipyard should always be capitalized.   subctl The artifact subctl should not be capitalized and should be formatted in code style.   Submariner The project name Submariner should always be capitalized.    Pronunciation of \u0026ldquo;Submariner\u0026rdquo; Both the \u0026ldquo;Sub-mariner\u0026rdquo; (\u0026ldquo;Sub-MARE-en-er\u0026rdquo;, like the watch) and \u0026ldquo;Submarine-er\u0026rdquo; (\u0026ldquo;Sub-muh-REEN-er\u0026rdquo;, like the Navy job) pronunciations are okay.\nThe second option, \u0026ldquo;Submarine-er\u0026rdquo;, has historically been more common as Chris Kim (the initial creator) imagined the iconography of the project as related to submarine cables.\n"
},
{
	"uri": "/getting-started/quickstart/managed-kubernetes/gke/",
	"title": "Google (GKE)",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers deploying two Google Kubernetes Engine (GKE) clusters on Google Cloud Platform (GCP) and connecting them with Submariner and Service Discovery.\nThe guide assumes clusters have non-overlapping Pod and Service CIDRs. Globalnet can be used if overlapping CIDRs can\u0026rsquo;t be avoided.\n The guide assumes you have the gcloud binary installed and configured and a GCP account with billing enabled for the active project.\n Cluster Creation Create two identical Kubernetes clusters on GKE. For this guide, the following minimal configuration was used, however not everything is required (see the note part below).\ngcloud container clusters create \u0026#34;cluster-a\u0026#34; \\  --zone \u0026#34;europe-west3-a\u0026#34; \\  --enable-ip-alias \\  --cluster-ipv4-cidr \u0026#34;10.0.0.0/14\u0026#34; \\  --services-ipv4-cidr=\u0026#34;10.4.0.0/20\u0026#34; \\  --cluster-version \u0026#34;1.17.13-gke.2001\u0026#34; \\  --username \u0026#34;admin\u0026#34; \\  --machine-type \u0026#34;g1-small\u0026#34; \\  --image-type \u0026#34;UBUNTU\u0026#34; \\  --disk-type \u0026#34;pd-ssd\u0026#34; \\  --disk-size \u0026#34;15\u0026#34; \\  --num-nodes \u0026#34;3\u0026#34; \\  --network \u0026#34;default\u0026#34; gcloud container clusters create \u0026#34;cluster-b\u0026#34; \\  --zone \u0026#34;europe-west3-a\u0026#34; \\  --enable-ip-alias \\  --cluster-ipv4-cidr \u0026#34;10.8.0.0/14\u0026#34; \\  --services-ipv4-cidr=\u0026#34;10.12.0.0/20\u0026#34; \\  --cluster-version \u0026#34;1.17.13-gke.2001\u0026#34; \\  --username \u0026#34;admin\u0026#34; \\  --machine-type \u0026#34;g1-small\u0026#34; \\  --image-type \u0026#34;UBUNTU\u0026#34; \\  --disk-type \u0026#34;pd-ssd\u0026#34; \\  --disk-size \u0026#34;15\u0026#34; \\  --num-nodes \u0026#34;3\u0026#34; \\  --network \u0026#34;default\u0026#34;  Make sure to use Kubernetes version 1.17 or higher, set by --cluster-version. The latest versions are listed in the GKE release notes.\n Prepare Clusters for Submariner The clusters need some changes in order for Submariner to successfully open the IPsec tunnel between them.\nPreparation: Node Configuration As of version 0.8 of Submariner (the current one while writing this), Google\u0026rsquo;s native CNI plugin is not directly supported. GKE clusters can be generated with Calico CNI instead, but this was not tested during this demo and therefore could hold surprises as well.\nSo as this guide uses Google\u0026rsquo;s native CNI plugin, configuration is needed for the eth0 interface of each node on every cluster. The used workaround deploys netshoot pods onto each node that configure the reverse path filtering. The scripts in this Github repository need to be executed in all clusters.\nwget https://raw.githubusercontent.com/sridhargaddam/k8sscripts/main/rp_filter_settings/update-rp-filter.sh wget https://raw.githubusercontent.com/sridhargaddam/k8sscripts/main/rp_filter_settings/configure-rp-filter.sh chmod +x update-rp-filter.sh chmod +x configure-rp-filter.sh gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; ./configure-rp-filter.sh gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; ./configure-rp-filter.sh Preparation: Firewall Configuration Submariner requires UDP ports 500, 4500, and 4800 to be open in both directions. Additionally the microservices\u0026rsquo; traffic needs to flow through the IPsec tunnel as TCP packets. Hence the TCP traffic has source and destination addresses originating in the participating clusters. Create those firewall rules on the GCP project. Use the same IP ranges as in the cluster creation steps above.\ngcloud compute firewall-rules create \u0026#34;allow-tcp-in\u0026#34; --allow=tcp \\  --direction=IN --source-ranges=10.12.0.0/20,10.8.0.0/14,10.4.0.0/20,10.0.0.0/14 gcloud compute firewall-rules create \u0026#34;allow-tcp-out\u0026#34; --allow=tcp --direction=OUT \\  --destination-ranges=10.12.0.0/20,10.8.0.0/14,10.4.0.0/20,10.0.0.0/14 gcloud compute firewall-rules create \u0026#34;udp-in-500\u0026#34; --allow=udp:500 --direction=IN gcloud compute firewall-rules create \u0026#34;udp-in-4500\u0026#34; --allow=udp:4500 --direction=IN gcloud compute firewall-rules create \u0026#34;udp-in-4800\u0026#34; --allow=udp:4800 --direction=IN gcloud compute firewall-rules create \u0026#34;udp-out-500\u0026#34; --allow=udp:500 --direction=OUT gcloud compute firewall-rules create \u0026#34;udp-out-4500\u0026#34; --allow=udp:4500 --direction=OUT gcloud compute firewall-rules create \u0026#34;udp-out-4800\u0026#34; --allow=udp:4800 --direction=OUT After this, the clusters are finally ready for Submariner!\nDeploy Submariner Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nDeploy the Broker on cluster-a.\ngcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; subctl deploy-broker The command will output a file named broker-info.subm to the directory it is run from, which will be used to setup the IPsec tunnel between clusters.\nVerify the Broker components are installed:\n$ kubectl get crds | grep submariner clusters.submariner.io endpoints.submariner.io gateways.submariner.io serviceimports.lighthouse.submariner.io kubectl get crds --context cluster-a | grep multicluster serviceexports.multicluster.x-k8s.io serviceimports.multicluster.x-k8s.io $ kubectl get ns | grep submariner submariner-k8s-broker Now it is time to register every cluster in the future ClusterSet to the Broker.\nFirst join the Broker-hosting cluster itself to the Broker:\ngcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; subctl join broker-info.subm --clusterid cluster-a --servicecidr 10.4.0.0/20 Submariner will figure out most required information on its own. The --clusterid and --servicecidr flags should be used to pass the same values as during the cluster creation steps above. You will also see a dialogue on the terminal that asks you to decide which of the three nodes will be the Gateway. Any node will work. It will be annotated with submariner.io/gateway: true.\nWhen a cluster is joined, the Submariner Operator is installed. It creates several components in the submariner-operator namespace:\n submariner-gateway DaemonSet, to open a gateway for the IPsec tunnel on one node submariner-routeagent DaemonSet, which runs on every worker node in order to route the internal traffic to the local gateway via VXLAN tunnels submariner-lighthouse-agent Deployment, which accesses the Kubernetes API server in the Broker cluster to exchange Service information with the Broker submariner-lighthouse-coredns Deployment, which - as an external DNS server - gets forwarded requests to the *.clusterset.local domain for cross-cluster communication by Kubernetes\u0026rsquo; internal DNS server  Check the DaemonSets and Deployments with the following command:\n$ gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; $ kubectl get ds,deploy -n submariner-operator NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/submariner-gateway 1 1 1 1 1 submariner.io/gateway=true 5m29s daemonset.apps/submariner-routeagent 3 3 3 3 3 \u0026lt;none\u0026gt; 5m27s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/submariner-lighthouse-agent 1/1 1 1 5m28s deployment.apps/submariner-lighthouse-coredns 2/2 2 2 5m27s deployment.apps/submariner-operator 1/1 1 1 5m43s Now join the second cluster to the Broker:\ngcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; subctl join broker-info.subm --clusterid cluster-b --servicecidr 10.12.0.0/20 Then verify connectivity and CIDR settings within the ClusterSet:\n$ gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; $ subctl show all CLUSTER ID ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster-a 10.156.0.8 34.107.75.239 libreswan local cluster-b 10.156.0.13 35.242.247.43 libreswan remote GATEWAY CLUSTER REMOTE IP CABLE DRIVER SUBNETS STATUS gke-cluster-b-default-pool-e2e7 cluster-b 10.156.0.13 libreswan 10.12.0.0/20, 10.8.0.0/14 connected NODE HA STATUS SUMMARY gke-cluster-a-default-pool-4e5f active All connections (1) are established COMPONENT REPOSITORY VERSION submariner quay.io/submariner 0.8.0-rc0 submariner-operator quay.io/submariner 0.8.0-rc0 service-discovery quay.io/submariner 0.8.0-rc0 $ gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; $ subctl show all CLUSTER ID ENDPOINT IP PUBLIC IP CABLE DRIVER TYPE cluster-b 10.156.0.13 35.242.247.43 libreswan local cluster-a 10.156.0.8 34.107.75.239 libreswan remote GATEWAY CLUSTER REMOTE IP CABLE DRIVER SUBNETS STATUS gke-cluster-a-default-pool-4e5f cluster-a 10.156.0.8 libreswan 10.4.0.0/20, 10.0.0.0/14 connected NODE HA STATUS SUMMARY gke-cluster-b-default-pool-e2e7 active All connections (1) are established COMPONENT REPOSITORY VERSION submariner quay.io/submariner 0.8.0-rc0 submariner-operator quay.io/submariner 0.8.0-rc0 service-discovery quay.io/submariner 0.8.0-rc0 Workaround for KubeDNS GKE uses KubeDNS by default for cluster-internal DNS queries. Submariner however only works with CoreDNS as of version 0.7. As a consequence, the *.clusterset.local domain stub needs to be added manually to KubeDNS. Query the ClusterIP of the submariner-lighthouse-coredns Service in cluster-a and cluster-b:\n$ gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; $ CLUSTER_IP=$(kubectl get svc submariner-lighthouse-coredns -n submariner-operator -o=custom-columns=ClusterIP:.spec.clusterIP | tail -n +2) $ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap data: stubDomains: | {\u0026#34;clusterset.local\u0026#34;:[\u0026#34;$CLUSTER_IP\u0026#34;]} metadata: labels: addonmanager.kubernetes.io/mode: EnsureExists name: kube-dns namespace: kube-system EOF $ gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; $ CLUSTER_IP=$(kubectl get svc submariner-lighthouse-coredns -n submariner-operator -o=custom-columns=ClusterIP:.spec.clusterIP | tail -n +2) $ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ConfigMap data: stubDomains: | {\u0026#34;clusterset.local\u0026#34;:[\u0026#34;$CLUSTER_IP\u0026#34;]} metadata: labels: addonmanager.kubernetes.io/mode: EnsureExists name: kube-dns namespace: kube-system EOF Automated Verification This will perform automated verifications between the clusters.\nKUBECONFIG=cluster-a.yml gcloud container clusters get-credentials cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; KUBECONFIG=cluster-b.yml gcloud container clusters get-credentials cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; KUBECONFIG=cluster-a.yml:cluster-b.yml subctl verify --kubecontexts cluster-a,cluster-b --only service-discovery,connectivity --verbose Reconfig after Node Restart If the GKE Nodes were at some point drained or deleted, the Submariner Pods needed to terminate. Once the Nodes are up again, remember to\n label one Node with kubectl label node \u0026lt;name\u0026gt; submariner.io/gateway=true in order for the Gateway to be deployed on this Node apply the Node Configuration workaround again change the applied KubeDNS workaround to reflect the current submariner-lighthouse-coredns IP.  This makes Submariner functional again and work can be continued.\nClean Up When you\u0026rsquo;re done, delete your clusters:\ngcloud container clusters delete cluster-a --zone=\u0026#34;europe-west3-a\u0026#34; gcloud container clusters delete cluster-b --zone=\u0026#34;europe-west3-a\u0026#34; "
},
{
	"uri": "/development/shipyard/first-time/",
	"title": "Adding Shipyard to a Project",
	"tags": [],
	"description": "",
	"content": "To use Shipyard in your project, it\u0026rsquo;s easiest to use Dapper and Make. To use Dapper you\u0026rsquo;ll need a specific Dockerfile that Dapper consumes to create a consistent environment based upon Shipyard\u0026rsquo;s base image. To use Make you\u0026rsquo;ll need some commands to enable Dapper and also include the targets which ship in the base image.\nDockerfile.dapper The project should have a Dockerfile.dapper Dockerfile which builds upon quay.io/submariner/shipyard-dapper-base.\nFor example:\nFROMquay.io/submariner/shipyard-dapper-baseENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT\u0026#34; \\  DAPPER_SOURCE=\u0026lt;your source directory\u0026gt; DAPPER_DOCKER_SOCKET=trueENV DAPPER_OUTPUT=${DAPPER_SOURCE}/outputWORKDIR${DAPPER_SOURCE}ENTRYPOINT [\u0026#34;./scripts/entry\u0026#34;]CMD [\u0026#34;ci\u0026#34;]You can also refer to the project\u0026rsquo;s own Dockerfile.dapper as an example.\nBuilding The Base Image To build the base container image used in the shared developer and CI enviroment, simply run:\nmake dapper-image Makefile The project\u0026rsquo;s Makefile should include targets to run everything in Dapper. These are defined in Shipyard\u0026rsquo;s Makefile.dapper which can be copied as is into your project and included in the Makefile. To use Shipyard\u0026rsquo;s built-in targets available in the base Dapper image, include the Makefile.inc file in the project\u0026rsquo;s Makefile within the section where the Dapper environment is detected.\nThe simplest Makefile would look like this:\nifneq (,$(DAPPER_HOST_ARCH)) # Running in Dapper  include $(SHIPYARD_DIR)/Makefile.inc else # Not running in Dapper  include Makefile.dapper endif # Disable rebuilding Makefile Makefile Makefile.dapper Makefile.inc: ; You can also refer to the project\u0026rsquo;s own Makefile as an example.\n"
},
{
	"uri": "/development/building-testing/",
	"title": "Building and Testing",
	"tags": [],
	"description": "",
	"content": "Submariner strives to be an open, welcoming community. Substantial tooling is provided to ease the contribution experience.\nStandard Development Environment Submariner provides a standard, shared environment for both development and CI that is maintained in the Shipyard project.\nLearn more about working with Shipyard here.\nBuilding and Testing Submariner provides a set of Make targets for building and testing in the standard development environment.\nLinting To run all linting:\nmake lint There are also Make targets for each type of linting:\nmake gitlint golangci-lint markdownlint yamllint See the linter configuration files at the root of each repository for details about which checks are enabled.\nNote that a few linters only run in CI via GitHub Actions and are not available in the standard development environment.\nUnit Tests To run Go unit tests:\nmake unit Building To build the Go binaries provided by a repository:\nmake build To package those Go binaries into container images:\nmake images Note that Submariner will automatically rebuild binaries and images when they have been modified and are required by tests.\nTo prune all Submariner-provided images, ensuring they will be rebuilt or pulled the next time they’re required:\nmake prune-images End-to-End Tests To run functional end-to-end tests with a full multi-cluster deployment:\nmake e2e Different types of deployments can be configured with using flags:\nmake e2e using=helm,globalnet The cable driver used to connect clusters can also be selected with using flags:\nmake e2e using=vxlan In order to deploy clusters with OVN Kubernetes, the following command can be used:\nmake e2e using=ovn See Shipyard\u0026rsquo;s Makefile.inc for the currently-supported using flags.\nA subset of tests can be selected with Ginkgo focus flags:\nmake e2e focus=dataplane To create a multi-cluster deployment and install Submariner but not run tests:\nmake deploy To create a multi-cluster deployment without Submariner:\nmake clusters To clean up a multi-cluster deployment from one of the previous commands:\nmake clean-clusters Shell Session in Development Environment To jump into a shell in Submariner\u0026rsquo;s standard development environment:\nmake shell "
},
{
	"uri": "/getting-started/quickstart/kind/",
	"title": "Sandbox Environment (kind)",
	"tags": [],
	"description": "",
	"content": "Deploy kind with Submariner Locally kind is a tool for running local Kubernetes clusters using Docker container nodes. This guide uses kind to demonstrate deployment and operation of Submariner in three Kubernetes clusters running locally on your computer.\nSubmariner provides automation to deploy clusters using kind and connect them using Submariner.\nPrerequisites  Install Docker and ensure it is running properly on your computer. Install and set up kubectl.  Deploy Automatically To create kind clusters and deploy Submariner with service discovery enabled, run:\ngit clone https://github.com/submariner-io/submariner-operator cd submariner-operator make deploy using=lighthouse By default, the automation configuration in the submariner-io/submariner-operator repository deploys two clusters, with cluster1 configured as the Broker. See the settings file for details.\nOnce you become familiar with Submariner\u0026rsquo;s basics, you may want to visit the Building and Testing page to learn more about customizing your Submariner development deployment. To understand how Submariner\u0026rsquo;s development deployment infrastructure works under the hood, see Submariner Deployment Customization in the Shipyard Advanced Options.\nDeploy Manually If you wish to try out Submariner deployment manually, an easy option is to create kind clusters using our scripts and deploy Submariner with subctl.\nCreate kind Clusters To create kind clusters, run:\ngit clone https://github.com/submariner-io/submariner-operator cd submariner-operator make clusters Once the clusters are deployed, make clusters will indicate how to access them:\nYour virtual cluster(s) are deployed and working properly and can be accessed with: export KUBECONFIG=$(find $(git rev-parse --show-toplevel)/output/kubeconfigs/ -type f -printf %p:) $ kubectl config use-context cluster1 # or cluster2, cluster3.. To clean everthing up, just run: make clean-clusters The export KUBECONFIG command has to be run before kubectl can be used.\nmake clusters creates two Kubernetes clusters: cluster1 and cluster2. To see the list of kind clusters, use the following command:\n$ kind get clusters cluster1 cluster2 To list the local Kubernetes contexts, use the following command:\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE cluster1 cluster1 cluster1 * cluster2 cluster2 cluster2 Since multiple clusters are running, you need to choose which cluster kubectl talks to. You can set a default cluster for kubectl by setting the current context in the Kubernetes kubeconfig file. Additionally, you can run the following command to set the current context for kubectl:\nkubectl config use-context \u0026lt;cluster name\u0026gt;  For more information on interacting with kind, please refer to the kind documentation.\n Install subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nUse cluster1 as Broker subctl deploy-broker --kubeconfig output/kubeconfigs/kind-config-cluster1 Join cluster1 and cluster2 to the Broker subctl join --kubeconfig output/kubeconfigs/kind-config-cluster1 broker-info.subm --clusterid cluster1 --natt=false subctl join --kubeconfig output/kubeconfigs/kind-config-cluster2 broker-info.subm --clusterid cluster2 --natt=false You now have a Submariner environment that you can experiment with.\nVerify Deployment Verify Automatically with subctl This will perform automated verifications between the clusters.\nexport KUBECONFIG=output/kubeconfigs/kind-config-cluster1:output/kubeconfigs/kind-config-cluster2 subctl verify --kubecontexts cluster1,cluster2 --only service-discovery,connectivity --verbose Verify Manually To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster2.\nDeploy ClusterIP Service kubectl --kubeconfig output/kubeconfigs/kind-config-cluster2 create deployment nginx --image=nginx kubectl --kubeconfig output/kubeconfigs/kind-config-cluster2 expose deployment nginx --port=80 subctl export service --kubeconfig output/kubeconfigs/kind-config-cluster2 --namespace default nginx Deploy Headless Service kubectl --kubeconfig output/kubeconfigs/kind-config-cluster2 create deployment nginx --image=nginx kubectl --kubeconfig output/kubeconfigs/kind-config-cluster2 expose deployment nginx --port=80 --cluster-ip=None subctl export service --kubeconfig output/kubeconfigs/kind-config-cluster2 --namespace default nginx Verify Run nettest from cluster1 to access the nginx service:\nkubectl --kubeconfig output/kubeconfigs/kind-config-cluster1 -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest \\ -- /bin/bash curl nginx.default.svc.clusterset.local Cleanup When you are done experimenting and you want to delete the clusters deployed in any of the previous steps, use the following command:\nmake clean-clusters "
},
{
	"uri": "/community/code-of-conduct/",
	"title": "Code of Conduct",
	"tags": [],
	"description": "",
	"content": "Submariner Community Code of Conduct Submariner follows the CNCF Code of Conduct.\nPlease report instances of abusive, harassing, or otherwise unacceptable behavior by contacting one or more of the Submariner Project Owners.\n"
},
{
	"uri": "/operations/deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "Submariner provides a Go-based Kubernetes custom controller, called an Operator, that provides easy API-based installation and management. A command line utility, subctl, wraps the Operator to aid users with manual deployments and easy experimentation. subctl greatly simplifies the deployment of Submariner, and is therefore the recommended deployment method. For complete information about subctl, please refer to this page.\nIn addition to Operator and subctl, Submariner also provides Helm Charts.\nInstalling subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nDeployment of the Broker The Broker is a set of Custom Resource Definitions (CRDs) backed by the Kubernetes datastore. The Broker must be deployed on a cluster whose Kubernetes API is accessible by all of the participating clusters.\nsubctl deploy-broker --kubeconfig \u0026lt;PATH-TO-KUBECONFIG-BROKER\u0026gt; This will create:\n The submariner-k8s-broker namespace. The Endpoint and Cluster CRDs in the cluster. A Service Account (SA) in the namespace for subsequent subctl access.  It also generates the broker-info.subm file which contains the following elements:\n The API endpoint. A CA certificate for the API endpoint. The Service Account token for accessing the API endpoint. A random IPsec PSK which will be stored only in this file. Service Discovery settings.  The cluster in which the Broker is deployed can also participate in the dataplane connectivity with other clusters, but it will need to be joined (see following step).\n You can customize the Broker namespace using the --broker-namespace flag, allowing you to use a namespace of your choice on the Broker for synchronising resources between clusters.\nsubctl deploy-broker --broker-namespace \u0026lt;CUSTOM-NAMESPACE\u0026gt; ... Reference the subctl deploy-broker flag docs for additional details.\nJoining clusters For each cluster you want to join, issue the following command:\nsubctl join --kubeconfig \u0026lt;PATH-TO-JOINING-CLUSTER\u0026gt; broker-info.subm --clusterid \u0026lt;ID\u0026gt; subctl will automatically discover as much as it can, and prompt the user for any missing necessary information. Note that each cluster must have a unique cluster ID; the cluster ID can be specified, or otherwise is going to be generated by default based on the cluster name in the kubeconfig file.\n"
},
{
	"uri": "/getting-started/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Submariner connects multiple Kubernetes clusters in a way that is secure and performant. Submariner flattens the networks between the connected clusters, and enables IP reachability between Pods and Services. Submariner also provides, via Lighthouse, service discovery capabilities. The service discovery model is built using the proposed Kubernetes Multi Cluster Services.\nSubmariner consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes clusters, both on-premises and on public clouds:\n Gateway Engine: manages the secure tunnels to other clusters. Route Agent: routes cross-cluster traffic from nodes to the active Gateway Engine. Broker: facilitates the exchange of metadata between Gateway Engines enabling them to discover one another. Service Discovery: provides DNS discovery of Services across clusters.  Submariner has optional components that provide additional functionality:\n Globalnet Controller: handles interconnection of clusters with overlapping CIDRs.  The diagram below illustrates the basic architecture of Submariner:\nTerminology and Concepts   ClusterSet - a group of two or more clusters with a high degree of mutual trust that share Services amongst themselves. Within a cluster set, all namespaces with a given name are considered to be the same namespace.\n  ServiceExport (CRD) - used to specify which Services should be exposed across all clusters in the cluster set. If multiple clusters export a Service with the same name and from the same namespace, they will be recognized as a single logical Service.\n  ServiceExports must be explicitly created by the user in each cluster and within the namespace in which the underlying Service resides, in order to signify that the Service should be visible and discoverable to other clusters in the cluster set. The ServiceExport object can be created manually or via the subctl export command.\n  When a Service is exported, it then becomes accessible as \u0026lt;service\u0026gt;.\u0026lt;ns\u0026gt;.svc.clusterset.local.\n    ServiceImport (CRD) - representation of a multi-cluster Service in each cluster. Created and used internally by Lighthouse and does not require any user action.\n  "
},
{
	"uri": "/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner consists of several main components that work in conjunction to securely connect workloads across multiple Kubernetes clusters. For more information about Submariner\u0026rsquo;s architecture, please refer to the Architecture section.\nThe Broker The Broker is an API to which all participating clusters are given access to, and where two objects are exchanged via CRDs:\n Cluster(.submariner.io): defines a participating cluster and its IP CIDRs. Endpoint(.submariner.io): defines a connection endpoint to a cluster, and the reachable cluster IPs from the endpoint.  The Broker must be deployed on a single Kubernetes cluster. This cluster’s API server must be reachable by all Kubernetes clusters connected by Submariner. It can be a dedicated cluster, or one of the connected clusters.\nThe Submariner Deployment on a Cluster Once Submariner is deployed on a cluster with the proper credentials to the Broker it will exchange Cluster and Endpoint objects with other clusters (via push/pull/watching), and start forming connections and routes to other clusters.\nPrerequisites Submariner has a few requirements to get started:\n At least two Kubernetes clusters, one of which is designated to serve as the central Broker that is accessible by all of your connected clusters; this can be one of your connected clusters, or a dedicated cluster. Minimum supported Kubernetes version is 1.17. Non-overlapping Pod and Service CIDRs between clusters. This is to prevent routing conflicts. For cases where addresses do overlap, Globalnet can be set up. IP reachability between the gateway nodes. When connecting two clusters, the gateways must have at least one-way connectivity to each other on their public or private IP address and encapsulation port. This is needed for creating the tunnels between the clusters. The default encapsulation port is 4500/UDP, for NAT Traversal discovery port 4490/UDP is used. For clusters behind corporate firewalls that block the default ports, Submariner also supports NAT Traversal (NAT-T) with the option to set custom non-standard ports like 4501/UDP. Submariner uses UDP port 4800 to encapsulate Pod traffic from worker and master nodes to the Gateway nodes. This is required in order to preserve the source IP addresses of the Pods. Ensure that firewall configuration allows 4800/UDP across all nodes in the cluster in both directions. Submariner uses TCP port 8080 to export metrics on the Gateway nodes. Ensure that firewall configuration allows ingress 8080/TCP on the Gateway nodes so that other nodes in the cluster can access it. Also, no other workload on the Gateway nodes should be listening on TCP port 8080. Worker node IPs on all connected clusters must be outside of the Pod/Service CIDR ranges.  An example of three clusters configured to use with Submariner (without Globalnet) would look like the following:\n   Cluster Name Provider Pod CIDR Service CIDR Cluster Nodes CIDR     broker AWS 10.42.0.0/16 10.43.0.0/16 192.168.1.0/24   west vSphere 10.0.0.0/16 10.1.0.0/16 192.168.1.0/24   east On-Prem 10.98.0.0/16 10.99.0.0/16 192.168.1.0/24    Support Matrix Submariner is designed to be cloud provider agnostic, and should run in any standard Kubernetes cluster. Submariner has been tested with the following network (CNI) Plugins:\n OpenShift-SDN Weave Flannel Canal Calico (see the Calico-specific deployment instructions) OVN  Submariner supports all currently-supported Kubernetes versions, as determined by the Kubernetes release policy.\nDeployment Submariner is deployed and managed using its Operator. Submariner\u0026rsquo;s Operator can be deployed using subctl, Helm, or directly.\nsubctl is the recommended deployment method because it has the most refined deployment user experience and additionally provides testing and bug-diagnosing capabilities.\n"
},
{
	"uri": "/getting-started/architecture/broker/",
	"title": "Broker",
	"tags": [],
	"description": "",
	"content": "Submariner uses a central Broker component to facilitate the exchange of metadata information between Gateway Engines deployed in participating clusters. The Broker is basically a set of Custom Resource Definitions (CRDs) backed by the Kubernetes datastore. The Broker also defines a ServiceAccount and RBAC components to enable other Submariner components to securely access the Broker\u0026rsquo;s API. There are no Pods or Services deployed with the Broker.\nSubmariner defines two CRDs that are exchanged via the Broker: Endpoint and Cluster. The Endpoint CRD contains the information about the active Gateway Engine in a cluster, such as its IP, needed for clusters to connect to one another. The Cluster CRD contains static information about the originating cluster, such as its Service and Pod CIDRs.\nThe Broker is a singleton component that is deployed on a cluster whose Kubernetes API must be accessible by all of the participating clusters. If there is a mix of on-premises and public clusters, the Broker can be deployed on a public cluster. The Broker cluster may be one of the participating clusters or a standalone cluster without the other Submariner components deployed. The Gateway Engine components deployed in each participating cluster are configured with the information to securely connect to the Broker cluster\u0026rsquo;s API.\nThe availability of the Broker cluster does not affect the operation of the dataplane on the participating clusters, that is the dataplane will continue to route traffic using the last known information while the Broker is unavailable. However, during this time, control plane components will be unable to advertise new or updated information to other clusters and learn about new or updated information from other clusters. When connection is re-established to the Broker, each component will automatically re-synchronize its local information with the Broker and update the dataplane if necessary.\n"
},
{
	"uri": "/operations/deployment/subctl/",
	"title": "subctl",
	"tags": [],
	"description": "",
	"content": "The subctl command-line utility simplifies the deployment and maintenance of Submariner by automating interactions with the Submariner Operator.\nSynopsis subctl [command] [--flags] ...\nInstallation Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nInstalling specific versions By default, https://get.submariner.io will provide the latest release for subctl, and hence Submariner. Specific versions can be requested by using the VERSION environment variable.\nAvalailable options are:\n latest: the latest stable release (default) devel: the devel branch code. rc: the latest release candidate. x.x.x (like 0.6.1, 0.5.0, etc)  For example\ncurl https://get.submariner.io | VERSION=devel bash Commands deploy-broker subctl deploy-broker [flags]\nThe deploy-broker command configures the cluster specified by the --kubeconfig flag (or KUBECONFIG env var) and the --kubecontext flag as the Broker. It installs the necessary CRDs and the submariner-k8s-broker namespace.\nIn addition, it generates a broker-info.subm file which can be used with the join command to connect clusters to the Broker. This file contains the following details:\n Encryption PSK key Broker access details for subsequent subctl runs Service Discovery settings  deploy-broker flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default $HOME/.kube/config)   --kubecontext \u0026lt;string\u0026gt; kubeconfig context to use   --repository \u0026lt;string\u0026gt; The repository from where the various Submariner images will be sourced (default \u0026ldquo;quay.io/submariner\u0026rdquo;)   --version \u0026lt;string\u0026gt; Image version   --components \u0026lt;strings\u0026gt; Comma-separated list of components to be installed - any of service-discovery,connectivity. The default is: service-discovery,connectivity   --globalnet Enable support for overlapping Cluster/Service CIDRs in connecting clusters (default disabled)   --globalnet-cidr-range \u0026lt;string\u0026gt; Global CIDR supernet range for allocating GlobalCIDRs to each cluster (default \u0026ldquo;242.0.0.0/8\u0026rdquo;)   --globalnet-cluster-size \u0026lt;value\u0026gt; Default cluster size for GlobalCIDR allocated to each cluster (amount of global IPs) (default 65536)   --ipsec-psk-from \u0026lt;string\u0026gt; Import IPsec PSK from existing Submariner broker file, like broker-info.subm (default broker-info.subm)   --broker-namespace \u0026lt;string\u0026gt; Namespace on the Broker used for synchronizing resources between clusters (default submariner-k8s-broker)    export export service subctl export service [flags] \u0026lt;name\u0026gt; creates a ServiceExport resource for the given Service name. This makes the corresponding Service discoverable from other clusters in the Submariner deployment.\nexport service flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default $HOME/.kube/config)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use   --namespace \u0026lt;string\u0026gt; Namespace in which the Service to be exported belongs    If no namespace flag is specified, it uses the default namespace from the current context, if present, otherwise it uses default.\njoin subctl join broker-info.subm [flags]\nThe join command deploys the Submariner Operator in a cluster using the settings provided in the broker-info.subm file. The service account credentials needed for the new cluster to access the Broker cluster will be created and provided to the Submariner Operator deployment.\njoin flags (general)    Flag Description     --cable-driver \u0026lt;string\u0026gt; Cable driver implementation. Available options are libreswan (default), wireguard and vxlan   --clusterid \u0026lt;string\u0026gt; Cluster ID used to identify the tunnels. Every cluster needs to have a unique cluster ID. If not provided, one will be generated by default based on the cluster name in the kubeconfig file; if the cluster name is not a valid cluster ID, the user will be prompted for one   --clustercidr \u0026lt;string\u0026gt; Specifies the cluster\u0026rsquo;s CIDR used to generate Pod IP addresses. If not specified, subctl will try to discover it and if unable to do so, it will prompt the user   --label-gateway Label getways (enabled by default). --label-gateway=false disables the prompt for a Worker node to use as gateway   --pod-debug Enable Submariner pod debugging (verbose logging in the deployed pods)   --load-balancer Enable a cloud loadbalancer in front of the gateways. This removes the need for dedicated nodes with a public IP address   --preferred-server Enable this cluster as a preferred IPsec server for dataplane connections (only available with libreswan cable driver)    join flags (Globalnet)    Flag Description     --globalnet Enable/disable Globalnet for this cluster (default true). This has no effect if Globalnet is not enabled globally via the Broker   --globalnet-cluster-size \u0026lt;value\u0026gt; If Globalnet is enabled, the cluster size for the GlobalCIDR allocated to this cluster (amount of global IPs)   --globalnet-cidr \u0026lt;string\u0026gt; If Globalnet is enabled, the specific Globalnet CIDR to use for this cluster. This setting is exclusive with --globalnet-cluster-size    join flags (IPsec)    Flag Description     --natt Enable NAT for IPsec (default enabled)   --ikeport \u0026lt;value\u0026gt; IPsec IKE port (default 500)   --ipsec-debug Enable IPsec debugging (verbose logging)   --nattport \u0026lt;value\u0026gt; IPsec NAT-T port (default 4500)    join flags (images and repositories)    Flag Description     --repository \u0026lt;string\u0026gt; The repository from where the various Submariner images will be sourced (default \u0026ldquo;quay.io/submariner\u0026rdquo;)   --version \u0026lt;string\u0026gt; Image version   --image-override \u0026lt;string\u0026gt;=\u0026lt;string\u0026gt; Component image override. This flag can be used more than once (example: \u0026ndash;image-override=submariner=quay.io/myUser/submariner:latest)    join flags (health check)    Flag Description     --health-check Enable/disable Gateway health check (default true)   --health-check-interval \u0026lt;uint\u0026gt; The interval in seconds at which health check packets will be sent (default 1)   --health-check-max-packet-loss-count \u0026lt;uint\u0026gt; The maximum number of packets lost at which the health checker will mark the connection as down (default 5)    show show networks subctl show networks [flags]\nInspects the cluster and reports information about the detected network plugin and detected Cluster and Service CIDRs.\nshow versions subctl show versions [flags]\nShows the version and image repository of each Submariner component in the cluster.\nshow gateways subctl show gateways [flags]\nShows summary information about the Submariner gateways in the cluster.\nshow connections subctl show connections [flags]\nShows information about the Submariner endpoint connections with other clusters.\nshow endpoints subctl show endpoints [flags]\nShows information about the Submariner endpoints in the cluster.\nshow all subctl show all [flags]\nShows the aggregated information from all the other show commands.\nshow flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default $HOME/.kube/config)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use    verify subctl verify --kubecontexts \u0026lt;context1\u0026gt;,\u0026lt;context2\u0026gt; [flags]\nThe verify command verifies a Submariner deployment between two clusters is functioning properly. \u0026lt;context1\u0026gt; will be ClusterA in the reports, while \u0026lt;context2\u0026gt; will be ClusterB in the reports. The --verbose flag is recommended to see what\u0026rsquo;s happening during the tests.\nThere are several suites of verifications that can be performed. By default all verifications are performed. Some verifications are deemed disruptive in that they change some state of the clusters as a side effect. If running the command interactively, you will be prompted for confirmation to perform disruptive verifications unless the --disruptive-tests flag is also specified. If running non-interactively (that is with no stdin), --disruptive-tests must be specified otherwise disruptive verifications are skipped.\nThe connectivity suite verifies dataplane connectivity across the clusters for the following cases:\n Pods (on Gateway nodes) to Services Pods (on non-Gateway nodes) to Services Pods (on Gateway nodes) to Pods Pods (on non-Gateway nodes) to Pods  The service-discovery suite verifies DNS discovery of \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local entries across the clusters.\nThe gateway-failover suite verifies the continuity of cross-cluster dataplane connectivity after a gateway failure in a cluster occurs. This suite requires a single gateway configured on ClusterA and other available Worker nodes capable of serving as gateways. Please note that this verification is disruptive.\nverify flags    Flag Description     --connection-attempts \u0026lt;value\u0026gt; The maximum number of connection attempts (default 2)   --connection-timeout \u0026lt;value\u0026gt; The timeout in seconds per connection attempt (default 60)   --operation-timeout \u0026lt;value\u0026gt; Operation timeout for Kubernetes API calls (default 240)   --report-dir \u0026lt;string\u0026gt; XML report directory (default \u0026ldquo;.\u0026quot;)   --verbose Produce verbose logs during connectivity verification   --only Comma separated list of specific verifications to perform   --disruptive-tests Enable verifications which are potentially disruptive to your deployment    benchmark benchmark throughput subctl benchmark throughput --kubecontexts \u0026lt;context1\u0026gt;[,\u0026lt;context2\u0026gt;] [flags]\nThe benchmark throughput command runs a throughput benchmark test between two specified clusters or within a single cluster. It deploys a Pod to run the iperf tool and logs the output to the console. When running benchmark throughput, two types of tests will be executed:\n Pod to Pod - where both Pods are scheduled on Gateway nodes Pod to Pod - where both Pods are scheduled on non-Gateway nodes  benchmark latency subctl benchmark latency --kubecontexts \u0026lt;context1\u0026gt;[,\u0026lt;context2\u0026gt;] [flags]\nThe benchmark latency command runs a latency benchmark test between two specified clusters or within a single cluster. It deploys a Pod to run the netperf tool and logs the output to the console. When running benchmark latency, two types of tests will be executed:\n Pod to Pod - where both Pods are scheduled on Gateway nodes Pod to Pod - where both Pods are scheduled on non-Gateway nodes  benchmark flags    Flag Description     --intra-cluster Performs the benchmark test within a single cluster between Pods from a Non-Gateway node to a Gateway node   --verbose Produce verbose logs during benchmark tests    diagnose The subctl diagnose command is a tool that runs various checks to help diagnose issues in a Submariner deployment or some configurations in the cluster that may prevent Submariner from working properly.\nBelow is a list of available sub-commands:\n   Diagnose command Description Flags     deployment checks that the Submariner components are properly deployed and running with no overlapping CIDRs    connections checks that the Gateway connections to other clusters are all established    k8s-version checks if Submariner can be deployed on the Kubernetes version    kube-proxy-mode [flags] checks if the kube-proxy mode is supported by Submariner --namespace \u0026lt;string\u0026gt;   cni checks if the detected CNI network plugin is supported by Submariner    firewall intra-cluster [flags] checks if the firewall configuration allows traffic via intra-cluster Submariner VXLAN interface --validation-timeout \u0026lt;value\u0026gt; , --verbose, --namespace \u0026lt;string\u0026gt;   firewall metrics [flags] checks if the firewall configuration allows metrics to be accessed from the Gateway nodes --validation-timeout \u0026lt;value\u0026gt; , --verbose, --namespace \u0026lt;string\u0026gt;   firewall inter-cluster \u0026lt;localkubeconfig\u0026gt; \u0026lt;remotekubeconfig\u0026gt; [flags] checks if the firewall configuration allows tunnels to be configured on the Gateway nodes --validation-timeout \u0026lt;value\u0026gt;, --verbose, --namespace \u0026lt;string\u0026gt;   all runs all diagnostic checks (except those requiring two kubecontexts)     diagnose flags descriptions    Flag Description     --namespace \u0026lt;string\u0026gt; Namespace in which validation pods should be deployed. If not specified, the default namespace is used   --validation-timeout \u0026lt;value\u0026gt; Timeout in seconds while validating the connection attempt   --verbose Produce verbose logs during validation    diagnose global flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s) (default $HOME/.kube/config)   --kubecontext \u0026lt;string\u0026gt; Kubeconfig context to use    gather The subctl gather command is a tool that collects various information from clusters to aid in troubleshooting a Submariner deployment, including Kubernetes resources and Pod logs. Clusters from which information is gathered are provided via the --kubeconfig flag (or the KUBECONFIG environment variable). By default it will gather information from all the cluster contexts contained in the kubeconfig. To gather information from specific clusters, contexts can be passed using kubecontexts flag.\nThe tool creates a UTC timestamped directory of the format submariner-YYYYMMDDHHMMSS containing various files. Kubernetes resources are written to YAML files with the naming format \u0026lt;cluster-name\u0026gt;_\u0026lt;resource-type\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;resource-name\u0026gt;.yaml. Pod logs are written to files with the format \u0026lt;cluster-name\u0026gt;_\u0026lt;pod-name\u0026gt;.log\nThe specific information collected is configurable. As part of gathering connectivity resources, it also collects information specific to the CNI and Submariner cable driver in use from each node using file format \u0026lt;cluster-name\u0026gt;_\u0026lt;node-name\u0026gt;_\u0026lt;command\u0026gt;.yaml\ngather flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s)   --kubecontexts \u0026lt;string\u0026gt; comma separated list of kube contexts to use. By default all contexts referenced by kubeconfig are used   --module \u0026lt;string\u0026gt; Comma-separated list of components for which to gather data. Default is operator,connectivity,service-discovery,broker   --type \u0026lt;string\u0026gt; Comma-separated list of data types to gather. Default is logs,resources    gather examples These examples assume that kubeconfigs have been passed using the KUBECONFIG environment variable. Alternately, add the --kubeconfig flag if the environment variable is not set.\ngather all from all clusters It is recommended to use this when reporting any issue.\nsubctl gather\ngather all from specific clusters subctl gather --kubecontexts cluster-east\ngather operator and connectivity logs from specific clusters subctl gather --kubecontexts cluster-east,cluster-west --module operator,connectivity --type logs\ngather broker and service-discovery resources from all clusters subctl gather --module broker,service-discovery --type resources\ncloud cloud prepare subctl cloud prepare [flags]\nThis command prepares the underlying cloud infrastructure for Submariner installation.\nprepare aws subctl cloud prepare aws [flags]\nThis command prepares an OpenShift installer-provisioned infrastructure (IPI) on AWS cloud for Submariner installation.\n   Flag Description     --credentials \u0026lt;string\u0026gt; AWS credentials configuration file (default $HOME/.aws/credentials)   --gateway-instance \u0026lt;string\u0026gt; Type of gateway instance machine (default c5d.large)   --gateways \u0026lt;int\u0026gt; Number of dedicated gateways to deploy (default 1)   --infra-id \u0026lt;string\u0026gt; AWS infra ID   --ocp-metadata \u0026lt;string\u0026gt; OCP metadata.json file (or directory containing it) to read AWS infra ID and region from   --profile \u0026lt;string\u0026gt; AWS profile to use for credentials   --region \u0026lt;string\u0026gt; AWS region   --metrics-port \u0026lt;int\u0026gt; Metrics port (default 8080)   --nat-discovery-port \u0026lt;int\u0026gt; NAT discovery port (default 4490)   --natt-port \u0026lt;int\u0026gt; IPsec NAT Traversal port (default 4500)   --vxlan-port \u0026lt;int\u0026gt; Internal VXLAN port (default 4800)    prepare gcp subctl cloud prepare gcp [flags]\nThis command prepares an OpenShift installer-provisioned infrastructure (IPI) on GCP cloud for Submariner installation.\n   Flag Description     --credentials \u0026lt;string\u0026gt; GCP credentials configuration file (default $HOME/.gcp/osServiceAccount.json)   --dedicated-gateway Whether a dedicated gateway node has to be deployed (default false)   --gateway-instance \u0026lt;string\u0026gt; Type of gateway instance machine (default n1-standard-4)   --gateways \u0026lt;int\u0026gt; Number of dedicated gateways to deploy (default 1)   --infra-id \u0026lt;string\u0026gt; GCP infra ID   --ocp-metadata \u0026lt;string\u0026gt; OCP metadata.json file (or directory containing it) to read GCP infra ID and region from   --project-id \u0026lt;string\u0026gt; GCP project ID   --region \u0026lt;string\u0026gt; GCP region   --metrics-port \u0026lt;int\u0026gt; Metrics port (default 8080)   --nat-discovery-port \u0026lt;int\u0026gt; NAT discovery port (default 4490)   --natt-port \u0026lt;int\u0026gt; IPsec NAT Traversal port (default 4500)   --vxlan-port \u0026lt;int\u0026gt; Internal VXLAN port (default 4800)    prepare generic flags This command prepares a generic cluster for Submariner installation. It assumes that the cloud already has the necessary firewall ports opened and will only label the required number of gateway nodes for Submariner installation.\n   Flag Description     --gateways \u0026lt;int\u0026gt; Number of dedicated gateways to deploy (default 1)    prepare global flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s)   --kubecontexts \u0026lt;string\u0026gt; Comma separated list of kube contexts to use. By default all contexts referenced by kubeconfig are used    cloud cleanup This command cleans up the cloud after Submariner uninstallation.\ncleanup aws subctl cloud cleanup aws [flags]\nThis command cleans up an OpenShift installer-provisioned infrastructure (IPI) on AWS-based cloud after Submariner uninstallation.\n   Flag Description     --credentials \u0026lt;string\u0026gt; AWS credentials configuration file (default $HOME/.aws/credentials)   --infra-id \u0026lt;string\u0026gt; AWS infra ID   --ocp-metadata \u0026lt;string\u0026gt; OCP metadata.json file (or directory containing it) to read AWS infra ID and region from   --profile \u0026lt;string\u0026gt; AWS profile to use for credentials   --region \u0026lt;string\u0026gt; AWS region    cleanup gcp subctl cloud cleanup gcp [flags]\nThis command cleans up an installer-provisioned infrastructure (IPI) on GCP-based cloud after Submariner uninstallation.\n   Flag Description     --credentials \u0026lt;string\u0026gt; GCP Credentials configuration file (default $HOME/.gcp/osServiceAccount.json)   --infra-id \u0026lt;string\u0026gt; GCP infra ID   --ocp-metadata \u0026lt;string\u0026gt; OCP metadata.json file (or directory containing it) to read GCP infra ID and region from   --project-id \u0026lt;string\u0026gt; GCP project ID   --region \u0026lt;string\u0026gt; GCP region    cleanup generic subctl cloud cleanup generic [flags]\nThis command removes the labels from gateway nodes after Submariner uninstallation.\ncleanup flags    Flag Description     --kubeconfig \u0026lt;string\u0026gt; Absolute path(s) to the kubeconfig file(s)   --kubecontexts \u0026lt;string\u0026gt; kubeconfig context to use    version subctl version\nPrints the version details for the subctl binary.\n"
},
{
	"uri": "/getting-started/quickstart/k3s/",
	"title": "K3s",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two K3s clusters and deploy Submariner with Service Discovery to interconnect the two clusters. Note that this guide focuses on Submariner deployment on clusters with non-overlapping Pod and Service CIDRs. For connecting clusters with overlapping CIDRs, please refer to the Globalnet guide.\nPrerequisites   Prepare two nodes with K3s-supported OS installed (These two nodes are referred to as node-a and node-b).\n  Choose the Pod CIDR and the Service CIDR for each K3s cluster.\nIn this guide, we will use the following CIDRs:\n   Cluster Pod CIDR Service CIDR     cluster-a 10.44.0.0/16 10.45.0.0/16   cluster-b 10.144.0.0/16 10.145.0.0/16      Deploy K3s Clusters Deploy cluster-a on node-a In this step you will deploy K3s on node-a using the the cluster-a CIDRs.\nPOD_CIDR=10.44.0.0/16 SERVICE_CIDR=10.45.0.0/16 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--cluster-cidr $POD_CIDR--service-cidr $SERVICE_CIDR\u0026#34; sh -s -  For more information on interacting with K3s, please refer to the k3s documentation.\n The kubeconfig file needs to be modified to set the Kubernetes API endpoint to the public IP of node-a, which is obtained from the first field yielded by hostname -I, and to give the context a name other than “default”. (This uses yq v4.7.0 or later.)\nsudo cp /etc/rancher/k3s/k3s.yaml kubeconfig.cluster-a sudo chown $(id -u):$(id -g) kubeconfig.cluster-a export IP=$(hostname -I | awk \u0026#39;{print $1}\u0026#39;) yq -i eval \\ \u0026#39;.clusters[].cluster.server |= sub(\u0026#34;127.0.0.1\u0026#34;, env(IP)) | .contexts[].name = \u0026#34;cluster-a\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; \\ kubeconfig.cluster-a Deploy cluster-b on node-b In this step you will deploy K3s on node-b using the the cluster-b CIDRs.\nPOD_CIDR=10.144.0.0/16 SERVICE_CIDR=10.145.0.0/16 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\u0026#34;--cluster-cidr $POD_CIDR--service-cidr $SERVICE_CIDR\u0026#34; sh -s - The kubeconfig file needs to be modified to set the Kubernetes API endpoint to the public IP of node-b, which is obtained from the first field yielded by hostname -I, and to give the context a name other than “default”.\nsudo cp /etc/rancher/k3s/k3s.yaml kubeconfig.cluster-b sudo chown $(id -u):$(id -g) kubeconfig.cluster-b export IP=$(hostname -I | awk \u0026#39;{print $1}\u0026#39;) yq -i eval \\ \u0026#39;.clusters[].cluster.server |= sub(\u0026#34;127.0.0.1\u0026#34;, env(IP)) | .contexts[].name = \u0026#34;cluster-b\u0026#34; | .current-context = \u0026#34;cluster-b\u0026#34;\u0026#39; \\ kubeconfig.cluster-b Next, copy kubeconfig.cluster-b to node-a.\nInstall subctl on node-a Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nUse cluster-a as the Broker On node-a, run:\nsubctl deploy-broker --kubeconfig kubeconfig.cluster-a Join cluster-a to the Broker subctl join --kubeconfig kubeconfig.cluster-a broker-info.subm --clusterid cluster-a --natt=false Join cluster-b to the Broker subctl join --kubeconfig kubeconfig.cluster-b broker-info.subm --clusterid cluster-b --natt=false Verify Deployment Verify Automatically with subctl This will perform automated verifications between the clusters.\nKUBECONFIG=kubeconfig.cluster-a:kubeconfig.cluster-b subctl verify --kubecontexts cluster-a,cluster-b --only service-discovery,connectivity --verbose Verify Manually To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service kubectl --kubeconfig kubeconfig.cluster-b create deployment nginx --image=nginx kubectl --kubeconfig kubeconfig.cluster-b expose deployment nginx --port=80 subctl export service --kubeconfig kubeconfig.cluster-b --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nkubectl --kubeconfig kubeconfig.cluster-a -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local "
},
{
	"uri": "/operations/usage/",
	"title": "User Guide",
	"tags": [],
	"description": "",
	"content": "Overview This guide is intended for users who have a Submariner environment set up and want to verify the installation and learn more about how to use Submariner and the main capabilities it provides. This guide assumes that there are two Kubernetes clusters, cluster2 and cluster3, forming a cluster set, and that the Broker is deployed into a separate cluster cluster1.\nMake sure you have subctl set up. Regardless of how Submariner was deployed, subctl can be used for various verification and troubleshooting tasks, as shown in this guide.\n This guide focuses on a non-Globalnet Submariner deployment.\n 1. Validate the Installation On the Broker The Broker facilitates the exchange of metadata information between the connected clusters, enabling them to discover one another. The Broker consists of only a set of Custom Resource Definitions (CRDs); there are no Pods or Services deployed with it.\nThis command validates that the Broker namespace has been created in the Broker cluster:\n$ export KUBECONFIG=cluster1/auth/kubeconfig $ kubectl config use-context cluster1 Switched to context \u0026#34;cluster1\u0026#34;. $ kubectl get namespace submariner-k8s-broker NAME STATUS AGE submariner-k8s-broker Active 5m This command validates that the Submariner CRDs have been created in the Broker cluster:\n$ kubectl get crds | grep -iE \u0026#39;submariner|multicluster.x-k8s.io\u0026#39; clusters.submariner.io 2020-11-30T13:49:16Z endpoints.submariner.io 2020-11-30T13:49:16Z gateways.submariner.io 2020-11-30T13:49:16Z serviceimports.multicluster.x-k8s.io 2020-11-30T13:52:39Z This command validates that the participating clusters have successfully joined the Broker:\n$ kubectl -n submariner-k8s-broker get clusters.submariner.io NAME AGE cluster2 5m9s cluster3 2m9s On Connected Clusters The commands below can be used on either cluster2 or cluster3 to verify that the two clusters have successfully formed a cluster set and are properly connected to one another. In this example, the commands are being issued on cluster2.\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. The command below lists all the Submariner related Pods. Ensure that the STATUS for each is Running, noting that some could have an intermediate transient status, like Pending or ContainerCreating, indicating they are still starting up. To continuously monitor the Pods, you can specify the --watch flag with the command:\n$ kubectl -n submariner-operator get pods NAME READY STATUS RESTARTS AGE submariner-gateway-btzrq 1/1 Running 0 76s submariner-lighthouse-agent-586cf4899-wn747 1/1 Running 0 75s submariner-lighthouse-coredns-c88f64f5-h77kw 1/1 Running 0 73s submariner-lighthouse-coredns-c88f64f5-qlw4x 1/1 Running 0 73s submariner-operator-dcbdf5669-n7jgp 1/1 Running 0 89s submariner-routeagent-bmgbc 1/1 Running 0 75s submariner-routeagent-rl9nh 1/1 Running 0 75s submariner-routeagent-wqmzs 1/1 Running 0 75s This command verifies on which Kubernetes node the Gateway Engine is running:\n$ kubectl get node --selector=submariner.io/gateway=true -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cluster2-worker Ready worker 6h59m v1.17.0 172.17.0.7 3.81.125.62 Ubuntu 19.10 5.8.18-200.fc32.x86_64 containerd://1.3.2 This command verifies the connection between the participating clusters:\n$ subctl show connections --kubecontext cluster2 Showing information for cluster \u0026#34;cluster2\u0026#34;: GATEWAY CLUSTER REMOTE IP CABLE DRIVER SUBNETS STATUS cluster3-worker cluster3 172.17.0.10 libreswan 100.3.0.0/16, 10.3.0.0/16 connected This command shows detailed information about the Gateway including the connections to other clusters. The section highlighted in bold shows the connection information for cluster3, including the connection status and latency statistics:\n $ kubectl -n submariner-operator describe Gateway Name: cluster2-worker Namespace: submariner-operator Labels:  Annotations: update-timestamp: 1606751397 API Version: submariner.io/v1 Kind: Gateway Metadata: Creation Timestamp: 2020-11-30T13:51:39Z Generation: 538 Resource Version: 28717 Self Link: /apis/submariner.io/v1/namespaces/submariner-operator/gateways/cluster2-worker UID: 682f791a-00b5-4f51-8249-80c7c82c4bbf Status: Connections: Endpoint: Backend: libreswan cable_name: submariner-cable-cluster3-172-17-0-10 cluster_id: cluster3 Health Check IP: 10.3.224.0 Hostname: cluster3-worker nat_enabled: false private_ip: 172.17.0.10 public_ip: Subnets: 100.3.0.0/16 10.3.0.0/16 Latency RTT: Average: 1.16693ms Last: 1.128109ms Max: 1.470344ms Min: 1.110059ms Std Dev: 68.57µs Status: connected Status Message:  Ha Status: active Local Endpoint: Backend: libreswan cable_name: submariner-cable-cluster2-172-17-0-7 cluster_id: cluster2 Health Check IP: 10.2.224.0 Hostname: cluster2-worker nat_enabled: false private_ip: 172.17.0.7 public_ip: Subnets: 100.2.0.0/16 10.2.0.0/16 Status Failure: Version: v0.8.0-pre0-1-g5d7f163 Events:   To validate that Service Discovery (Lighthouse) is installed properly, check that the ServiceExport and ServiceImport CRDs have been deployed in the cluster:\n$ kubectl get crds | grep -iE \u0026#39;multicluster.x-k8s.io\u0026#39; serviceexports.multicluster.x-k8s.io 2020-11-30T13:50:34Z serviceimports.multicluster.x-k8s.io 2020-11-30T13:50:33Z Verify that the submariner-lighthouse-coredns Service is ready:\n$ kubectl -n submariner-operator get service submariner-lighthouse-coredns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE submariner-lighthouse-coredns ClusterIP 100.2.177.123 \u0026lt;none\u0026gt; 53/UDP 126m Verify that CoreDNS was properly configured to forward requests sent for the clusterset.local domain to the to Lighthouse CoreDNS Server in the cluster:\n$ kubectl -n kube-system describe configmap coredns Name: coredns Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== Corefile: ---- #lighthouse-start AUTO-GENERATED SECTION. DO NOT EDIT clusterset.local:53 { forward . 100.2.177.123 } #lighthouse-end .:53 { errors health { lameduck 5s } ready kubernetes cluster2.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } Note that 100.2.177.123 is the ClusterIP address of the submariner-lighthouse-coredns Service we verified earlier.\n2. Export Services Across Clusters At this point, we have enabled secure IP communication between the connected clusters and formed the cluster set infrastructure. However, further configuration is required in order to signify that a Service should be visible and discoverable to other clusters in the cluster set. In following sections, we will define a Service and show how to export it to other clusters.\nThis guide uses a simple nginx server for demonstration purposes.\nIn the example below, we create the nginx resources within the nginx-test namespace. Note that the namespace must be created in both clusters for service discovery to work properly.\n Test ClusterIP Services 1. Create an nginx Deployment on cluster3 $ export KUBECONFIG=cluster3/auth/kubeconfig $ kubectl config use-context cluster3 Switched to context \u0026#34;cluster3\u0026#34;. The following commands create an nginx Service in the nginx-test namespace which targets TCP port 8080, with name http, on any Pod with the app: nginx label and exposes it on an abstracted Service port. When created, the Service is assigned a unique IP address (also called ClusterIP):\n$ kubectl create namespace nginx-test namespace/nginx-test created $ kubectl -n nginx-test create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine deployment.apps/nginx created kubectl apply the following YAML within the nginx-test namespace to create the service:\napiVersion: v1 kind: Service metadata: labels: app: nginx name: nginx namespace: nginx-test spec: ports: - name: http port: 8080 protocol: TCP targetPort: 8080 selector: app: nginx sessionAffinity: None type: ClusterIP status: loadBalancer: {} $ kubectl -n nginx-test apply -f nginx-svc.yaml service/nginx exposed Verify that the Service exists and is running:\n$ kubectl -n nginx-test get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 100.3.220.176 \u0026lt;none\u0026gt; 8080/TCP 2m41s $ kubectl -n nginx-test get pods -l app=nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-667744f849-t26s5 1/1 Running 0 3m 10.3.0.5 cluster3-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 2. Export the Service In order to signify that the Service should be visible and discoverable to other clusters in the cluster set, a ServiceExport object needs to be created. This is done using the subctl export command:\n$ subctl export service --namespace nginx-test nginx Service exported successfully After creation of the ServiceExport, the nginx Service will be exported to other clusters via the Broker. The Status information on the ServiceExport object will indicate this:\n$ kubectl -n nginx-test describe serviceexports Name: nginx Namespace: nginx-test Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-12-01T12:35:32Z Generation: 1 Resource Version: 302209 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/nginx-test/serviceexports/nginx UID: afe0533c-7cca-4443-9d8a-aee8e888e8bc Status: Conditions: Last Transition Time: 2020-12-01T12:35:32Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-12-01T12:35:32Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; Once exported, the Service can be discovered as nginx.nginx-test.svc.clusterset.local across the cluster set.\n3. Consume the Service on cluster2 Verify that the exported nginx Service was imported to cluster2 as expected. Submariner (via Lighthouse) automatically creates a corresponding ServiceImport:\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. $ kubectl get -n submariner-operator serviceimport NAME TYPE IP AGE nginx-nginx-test-cluster3 ClusterSetIP [100.3.220.176] 13m Next, run a test Pod on cluster2 and try to access the nginx Service from within the Pod:\n$ kubectl create namespace nginx-test namespace/nginx-test created $ kubectl run -n nginx-test tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash bash-5.0# curl nginx.nginx-test.svc.clusterset.local:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; bash-5.0# dig nginx.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; nginx.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 34800 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 6ff7ea72c14ce2d4 (echoed) ;; QUESTION SECTION: ;nginx.nginx-test.svc.clusterset.local. IN\tA ;; ANSWER SECTION: nginx.nginx-test.svc.clusterset.local. 5 IN A\t100.3.220.176 ;; Query time: 16 msec ;; SERVER: 100.2.0.10#53(100.2.0.10) ;; WHEN: Mon Nov 30 17:52:55 UTC 2020 ;; MSG SIZE rcvd: 125 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; SRV _http._tcp.nginx.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 21993 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 3f6018af2626ebd2 (echoed) ;; QUESTION SECTION: ;_http._tcp.nginx.nginx-test.svc.clusterset.local. IN SRV ;; ANSWER SECTION: _http._tcp.nginx.nginx-test.svc.clusterset.local. 5 IN SRV 0 50 8080 nginx.nginx-test.svc.clusterset.local. ;; Query time: 3 msec ;; SERVER: 100.2.0.10#53(100.2.0.10) ;; WHEN: Fri Jul 23 07:35:51 UTC 2021 ;; MSG SIZE rcvd: 194 Note that DNS resolution works across the clusters, and that the IP address 100.3.220.176 returned is the same ClusterIP associated with the nginx Service on cluster3.\n4. Create an nginx Deployment on cluster2 If multiple clusters export a Service with the same name and from the same namespace, it will be recognized as a single logical Service. To test this, we will deploy the same nginx Service in the same namespace on cluster2:\n$ kubectl -n nginx-test create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine deployment.apps/nginx created kubectl apply the following YAML within the nginx-test namespace to create the service:\napiVersion: v1 kind: Service metadata: labels: app: nginx name: nginx namespace: nginx-test spec: ports: - name: http port: 8080 protocol: TCP targetPort: 8080 selector: app: nginx sessionAffinity: None type: ClusterIP status: loadBalancer: {} $ kubectl -n nginx-test apply -f nginx-svc.yaml service/nginx exposed Verify the Service exists and is running:\n$ kubectl -n nginx-test get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 100.2.29.136 \u0026lt;none\u0026gt; 8080/TCP 1m40s $ kubectl -n nginx-test get pods -l app=nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-5578584966-d7sj7 1/1 Running 0 22s 10.2.224.3 cluster2-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5. Export the Service As before, use the subctl export command to export the Service:\n$ subctl export service --namespace nginx-test nginx Service exported successfully After creation of the ServiceExport, the nginx Service will be exported to other clusters via the Broker. The Status information on the ServiceExport object will indicate this:\n$ kubectl -n nginx-test describe serviceexports Name: nginx Namespace: nginx-test Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-12-07T17:37:59Z Generation: 1 Resource Version: 3131 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/nginx-test/serviceexports/nginx UID: 7348eb3c-9558-4dc7-be1d-b0255a2038fd Status: Conditions: Last Transition Time: 2020-12-07T17:37:59Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-12-07T17:37:59Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; 6. Consume the Service from cluster2 Run a test Pod on cluster2 and try to access the nginx Service from within the Pod:\nkubectl run -n nginx-test tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash bash-5.0# curl nginx.nginx-test.svc.clusterset.local:8080 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; bash-5.0# dig nginx.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; nginx.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 55022 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 6f9db9800a9a9779 (echoed) ;; QUESTION SECTION: ;nginx.nginx-test.svc.clusterset.local. IN\tA ;; ANSWER SECTION: nginx.nginx-test.svc.clusterset.local. 5 IN A\t100.2.29.136 ;; Query time: 5 msec ;; SERVER: 100.3.0.10#53(100.3.0.10) ;; WHEN: Tue Dec 01 07:45:48 UTC 2020 ;; MSG SIZE rcvd: 125 bash-5.0# dig SRV _http._tcp.nginx.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; SRV _http._tcp.nginx.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 19656 ;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 8fe1ebfcf9165a8d (echoed) ;; QUESTION SECTION: ;_http._tcp.nginx.nginx-test.svc.clusterset.local. IN SRV ;; ANSWER SECTION: _http._tcp.nginx.nginx-test.svc.clusterset.local. 5 IN SRV 0 50 8080 nginx.nginx-test.svc.clusterset.local. ;; Query time: 16 msec ;; SERVER: 100.2.0.10#53(100.2.0.10) ;; WHEN: Fri Jul 23 09:11:21 UTC 2021 ;; MSG SIZE rcvd: 194 At this point we have the same nginx Service deployed within the nginx-test namespace on both clusters. Note that DNS resolution works, and the IP address 100.2.29.136 returned is the ClusterIP associated with the local nginx Service deployed on cluster2. This is expected, as Submariner prefers to handle the traffic locally whenever possible.\n Service Discovery for Services Deployed to Multiple Clusters Submariner follows this logic for service discovery across the cluster set:\n  If an exported Service is not available in the local cluster, Lighthouse DNS returns the IP address of the ClusterIP Service from one of the remote clusters on which the Service was exported. If it is an SRV query, an SRV record with port and domain name corresponding to the ClusterIP will be returned.\n  If an exported Service is available in the local cluster, Lighthouse DNS always returns the IP address of the local ClusterIP Service. In this example, if a Pod from cluster2 tries to access the nginx Service as nginx.nginx-test.svc.clusterset.local now, Lighthouse DNS resolves the Service as 100.2.29.136 which is the local ClusterIP Service on cluster2. Similarly, if a Pod from cluster3 tries to access the nginx Service as nginx.nginx-test.svc.clusterset.local, Lighthouse DNS resolves the Service as 100.3.220.176 which is the local ClusterIP Service on cluster3.\n  If multiple clusters export a Service with the same name and from the same namespace, Lighthouse DNS load-balances between the clusters in a round-robin fashion. If, in our example, a Pod from a third cluster that joined the cluster set tries to access the nginx Service as nginx.nginx-test.svc.clusterset.local, Lighthouse will round-robin the DNS responses across cluster2 and cluster3, causing requests to be served by both clusters. Note that Lighthouse returns IPs from connected clusters only. Clusters in disconnected state are ignored.\n  Applications can always access a Service from a specific cluster by prefixing the DNS query with cluster-id as follows: \u0026lt;cluster-id\u0026gt;.\u0026lt;svcname\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local. In our example, querying for cluster2.nginx.nginx-test.svc.clusterset.local always returns the ClusterIP Service on cluster2. Similarly, cluster3.nginx.nginx-test.svc.clusterset.local always returns the ClusterIP Service on cluster3.\n  Test StatefulSet and Headless Service Submariner also supports Headless Services with StatefulSets, making it possible to access individual Pods via their stable DNS name. Kubernetes supports this by introducing stable Pod IDs composed of \u0026lt;pod-name\u0026gt;.\u0026lt;svc-name\u0026gt;.\u0026lt;ns\u0026gt;.svc.cluster.local within a single cluster, which Submariner extends to \u0026lt;pod-name\u0026gt;.\u0026lt;cluster-id\u0026gt;.\u0026lt;svc-name\u0026gt;.\u0026lt;ns\u0026gt;.svc.clusterset.local across the cluster set. The Headless Service in this case offers one single Service for all the underlying Pods.\nLike a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. StatefulSets are typically used for applications that require stable unique network identifiers, persistent storage, and ordered deployment and scaling.\n1. Create a StatefulSet and Headless Service on cluster3 kubectl apply the following yaml within the nginx-test namespace:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web This specification will create a StatefulSet named web which indicates that two replicas of the nginx container will be launched in unique Pods. This also creates a Headless Service called nginx-ss on the nginx-test namespace. Note that Headless Service is requested by explicitly specifying \u0026ldquo;None\u0026rdquo; for the clusterIP (.spec.clusterIP).\n$ kubectl -n nginx-test apply -f ./nginx-ss.yaml service/nginx-ss created statefulset.apps/web created Verify the Service and StatefulSet:\n$ kubectl -n nginx-test get service nginx-ss NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ss ClusterIP None \u0026lt;none\u0026gt; 80/TCP 83s $ kubectl -n nginx-test describe statefulset web Name: web Namespace: nginx-test CreationTimestamp: Mon, 30 Nov 2020 21:53:01 +0200 Selector: app.kubernetes.io/instance=nginx-ss,app.kubernetes.io/name=nginx-ss Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Replicas: 2 desired | 2 total Update Strategy: RollingUpdate Partition: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app.kubernetes.io/instance=nginx-ss app.kubernetes.io/name=nginx-ss Containers: nginx-ss: Image: nginxinc/nginx-unprivileged:stable-alpine Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Volume Claims: \u0026lt;none\u0026gt; Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 94s statefulset-controller create Pod web-0 in StatefulSet web successful Normal SuccessfulCreate 85s statefulset-controller create Pod web-1 in StatefulSet web successful 2. Export the Service on cluster-3 As before, use the subctl export command to export the Service:\n$ subctl export service --namespace nginx-test nginx-ss Service exported successfully After creation of the ServiceExport, the nginx-ss Service will be exported to other clusters via the Broker. The Status information on the ServiceExport object will indicate this:\n$ kubectl -n nginx-test describe serviceexport nginx-ss Name: nginx-ss Namespace: nginx-test Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-11-30T19:59:44Z Generation: 1 Resource Version: 83431 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/nginx-test/serviceexports/nginx-ss UID: 2c0d6419-6160-431e-990c-8a9993363b10 Status: Conditions: Last Transition Time: 2020-11-30T19:59:44Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-11-30T19:59:44Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; Once the Service is exported successfully, it can be discovered as nginx-ss.nginx-test.svc.clusterset.local across the cluster set. In addition, the individual Pods can be accessed as web-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local and web-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local.\n3. Consume the Service from cluster2 Verify that the exported nginx-ss Service was imported to cluster2. Submariner (via Lighthouse) automatically creates a corresponding ServiceImport:\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. $ kubectl get -n submariner-operator serviceimport NAME TYPE IP AGE nginx-nginx-test-cluster3 ClusterSetIP [100.3.220.176] 166m nginx-ss-nginx-test-cluster3 Headless 5m48s Next, run a test Pod on cluster2 and try to access the nginx-ss Service from within the Pod:\nkubectl run -n nginx-test tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash bash-5.0# dig nginx-ss.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; nginx-ss.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 19729 ;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: 0b17506cb2b4a93b (echoed) ;; QUESTION SECTION: ;nginx-ss.nginx-test.svc.clusterset.local. IN A ;; ANSWER SECTION: nginx-ss.nginx-test.svc.clusterset.local. 5 IN A\t10.3.0.5 nginx-ss.nginx-test.svc.clusterset.local. 5 IN A\t10.3.224.3 ;; Query time: 1 msec ;; SERVER: 100.2.0.10#53(100.2.0.10) ;; WHEN: Mon Nov 30 20:18:08 UTC 2020 ;; MSG SIZE rcvd: 184 bash-5.0# dig SRV _web._tcp.nginx-ss.nginx-test.svc.clusterset.local ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.16.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; SRV _web._tcp.nginx-ss.nginx-test.svc.clusterset.local ;; global options: +cmd ;; Got answer: ;; WARNING: .local is reserved for Multicast DNS ;; You are currently testing what happens when an mDNS query is leaked to DNS ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 16402 ;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; WARNING: recursion requested but not available ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ; COOKIE: cf1e04578842eb5b (echoed) ;; QUESTION SECTION: ;_web._tcp.nginx-ss.nginx-test.svc.clusterset.local. IN SRV ;; ANSWER SECTION: _web._tcp.nginx-ss.nginx-test.svc.clusterset.local. 5 IN SRV 0 50 80 web-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local. _web._tcp.nginx-ss.nginx-test.svc.clusterset.local. 5 IN SRV 0 50 80 web-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local. ;; Query time: 2 msec ;; SERVER: 100.2.0.10#53(100.2.0.10) ;; WHEN: Fri Jul 23 07:38:03 UTC 2021 ;; MSG SIZE rcvd: 341 You can also access the individual Pods:\nbash-5.0# nslookup web-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local Server:\t100.2.0.10 Address:\t100.2.0.10#53 Name:\tweb-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local Address: 10.3.0.5 bash-5.0# nslookup web-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local Server:\t100.2.0.10 Address:\t100.2.0.10#53 Name:\tweb-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local Address: 10.3.224.3 In case of SRV you can access pod from individual clusters but not the pods direcly:\nbash-5.0# nslookup -q=SRV _web._tcp.cluster3.nginx-ss.nginx-test.svc.clusterset.local Server:\t100.2.0.10 Address:\t100.2.0.10#53 _web._tcp.cluster3.nginx-ss.nginx-test.svc.clusterset.local\tservice = 0 50 80 web-0.cluster3.nginx-ss.nginx-test.svc.clusterset.local. _web._tcp.cluster3.nginx-ss.nginx-test.svc.clusterset.local\tservice = 0 50 80 web-1.cluster3.nginx-ss.nginx-test.svc.clusterset.local. Clean the Created Resources To remove the previously created Kubernetes resources, simply delete the nginx-test namespace from both clusters:\n$ export KUBECONFIG=cluster2/auth/kubeconfig $ kubectl config use-context cluster2 Switched to context \u0026#34;cluster2\u0026#34;. $ kubectl delete namespace nginx-test namespace \u0026#34;nginx-test\u0026#34; deleted $ export KUBECONFIG=cluster3/auth/kubeconfig $ kubectl config use-context cluster3 Switched to context \u0026#34;cluster3\u0026#34;. $ kubectl delete namespace nginx-test namespace \u0026#34;nginx-test\u0026#34; deleted "
},
{
	"uri": "/operations/",
	"title": "Operations",
	"tags": [],
	"description": "",
	"content": " Deployment  subctl Helm Calico CNI   User Guide Monitoring Troubleshooting Known Issues Uninstalling Submariner  "
},
{
	"uri": "/operations/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner provides a number of Prometheus metrics, and sets up ServiceMonitor instances which allow these metrics to be scraped by an in-cluster Prometheus deployment. Prometheus is a pluggable metrics collection and storage system and can act as a data source for Grafana, a metrics visualization frontend. Unlike some metrics collectors, Prometheus requires the collectors to pull metrics from each source.\nPrometheus Operator To start monitoring Submariner using the Prometheus Operator, Prometheus needs to be configured to scrape the Submariner Operator’s namespace (submariner-operator by default). The specifics depend on your Prometheus deployment, but typically, this will require you to:\n  Add the Submariner Operator’s namespace to Prometheus’ ClusterRoleBinding.\n  Ensure that Prometheus’ configuration doesn’t prevent it from scraping this namespace.\n  A minimal Prometheus object providing access to the Submariner metrics is as follows:\napiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: name: prometheus labels: prometheus: prometheus spec: replicas: 1 serviceAccountName: prometheus serviceMonitorNamespaceSelector: {} serviceMonitorSelector: matchLabels: name: submariner-operator OpenShift Setup OpenShift 4.5 or later will automatically discover the Submariner metrics with service monitors in the openshift-monitoring namespace.\nMetrics Reference Submariner metrics provide insights into both the state of Submariner itself, as well as the inter-cluster network behavior of your cluster set. All Submariner metrics are exported within the submariner-operator namespace by default.\nThe following metrics are exposed currently:\nSubmariner Gateway    Name Label Description     submariner_gateways  The number of gateways in the cluster   submariner_gateway_creation_timestamp local_cluster, local_hostname Timestamp of gateway creation time   submariner_gateway_sync_iterations  Gateway synchronization iterations   submariner_gateway_rx_bytes cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Count of bytes received by cable driver and cable   submariner_gateway_tx_bytes cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Count of bytes transmitted by cable driver and cable    Submariner Connections    Name Label Description     submariner_requested_connections local_cluster, local_hostname, remote_cluster, remote_hostname, status: “connecting”, “connected”, or “error” The number of connections by endpoint and status   submariner_connections cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip, status: “connecting”, “connected”, or “error” The number of connections and corresponding status by cable driver and cable   submariner_connection_established_timestamp cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Timestamp of last successful connection established by cable driver and cable   submariner_connection_latency_seconds cable_driver, local_cluster, local_hostname, local_endpoint_ip, remote_cluster, remote_hostname, remote_endpoint_ip Connection latency in seconds; last RTT, by cable driver and cable    Globalnet    Name Label Description     submariner_global_IP_availability cidr Count of available global IPs per CIDR   submariner_global_IP_allocated cidr Count of global IPs allocated for Pods/Services per CIDR    Service Discovery    Name Label Description     submariner_service_import direction, operation, syncer_name Count of imported Services   submariner_service_export direction, operation, syncer_name Count of exported Services   submariner_service_discovery_query source_cluster, destination_cluster, destination_service_name, destination_service_ip, destination_service_namespace Count DNS queries handled by Lighthouse plugin    "
},
{
	"uri": "/getting-started/quickstart/managed-kubernetes/rancher/",
	"title": "Rancher",
	"tags": [],
	"description": "",
	"content": " Prerequisites These instructions were developed with Rancher v2.4.x\nMake sure you are familiar with Rancher, and creating clusters. You can create either node driver clusters or Custom clusters, as long as your designated gateway nodes can communicate with each other.\nCreate and Deploy Cluster A In this step you will deploy cluster A, with the default IP CIDRs\n   Pod CIDR Service CIDR     10.42.0.0/16 10.43.0.0/16    Use the Rancher UI to create a cluster, leaving the default options selected.\nMake sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nCreate and Deploy Cluster B In this step you will deploy cluster B, modifying the default IP CIDRs\n   Pod CIDR Service CIDR     10.44.0.0/16 10.45.0.0/16    Create your cluster, but select Edit as YAML in the cluster creation UI. Edit the services stanza to reflect the options below, while making sure to keep the options that were already defined.\nservices: kube-api: service_cluster_ip_range: 10.45.0.0/16 kube-controller: cluster_cidr: 10.44.0.0/16 service_cluster_ip_range: 10.45.0.0/16 kubelet: cluster_domain: cluster.local cluster_dns_server: 10.45.0.10 Make sure you create at least one node that has a publicly accessible IP with the label submariner.io/gateway: \u0026quot;true\u0026quot;, either via node pool or via a custom node registration command.\nOnce you have done this, you can deploy your cluster.\nInstall subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nObtain the kubeconfig files from the Rancher UI for each of your clusters, placing them in the respective kubeconfigs.\n   Cluster Kubeconfig File Name     Cluster A kubeconfig-cluster-a   Cluster B kubeconfig-cluster-b    Edit the kubeconfig files so they use the context names “cluster-a” and “cluster-b”.\nUse cluster-a as Broker subctl deploy-broker --kubeconfig kubeconfig-cluster-a Join cluster-a and cluster-b to the Broker subctl join --kubeconfig kubeconfig-cluster-a broker-info.subm --clusterid cluster-a subctl join --kubeconfig kubeconfig-cluster-b broker-info.subm --clusterid cluster-b Verify connectivity This will run a series of E2E tests to verify proper connectivity between the cluster Pods and Services\nexport KUBECONFIG=kubeconfig-cluster-a:kubeconfig-cluster-b subctl verify --kubecontexts cluster-a,cluster-b --only connectivity --verbose "
},
{
	"uri": "/development/code-review/",
	"title": "Code Review Guide",
	"tags": [],
	"description": "",
	"content": "Code Review Guide This guide is meant to facilitate Submariner code review by sharing norms, best practices, and useful patterns.\nSubmariner follows the Kubernetes Code Review Guide wherever relevant. This guide collects the most important highlights of the Kubernetes process and adds Submariner-specific extensions.\nTwo non-author Committer approvals required Pull Requests to Submariner require two approvals from a Committer to the relevant part of the code base, as defined by the CODEOWNERS file at the root of each repository and the Community Membership/Committers process.\nNo merge commits Kubernetes recommends avoiding merge commits.\nWith our current GitHub setup, pull requests are liable to include merge commits temporarily. Whenever a PR is updated through the UI, GitHub merges the target branch into the PR. However, since we merge PRs by either squashing or rebasing them, those merge commits disappear from the series of commits which ultimately ends up in the target branch.\nSquash/amend commits into discrete steps Kubernetes recommends squashing commits using these guidelines.\nAfter a review, prepare your PR for merging by squashing your commits.\nAll commits left on your branch after a review should represent meaningful milestones or units of work. Use commits to add clarity to the development and review process. Keep in mind that smaller commits are easier to review.\nBefore merging a PR, squash the following kinds of commits:\n Fixes/review feedback Typos Merges and rebases Work in progress Aim to have every commit in a PR compile and pass tests independently if you can, but it\u0026rsquo;s not a requirement.  Address code review feedback with new commits When addressing review comments, as a general rule, push a new commit instead of amending to the prior commit as the former makes it easy for reviewers to determine what changed.\nTo avoid cluttering the git log, squash the review commits into the appropriate commit before merging. The committer can do this in GitHub via the \u0026ldquo;Squash and merge\u0026rdquo; option. However you may want to preserve other commits, in which case squashing will need to be done manually via the Git CLI. To make that simpler, you can commit the review-prompted changes with git commit --fixup with the appropriate commit hash. This will keep them as separate commits, and if you later rebase with the --autosquash option (that is git rebase --autosquash -i) they will automatically be selected for squashing.\nCommit message formatting Kubernetes recommends these commit message practices.\nIn summary:\n Separate subject from body with a blank line Limit the subject line to 50 characters Capitalize the subject line Do not end the subject line with a period Use the imperative mood in the subject line Wrap the body at 72 characters Use the body to explain what and why vs how  GitLint will automatically be run against all commits to try to validate these conventions.\nDismiss reviews after substantial changes If a PR is substantially changed after a code review, the author should dismiss the stale reviews.\nWith the current GitHub configuration, reviews are not automatically dismissed when PRs are updated. This is to cause less drag for the typical cases, like minor merge conflicts. As Submariner grows, it might make sense to trade this low-drag solution for one where only exactly the reviewed code can be merged.\nAddress all -1s before merging If someone requests changes (\u0026ldquo;votes -1\u0026rdquo;) for a PR, a best-effort should be made to address those concerns and achieve a neutral position or approval (0/+1 vote) before the PR is merged.\nUpdate branch only after required reviews To avoid wasting resources by running unnecessary jobs, only use the Update branch button to add a merge commit once a PR is actually ready to merge (has required reviews and no -1s). Unless other relevant code has changed, the new job results don\u0026rsquo;t tell us anything new. Since changes are constantly being merged, it\u0026rsquo;s likely another merge commit and set of jobs will be necessary right before merging anyway.\nMark work-in-progress PRs as drafts To clearly indicate a PR is still under development and not yet ready for review, mark it as a draft. It is not necessary to modify PR summaries or commit messages (e.g. \u0026ldquo;WIP\u0026rdquo;, \u0026ldquo;DO NOT MERGE\u0026rdquo;). Keeping the same PR summary keeps email notifications threaded, and using the commit message you plan to merge will allow gitlint to verify it. PRs should typically be marked as drafts if any CI is failing that the author can fix before asking for code review.\nPlease do this when opening the PR: instead of clicking on the “Create pull request” button, click on the drop-down arrow next to it, and select “Create draft pull request”. This will avoid notifying code owners; they will be notified when the PR is marked as ready for review.\nUse private forks for debugging PRs by running CI If a PR is not expected to pass CI but the author wants to see the results to enable development, use a personal fork to run CI. This avoids clogging the GitHub Actions job queue of the Submariner-io GitHub Organization. After the same git push to your personal fork you\u0026rsquo;d typically do for a PR, simply choose your fork as the \u0026ldquo;base repository\u0026rdquo; of the PR in GitHub\u0026rsquo;s \u0026ldquo;Open a pull request\u0026rdquo; UI. Make sure your fork\u0026rsquo;s main branch is up-to-date. After creating the PR, CI will trigger as usual but the jobs will count towards your personal queue. You will need to open a new PR against the main repository once your proposed change is ready for review.\nManage dependency among pull requests If a PR (child) is dependent on another PR (parent), irrespective of the project, comment on the child PR with the parent PR\u0026rsquo;s number with Depends on \u0026lt;Parent PR number\u0026gt; or depends on \u0026lt;Parent PR number\u0026gt;. This will trigger a PR Dependencies/Check Dependencies workflow. The workflow will add a dependent label to the child PR. The workflow will fail until the parent PR is merged and will pass once the parent PR is merged. This will prevent merging the child PR until the parent PR is merged.\nTest new functionality As new functionality is added, tests of that functionality should be added to automated test suites. As far as possible, such tests should be added in the same PR that adds the feature.\nFull end-to-end testing of new pull requests On some repositories, full E2E testing of pull requests will be done once a label ready-to-test has been assigned to the request. The label will be automatically assigned once the PR reaches the necessary number of approvals.\nYou can assign this label manually to the PR in order to trigger the full E2E test suite.\n"
},
{
	"uri": "/development/shipyard/advanced/",
	"title": "Advanced Features",
	"tags": [],
	"description": "",
	"content": "Shipyard has many advanced features to use in consuming projects.\nTo utilize an advanced feature in a project consuming Shipyard, a good practice is to change the project\u0026rsquo;s Makefile to have the advanced logic that is always needed. Any variable functionality can then be passed as desired in the command line.\nImage Building Helper Script Shipyard ships an image building script build_image.sh which can be used to build the image(s) that you require. The script has built in caching capabilities to speed up local and pull request CI builds, by utilizing docker\u0026rsquo;s ability to reuse layers from a cached image.\nThe script accepts several flags:\n tag: The tag to set for the local image (defaults to dev). repo: The repo portion to use for the image name. image (-i): The image name itself. dockerfile (-f): The Dockerfile to build the image from. [no]cache: Turns the caching on (or off).  For example, to build the submariner image use:\n${SCRIPTS_DIR}/build_image.sh -i submariner -f package/Dockerfile Deployment Scripts Features Per Cluster Settings Shipyard supports specifying different settings for each deployed cluster. The settings are sent to supporting scripts using a --settings flag.\nThe settings are specified in a YAML file, where default and per cluster settings can be provided. All clusters are listed under the clusters key, and each cluster can have specific deployment settings. All cluster specific settings (except broker) can be specified on the root of the settings file to determine defaults.\nThe possible settings are:\n broker: Special key to mark the broker, set to anything to select a broker. By default, the first cluster is selected. cni: Which CNI to deploy on the cluster, currently supports weave and ovn (leave empty or unset for kindnet). nodes: A space separated list of nodes to deploy, supported types are control-plane and worker. submariner: If Submariner should be deployed, set to true. Otherwise, leave unset (or set to false explicitly).  For example, a basic settings file that deploys a couple of clusters with weave CNI:\ncni: weave submariner: true nodes: control-plane worker worker clusters: cluster1: cluster2: The following settings file deploys two clusters with one control node and two workers, with weave and Submariner. The third cluster will host the broker and as such needs no CNI, only a worker node, and no Submariner deployment:\ncni: weave submariner: true nodes: control-plane worker worker clusters: cluster1: cluster2: cluster3: broker: true cni: submariner: false nodes: control-plane Clusters Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make clusters via a make variable CLUSTERS_ARGS (or an environment variable with the same name). These flags affect how the clusters are deployed (and possibly influence how Submariner works).\nFlags of note:\n  k8s_version: Allows to specify the Kubernetes version that kind will deploy. Available versions can be found here.\nmake clusters CLUSTERS_ARGS=\u0026#39;--k8s_version 1.18.0\u0026#39;   globalnet: When set, deploys the clusters with overlapping Pod \u0026amp; Service CIDRs to simulate this scenario.\nmake clusters CLUSTERS_ARGS=\u0026#39;--globalnet\u0026#39;   Submariner Deployment Customization It\u0026rsquo;s possible to supply extra flags when calling make deploy via a make variable DEPLOY_ARGS (or an environment variable with the same name). These flags affect how Submariner is deployed on the clusters.\nSince deploy relies on clusters then effectively you could also specify CLUSTERS_ARGS to control the cluster deployment (provided the cluster hasn\u0026rsquo;t been deployed yet).\nFlags of note (see the flags defined in deploy.sh for the full list):\n  deploytool: Specifies the deployment tool to use: operator (default), helm, bundle or ocm.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator\u0026#39;   deploytool_broker_args: Any extra arguments to pass to the deploy tool when deploying the broker.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_broker_args \u0026#34;--components service-discovery,connectivity\u0026#34;\u0026#39;   deploytool_submariner_args: Any extra arguments to pass to the deploy tool when deploying Submariner.\nmake deploy DEPLOY_ARGS=\u0026#39;--deploytool operator --deploytool_submariner_args \u0026#34;--cable-driver wireguard\u0026#34;\u0026#39;   As shown above, arguments can be passed directly to the Broker and Submariner. The deploy script also has flags that group common options together for easier user experience. For example, the service_discovery flag in deploy.sh will handle the --components service-discovery,connectivity flags mentioned above. Other examples:\n  globalnet: When set, deploys Submariner with the globalnet controller, and assigns a unique Global CIDR to each cluster.\nmake deploy DEPLOY_ARGS=\u0026#39;--globalnet\u0026#39;   cable_driver: Override the default cable driver to configure the tunneling method for connections between clusters.\nmake deploy DEPLOY_ARGS=\u0026#39;--cable_driver wireguard\u0026#39;   Example: Passing Deployment Variables As an example, in order to deploy with Lighthouse and support both Operator and Helm deployments, one can add this snippet to the Makefile:\nifeq ($(deploytool),operator) DEPLOY_ARGS += --deploytool operator --deploytool_broker_args \u0026#39;--components service-discovery,connectivity\u0026#39; else DEPLOY_ARGS += --deploytool helm --deploytool_broker_args \u0026#39;--set submariner.serviceDiscovery=true\u0026#39; --deploytool_submariner_args \u0026#39;--set submariner.serviceDiscovery=true,lighthouse.image.repository=localhost:5000/lighthouse-agent,serviceAccounts.lighthouse.create=true\u0026#39; endif In such a case, the call to deploy the environment would look like this:\nmake deploy [deploytool=operator] Note that deploytool is a variable used to determine the tool to use, but isn\u0026rsquo;t passed to or used by Shipyard.\n"
},
{
	"uri": "/getting-started/quickstart/openshift/aws/",
	"title": "On AWS",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two OpenShift Container Platform (OCP) clusters on AWS with full stack automation, also known as installer-provisioned infrastructure (IPI). Once the OpenShift clusters are deployed, we deploy Submariner with Service Discovery to interconnect the two clusters. Note that this guide focuses on Submariner deployment on clusters with non-overlapping Pod and Service CIDRs. For connecting clusters with overlapping CIDRs, please refer to the Submariner with Globalnet guide.\nPrerequisites Before we begin, the following tools need to be downloaded and added to your $PATH:\n OpenShift installer, pull secret, and command line interface. All can be downloaded from here. AWS CLI which can be downloaded from here.  Please ensure that the tools you downloaded above are compatible with your OpenShift Container Platform version. For more information, please refer to the official OpenShift documentation.\n  Setup Your AWS Profile Configure the AWS CLI with the settings required to interact with AWS. These include your security credentials, the default AWS Region, and the default output format:\n$ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text  Create and Deploy cluster-a In this step you will deploy cluster-a using the default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nCreate and Deploy cluster-b In this step you will deploy cluster-b, modifying the default IP CIDRs to avoid IP address conflicts with cluster-a. You can change the IP addresses block and prefix based on your requirements. For more information on IPv4 CIDR conversion, please check this page.\nIn this example, we will use the following IP ranges:\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the Pod network CIDR from 10.128.0.0/14 to 10.132.0.0/14:\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the Service network CIDR from 172.30.0.0/16 to 172.31.0.0/16:\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy the cluster:\nopenshift-install create cluster --dir cluster-b When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nInstall subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nPrepare AWS Clusters for Submariner Submariner Gateway nodes need to be able to accept traffic over UDP ports (4500 and 4490 by default). Submariner also uses UDP port 4800 to encapsulate traffic from the worker and master nodes to the Gateway nodes, and TCP port 8080 to retrieve metrics from the Gateway nodes. Additionally, the default OpenShift deployment does not allow assigning an elastic public IP to existing worker nodes, which may be necessary on one end of the tunnel connection.\nsubctl cloud prepare is a command designed to update your OpenShift installer provisioned infrastructure for Submariner deployments, handling the requirements specified above.\nThe default EC2 instance type for the Submariner gateway node is c5d.large, optimized for better CPU which is found to be a bottleneck for IPsec and Wireguard drivers. Please ensure that the AWS Region you deploy to supports this instance type. Alternatively, you can choose to deploy using a different instance type.\n Prepare OpenShift-on-AWS cluster-a for Submariner:\nexport KUBECONFIG=cluster-a/auth/kubeconfig subctl cloud prepare aws --ocp-metadata path/to/cluster-a/metadata.json Prepare OpenShift-on-AWS cluster-b for Submariner:\nexport KUBECONFIG=cluster-b/auth/kubeconfig subctl cloud prepare aws --ocp-metadata path/to/cluster-b/metadata.json Note that certain parameters, such as the tunnel UDP port and AWS instance type for the gateway, can be customized. For example:\nsubctl cloud prepare aws --ocp-metadata path/to/metadata.json --natt-port 4501 --gateway-instance m4.xlarge Submariner can be deployed in HA mode by setting the gateways flag:\nsubctl cloud prepare aws --ocp-metadata path/to/metadata.json --gateways 3 Install Submariner with Service Discovery To install Submariner with multi-cluster Service Discovery follow the steps below:\nUse cluster-a as Broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the Broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-b.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the Headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification The contexts on both config files are named admin and need to be modified before running the verify command. Here is how this can be done using yq:\nyq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-a\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-a/auth/kubeconfig yq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-b\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-b/auth/kubeconfig This will perform automated verifications between the clusters.\nexport KUBECONFIG=cluster-a/auth/kubeconfig:cluster-b/auth/kubeconfig subctl verify --kubecontexts cluster-a,cluster-b --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/getting-started/quickstart/openshift/gcp-lb/",
	"title": "On GCP (LoadBalancer mode)",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two OpenShift Container Platform (OCP) clusters on GCP leveraging a cloud network load balancer service in front of the Submariner gateways.\nThe main benefit of this mode is that there is no need to dedicate specialized nodes with a public IP address to act as gateways. The administrator only needs to manually label any existing node or nodes in each cluster as Submariner gateways, and the submariner-operator will take care of creating a LoadBalancer type Service pointing to the active Submariner gateway.\nPlease note that this mode is still experimental and may need more testing. For example we haven\u0026rsquo;t measured the impact on HA failover times.\n Prerequisites Before we begin, the following tools need to be downloaded and added to your $PATH:\n OpenShift installer, pull secret, and command line interface. All can be downloaded from here. GCP CLI which can be downloaded from here.  Please ensure that the tools you downloaded above are compatible with your OpenShift Container Platform version. For more information, please refer to the official OpenShift documentation.\n  Setup Your GCP Profile Configure the GCP Credentials like project_id, private_key etc in ~/.gcp/osServiceAccount.json file. Please refer to the official doc for detailed instructions Create and Deploy cluster-a In this step you will deploy cluster-a using the default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nCreate and Deploy cluster-b In this step you will deploy cluster-b, modifying the default IP CIDRs to avoid IP address conflicts with cluster-a. You can change the IP addresses block and prefix based on your requirements. For more information on IPv4 CIDR conversion, please check this page.\nIn this example, we will use the following IP ranges:\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the Pod network CIDR from 10.128.0.0/14 to 10.132.0.0/14:\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the Service network CIDR from 172.30.0.0/16 to 172.31.0.0/16:\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy the cluster:\nopenshift-install create cluster --dir cluster-b When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nInstall subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nPrepare GCP Clusters for Submariner Submariner Gateway nodes need to be able to accept traffic over UDP ports (4500 and 4490 by default). Submariner also uses UDP port 4800 to encapsulate traffic from the worker and master nodes to the Gateway nodes, and TCP port 8080 to retrieve metrics from the Gateway nodes. Additionally, the default OpenShift deployment does not allow assigning an elastic public IP to existing worker nodes, which may be necessary on one end of the tunnel connection.\nsubctl cloud prepare is a command designed to update your OpenShift installer provisioned infrastructure for Submariner deployments, handling the requirements specified above.\nRun the command for cluster-a:\nexport KUBECONFIG=cluster-a/auth/kubeconfig subctl cloud prepare gcp --ocp-metadata cluster-a/metadata.json Run the command for cluster-b:\nexport KUBECONFIG=cluster-b/auth/kubeconfig subctl cloud prepare gcp --ocp-metadata cluster-b/metadata.json Install Submariner with Service Discovery To install Submariner with multi-cluster Service Discovery follow the steps below:\nUse cluster-a as Broker subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig Join cluster-a and cluster-b to the Broker subctl join --load-balancer --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --load-balancer --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-b.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the Headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification The contexts on both config files are named admin and need to be modified before running the verify command. Here is how this can be done using yq:\nyq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-a\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-a/auth/kubeconfig yq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-b\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-b/auth/kubeconfig This will perform automated verifications between the clusters.\nexport KUBECONFIG=cluster-a/auth/kubeconfig:cluster-b/auth/kubeconfig subctl verify --kubecontexts cluster-a,cluster-b --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/community/contributor-roles/",
	"title": "Contributor Roles",
	"tags": [],
	"description": "",
	"content": "This is a stripped-down version of the Kubernetes Community Membership process.\nAlthough we aspire to follow the Kubernetes process, some parts are not currently relevant to our structure or possible with our tooling:\n The SIG and subproject abstraction layers don\u0026rsquo;t apply to Submariner. Submariner is treated as a single project with file-based commit rights, not a \u0026ldquo;project\u0026rdquo; per repository. We hope to eventually move to Kubernetes OWNERS and Prow, but until we do so we can\u0026rsquo;t support advanced role-based automation (reviewers vs approvers; PR workflow commands like /okay-to-test, /lgtm, /approved). Project Owners are given responsibility for some tasks that are handled by dedicated teams in Kubernetes (security responses, Code of Conduct violations, and managing project funds). Submariner aspires to create dedicated teams for these tasks as the community grows.   This doc outlines the various responsibilities of contributor roles in Submariner.\n   Role Responsibilities Requirements Defined by     Member Active contributor in the community Sponsored by 2 committers, multiple contributions to the project Submariner GitHub org member   Committer Approve contributions from other members History of review and authorship CODEOWNERS file entry   Owner Set direction and priorities for the project Demonstrated responsibility and excellent technical judgement for the project Submariner-owners GitHub team member and *entry in all CODEOWNERS files    New Contributors New contributors should be welcomed to the community by existing members, helped with PR workflow, and directed to relevant documentation and communication channels.\nWe require every contributor to certify that they are legally permitted to contribute to our project. A contributor expresses this by consciously signing their commits, and by this act expressing that they comply with the Developer Certificate Of Origin.\nEstablished Community Members Established community members are expected to demonstrate their adherence to the principles in this document, familiarity with project organization, roles, policies, procedures, conventions, etc., and technical and/or writing ability. Role-specific expectations, responsibilities, and requirements are enumerated below.\nMember Members are continuously active contributors in the community. They can have issues and PRs assigned to them and participate through GitHub teams. Members are expected to remain active contributors to the community.\nDefined by: Member of the Submariner GitHub organization.\nMember Requirements  Enabled two-factor authentication on their GitHub account Have made multiple contributions to the project or community. Contribution may include, but is not limited to:  Authoring or reviewing PRs on GitHub Filing or commenting on issues on GitHub Contributing to community discussions (e.g. meetings, Slack, email discussion forums, Stack Overflow)   Subscribed to submariner-dev@googlegroups.com Have read the community and development guides Actively contributing Sponsored by 2 committers. Note the following requirements for sponsors:  Sponsors must have close interactions with the prospective member - e.g. code/design/proposal review, coordinating on issues, etc. Sponsors must be committers in at least 1 CODEOWNERS file either in any repo in the Submariner org   Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the member template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Member Responsibilities and Privileges  Responsive to issues and PRs assigned to them Responsive to mentions of teams they are members of Active owner of code they have contributed (unless ownership is explicitly transferred)  Code is well tested Tests consistently pass Addresses bugs or issues discovered after code is accepted   They can be assigned to issues and PRs, and people can ask members for reviews  Note: Members who frequently contribute code are expected to proactively perform code reviews and work towards becoming a committer.\nMembers can be removed by stepping down or by two thirds vote of Project Owners.\nCommitters Committers are able to review code for quality and correctness on some part of the project. They are knowledgeable about both the codebase and software engineering principles.\nUntil automation supports approvers vs reviewers: They also review for holistic acceptance of a contribution including: backwards / forwards compatibility, adhering to API and flag conventions, subtle performance and correctness issues, interactions with other parts of the system, etc.\nDefined by: Entry in an CODEOWNERS file in a repo owned by the Submariner project.\nCommitter status is scoped to a part of the codebase.\nCommitter Requirements The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Member for at least 3 months Primary reviewer for at least 5 PRs to the codebase Reviewed at least 20 substantial PRs to the codebase Knowledgeable about the codebase Sponsored by two committers or project owners  With no objections from other committers or project owners   May either self-nominate or be nominated by a committer/owner Open an issue against the submariner-io/submariner repo  Ensure your sponsors are @mentioned on the issue Complete every item on the checklist (preview the current version of the committer template) Make sure that the list of contributions included is representative of your work on the project   Have your sponsoring committers/owners reply confirmation of sponsorship: +1 Once your sponsors have responded, your request will be reviewed. Any missing information will be requested.  Committer Responsibilities and Privileges The following apply to the part of codebase for which one would be a committer in an CODEOWNERS file:\n Responsible for project quality control via code reviews  Focus on code quality and correctness, including testing and factoring Until automation supports approvers vs reviewers: Focus on holistic acceptance of contribution such as dependencies with other features, backwards / forwards compatibility, API and flag definitions, etc   Expected to be responsive to review requests as per community expectations Assigned PRs to review related to project of expertise Assigned test bugs related to project of expertise Granted \u0026ldquo;read access\u0026rdquo; to submariner repo May get a badge on PR and issue comments Demonstrate sound technical judgement Mentor contributors and reviewers  Committers can be removed by stepping down or by two thirds vote of Project Owners.\nProject Owner Project owners are the technical authority for the Submariner project. They MUST have demonstrated both good judgement and responsibility towards the health the project. Project owners MUST set technical direction and make or approve design decisions for the project - either directly or through delegation of these responsibilities.\nDefined by: Member of the submariner-owners GitHub team and *entry in all CODEOWNERS files.\nOwner Requirements Unlike the roles outlined above, the owners of the project are typically limited to a relatively small group of decision makers and updated as fits the needs of the project.\nThe following apply to people who would be an owner:\n Deep understanding of the technical goals and direction of the project Deep understanding of the technical domain of the project Sustained contributions to design and direction by doing all of:  Authoring and reviewing proposals Initiating, contributing and resolving discussions (emails, GitHub issues, meetings) Identifying subtle or complex issues in designs and implementation PRs   Directly contributed to the project through implementation and / or review  Owner Removal and Future Elected Governance Removal of Project Owners is currently frozen except for stepping down or violations of the Code of Conduct. This is a temporary governance step to define a removal process for extreme cases while protecting the project from dominance by a company. Once the Submariner community is diverse enough to replace Project Owners with an elected governance system, the project should do so. If the project hasn\u0026rsquo;t replaced Project Owners with elected governance by June 1st 2023, and if there are committers from at least three different companies, the project defaults to replacing Project Owners with a Technical Steering Committee elected by OpenDaylight\u0026rsquo;s TSC Election System with a single Committer at Large Represented Group (defined below) and a 49% company cap.\nMin Seats: 5 Max Seats: 5 Voters: Submariner Committers Duplicate Voter Strategy: Vote-per-Person Owner Responsibilities and Privileges The following apply to people who would be an owner:\n Make and approve technical design decisions for the project Set technical direction and priorities for the project Define milestones and releases Mentor and guide committers and contributors to the project Ensure continued health of project  Adequate test coverage to confidently release Tests are passing reliably (i.e. not flaky) and are fixed when they fail   Ensure a healthy process for discussion and decision making is in place Work with other project owners to maintain the project\u0026rsquo;s overall health and success holistically Receive security disclosures and ensure an adequate response. Receive reports of Code of Conduct violations and ensure an adequate response. Decide how funds raised by the project are spent.  "
},
{
	"uri": "/operations/deployment/helm/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": "Deploying with Helm Installing Helm The latest Submariner charts require Helm 3; once you have that, run\nexport KUBECONFIG=\u0026lt;kubeconfig-of-broker\u0026gt; helm repo add submariner-latest https://submariner-io.github.io/submariner-charts/charts Exporting environment variables needed later export BROKER_NS=submariner-k8s-broker export SUBMARINER_NS=submariner-operator export SUBMARINER_PSK=$(LC_CTYPE=C tr -dc \u0026#39;a-zA-Z0-9\u0026#39; \u0026lt; /dev/urandom | fold -w 64 | head -n 1) Deploying the Broker helm install \u0026#34;${BROKER_NS}\u0026#34; submariner-latest/submariner-k8s-broker \\  --create-namespace \\  --namespace \u0026#34;${BROKER_NS}\u0026#34; Setup more environment variables we will need later for joining clusters.\nexport SUBMARINER_BROKER_CA=$(kubectl -n \u0026#34;${BROKER_NS}\u0026#34; get secrets \\  -o jsonpath=\u0026#34;{.items[?(@.metadata.annotations[\u0026#39;kubernetes\\.io/service-account\\.name\u0026#39;]==\u0026#39;${BROKER_NS}-client\u0026#39;)].data[\u0026#39;ca\\.crt\u0026#39;]}\u0026#34;) export SUBMARINER_BROKER_TOKEN=$(kubectl -n \u0026#34;${BROKER_NS}\u0026#34; get secrets \\  -o jsonpath=\u0026#34;{.items[?(@.metadata.annotations[\u0026#39;kubernetes\\.io/service-account\\.name\u0026#39;]==\u0026#39;${BROKER_NS}-client\u0026#39;)].data.token}\u0026#34; \\  | base64 --decode) export SUBMARINER_BROKER_URL=$(kubectl -n default get endpoints kubernetes \\  -o jsonpath=\u0026#34;{.subsets[0].addresses[0].ip}:{.subsets[0].ports[?(@.name==\u0026#39;https\u0026#39;)].port}\u0026#34;) Joining a cluster This step needs to be repeated for every cluster you want to connect with Submariner.\nexport KUBECONFIG=kubeconfig-of-the-cluster-to-join export CLUSTER_ID=the-id-of-the-cluster export CLUSTER_CIDR=x.x.x.x/x # the cluster\u0026#39;s Pod IP CIDR export SERVICE_CIDR=x.x.x.x/x # the cluster\u0026#39;s Service IP CIDR If your clusters have overlapping IPs (Cluster/Service CIDRs), please set:\nexport GLOBALNET=true export GLOBAL_CIDR=242.x.x.x/x # using an individual non-overlapping # range for each cluster you join. Joining the cluster:\nhelm install submariner-operator submariner-latest/submariner-operator \\  --create-namespace \\  --namespace \u0026#34;${SUBMARINER_NS}\u0026#34; \\  --set ipsec.psk=\u0026#34;${SUBMARINER_PSK}\u0026#34; \\  --set broker.server=\u0026#34;${SUBMARINER_BROKER_URL}\u0026#34; \\  --set broker.token=\u0026#34;${SUBMARINER_BROKER_TOKEN}\u0026#34; \\  --set broker.namespace=\u0026#34;${BROKER_NS}\u0026#34; \\  --set broker.ca=\u0026#34;${SUBMARINER_BROKER_CA}\u0026#34; \\  --set broker.globalnet=\u0026#34;${GLOBALNET}\u0026#34; \\  --set submariner.serviceDiscovery=true \\  --set submariner.cableDriver=libreswan \\ # or wireguard or vxlan --set submariner.clusterId=\u0026#34;${CLUSTER_ID}\u0026#34; \\  --set submariner.clusterCidr=\u0026#34;${CLUSTER_CIDR}\u0026#34; \\  --set submariner.serviceCidr=\u0026#34;${SERVICE_CIDR}\u0026#34; \\  --set submariner.globalCidr=\u0026#34;${GLOBAL_CIDR}\u0026#34; \\  --set submariner.natEnabled=\u0026#34;true\u0026#34; \\ # disable this if no NAT will happen between gateways --set serviceAccounts.globalnet.create=\u0026#34;${GLOBALNET}\u0026#34; \\  --set serviceAccounts.lighthouseAgent.create=true \\  --set serviceAccounts.lighthouseCoreDns.create=true Some image override settings you could use\n--set operator.image.repository=\u0026#34;localhost:5000/submariner-operator\u0026#34; \\  --set operator.image.tag=\u0026#34;local\u0026#34; \\  --set operator.image.pullPolicy=\u0026#34;IfNotPresent\u0026#34; If installing on OpenShift, please also add the Submariner service accounts (SAs) to the privileged Security Context Constraint.\noc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-routeagent oc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-engine Perform automated verification Automated verification of the deployment can be performed by using the verification tests embedded in the subctl command line tool via the subctl verify command.\nInstall subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nRun the verification Ensure your kubeconfigs have different context names for each cluster, e.g. “cluster-a” and “cluster-b”; then run\nKUBECONFIG=cluster-a/auth/kubeconfig:cluster-b/auth/kubeconfig subctl verify --kubecontexts cluster-a,cluster-b --verbose "
},
{
	"uri": "/getting-started/quickstart/managed-kubernetes/",
	"title": "Managed Kubernetes",
	"tags": [],
	"description": "",
	"content": " Google (GKE) Rancher  "
},
{
	"uri": "/getting-started/quickstart/",
	"title": "Quickstart Guides",
	"tags": [],
	"description": "",
	"content": " Sandbox Environment (kind) K3s Managed Kubernetes  Google (GKE) Rancher   OpenShift  On AWS On AWS with Globalnet Hybrid vSphere and AWS   External Network (Experimental)  "
},
{
	"uri": "/getting-started/architecture/gateway-engine/",
	"title": "Gateway Engine",
	"tags": [],
	"description": "",
	"content": "The Gateway Engine component is deployed in each participating cluster and is responsible for establishing secure tunnels to other clusters.\nThe Gateway Engine has a pluggable architecture for the cable engine component that maintains the tunnels. The following implementations are available:\n an IPsec implementation using Libreswan. This is currently the default. an implementation for WireGuard (via the wgctrl library). an un-encrypted tunnel implementation using VXLAN.  The cable driver can be specified via the --cable-driver flag while joining a cluster using subctl. For more information, please refer to the subctl guide.\nWireGuard needs to be installed on Gateway nodes. See the WireGuard installation instructions.\n VXLAN connections are unencrypted by design. This is typically useful for environments in which all of the participating clusters run on-premises, the underlying inter-network fabric is controlled, and in many cases already encrypted by other means. Other common use case is to leverage the VXLAN cable engine over a virtual network peering on public clouds (for e.g, VPC Peering on AWS). In this case, the VXLAN connection will be established on top of a peering link which is provided by the underlying cloud infrastructure and is already secured. In both cases, the expectation is that connected clusters should be directly reachable without NAT.\n Instances of the Gateway Engine run on specifically designated nodes in a cluster of which there may be more than one for fault tolerance. Submariner supports active/passive High Availability for the Gateway Engine component, which means that there is only one active Gateway Engine instance at a time in a cluster. They perform a leader election process to determine the active instance and the others await in standby mode ready to take over should the active instance fail.\nThe Gateway Engine is deployed as a DaemonSet that is configured to only run on nodes labelled with \u0026ldquo;submariner.io/gateway=true\u0026rdquo;.\n The active Gateway Engine communicates with the central Broker to advertise its Endpoint and Cluster resources to the other clusters connected to the Broker, also ensuring that it is the sole Endpoint for its cluster. The Route Agent Pods running in the cluster learn about the local Endpoint and setup the necessary infrastructure to route cross-cluster traffic from all nodes to the active Gateway Engine node. The active Gateway Engine also establishes a watch on the Broker to learn about the active Endpoint and Cluster resources advertised by the other clusters. Once two clusters are aware of each other\u0026rsquo;s Endpoints, they can establish a secure tunnel through which traffic can be routed.\nGateway Failover If the active Gateway Engine fails, another Gateway Engine on one of the other designated nodes will gain leadership and perform reconciliation to advertise its Endpoint and to ensure that it is the sole Endpoint. The remote clusters will learn of the new Endpoint via the Broker and establish a new tunnel. Similarly, the Route Agent Pods running in the local cluster automatically update the route tables on each node to point to the new active Gateway node in the cluster.\nThe impact on datapath for various scenarios in a kind setup are captured in the following spreadsheet.\nGateway Health Check The Gateway Engine continuously monitors the health of connected clusters. It periodically pings each cluster and collects statistics including basic connectivity, round trip time (RTT) and average latency. This information is updated in the Gateway resource. Whenever the Gateway Engine detects that a ping to a particular cluster has failed, its connection status is marked with an error state. Service Discovery uses this information to avoid unhealthy clusters during Service discovery.\nThe health checking feature can be enabled/disabled via an option on the subctl join command.\nLoad Balancer mode The load balancer mode is still experimental, and is yet to be tested in all cloud providers nor in different failover scenarios.\n The load balancer mode is designed to simplify the deployment of Submariner in cloud environments where worker nodes don\u0026rsquo;t have a dedicated public IP available.\nWhen enabled for a cluster during subctl join, the operator will create a LoadBalancer type Service exposing both the encapsulation dataplane port as well as the NAT-T discovery port. This load balancer targets Pods labeled with gateway.submariner.io/status=active and app=submariner-gateway.\nWhen the LoadBalancer mode is enabled, the preferred-server mode is enabled automatically for the cluster, as IPsec is incompatible with the bi-directional connection mode and the load balancers and client/server connectivity is necessary.\nIf a failover occurred, the load balancer would update to the new available and active gateway endpoints.\nPreferred-server mode This mode is specific to the libreswan cable-driver which is based on IPsec. Other cable drivers ignore this setting.\nWhen enabled for a cluster during subctl join, the gateway will try to establish connection with other clusters by configuring the IPsec connection in server mode, and waiting for remote connections.\nRemote clusters will identify the preferred-server mode of this cluster, and attempt the connection. This is useful in environments where on-premises clusters don\u0026rsquo;t have access to port mapping.\nWhen both sides of a connection are in preferred-server mode, they will compare the endpoint cable names to decide which one will be server and which one will be client. When cable names are ordered alphabetically, the first one will be the client, the second one will be the server.\n"
},
{
	"uri": "/operations/nat-traversal/",
	"title": "NAT Traversal",
	"tags": [],
	"description": "",
	"content": "Basic Overview Submariner establishes the dataplane tunnels between clusters over port 4500/UDP by default. This port can be customized per cluster and per gateway and is published as part of the Endpoint objects.\nPublic vs Private IP Endpoint objects publish both a PrivateIP and a PublicIP. The PrivateIP is the IP assigned to an interface on the gateway node where the Endpoint originated. The PublicIP is the source IP for the packets sent from the gateway to the Internet which is discovered by default via ipify.org, or my-ip.io and seeip.org fallbacks.\nAlternative methods can be configured on each gateway Node via the gateway.submariner.io/public-ip annotation:\nkubectl annotate node $GW gateway.submariner.io/public-ip=\u0026lt;resolver\u0026gt;,[resolver...] Resolvers are evaluated one by one, using the result of the first one to succeed. \u0026lt;resolver\u0026gt; should be written in the following form: method:parameter, and the following methods are implemented:\n   Method Parameter Notes     api HTTPS endpoint to contact, for example api.ipify.org The result body is inspected looking for the IP address   lb LoadBalancer Service name in the submariner-operator namespace A network load balancer should be used   ipv4 Fixed IPv4 address used as public IP    dns FQDN DNS entry to be resolved The A entry of the FQDN will be resolved and used    For example, when using a fixed public IPv4 address for a gateway, this can be used:\nkubectl annotate node $GW gateway.submariner.io/public-ip=ipv4:1.2.3.4 Reachability For two gateway Endpoints to connect to one another, at least one of them should be reachable either on its public or private IP address and the firewall configuration should allow the tunnel encapsulation port. If one of the clusters is designated as a preferred server, then only its Endpoint needs to be reachable to the other endpoints. This can be accomplished by joining the cluster in preferred server mode.\nsubctl join --kubeconfig A --preferred-server ... broker_info.subm Each gateway implements a UDP NAT-T discovery protocol where each gateway queries the gateways of the remote clusters on both the public and private IPs in order to determine the most suitable IP and its NAT characteristics to use for the tunnel connections, with a preference for the private IP.\nThis protocol is enabled by default on port 4490/UDP and can assign non default ports by annotating the gateway nodes:\nkubectl annotate node $GW gateway.submariner.io/natt-discovery-port=4490 If the NATT discovery protocol fails to determine reachability between two endpoints then it falls back to the NAT setting specified on join (the natEnabled field of the Submariner object or the --natt parameter of subctl), that is, if NAT is enabled, the public IP is used otherwise the private IP is used.\nIP Selection Algorithm The following flow chart describes the IP selection algorithm:\nPort Selection If the gateways of a cluster don\u0026rsquo;t have public floating or elastic IPs assigned to them then it\u0026rsquo;s recommended to use a separate UDP port for every node marked as a gateway. This will allow eventual port mapping on a router when communicating to clusters on remote sites with no direct routing.\nIf a cluster is behind a router which will NAT the traffic, it\u0026rsquo;s recommended to map the open ports into the gateway node private IPs, see the port mapping section. It could temporarily work without mapping, because most routers when performing NAT to the external network will not randomize or modify the source port of packets, but this will happen as soon as two connections collide over the same source port.\n UDP Dataplane Protocol (IPsec, WireGuard or VXLAN) By default, Submariner uses the 4500/UDP port for the dataplane. This can be changed cluster-wide via the --nattport flag on join although it\u0026rsquo;s possible to specify the port to be used per gateway node:\nkubectl annotate node $GW gateway.submariner.io/udp-port=4501 This allows individual gateways on the cluster to have different port numbers, hence allowing individual port-mapping if a public IP is shared.\nIPsec ESP or UDP Encapsulation IPsec in the Libreswan cable driver will be configured for the more performant ESP protocol whenever possible, which is normally when NAT is not detected and connectivity over the private IP is possible.\nIf your network and routers filter the IP\u0026gt;ESP packets, encapsulation can be forced by using the --force-udp-encaps during subctl join.\nPractical Examples All Private and Routed This is the simplest practical case where all gateways can contact all other gateways via routing on their private IPs and no NAT is needed.\nThe NATT discovery protocol will determine that the private IPs are preferred, and will try to avoid using NAT.\nAll Public Cloud, with Some Private Reachability In this case case, the gateways for clusters A and B have direct reachability over their private IPs (10.0.0.1 and 10.1.0.1) possibly with large MTU capabilities. The same is true for clusters C and D (192.168.0.4 and 192.168.128.4).\nBetween any other pair of clusters reachability is only possible over their public IPs and the IP packets will undergo DNAT + SNAT translation at the border via the elastic or floating IP and also, while on transit via the public network, the MTU will be limited to 1500 bytes or less.\nEndpoints    Endpoint Private IP Public IP     A 10.0.0.1 1.1.1.1   B 10.1.0.1 1.1.1.2   C 192.168.0.4 2.1.1.1   D 192.168.128.4 2.1.1.2    Connections    Left Cluster Left IP Left Port Right Cluster Right IP Right Port NAT     A 10.0.0.1 4500 B 10.1.0.1 4500 no   C 192.168.0.4 4500 D 192.168.128.4 4500 no   A 1.1.1.1 4500 C 2.1.1.1 4500 yes   A 1.1.1.1 4500 D 2.1.1.2 4500 yes   B 1.1.1.2 4500 C 2.1.1.1 4500 yes   B 1.1.1.2 4500 D 2.1.1.2 4500 yes    The default configuration for the NAT-T discovery protocol will detect the IPs to use, make sure that the gateways have port 4490/udp open, as well as the encapsulation port 4500/udp.\nPublic Cloud vs On-Premises In this case case, A \u0026amp; B cluster gateways have direct reachability over their private IPs (10.0.0.1 and 10.1.0.1) possibly with large MTU capabilities. The same is true for the C \u0026amp; D cluster gateways (192.168.0.4 and 192.168.128.4).\nBetween all other cluster pairs reachability is only possible over their public IPs, the IP packets from A \u0026amp; B will undergo DNAT + SNAT translation at the border via the elastic or floating IP, the packets from C \u0026amp; D will undergo SNAT translation to the public IP of the router 2.1.1.1 and also, while on transit via the public network, the MTU will be limited to 1500 bytes or less.\nEndpoints for Public Cloud to On-Premises    Endpoint Private IP Public IP     A 10.0.0.1 1.1.1.1   B 10.1.0.1 1.1.1.2   C 192.168.0.4 2.1.1.1   D 192.168.128.4 2.1.1.1    Connections for Public Cloud to On-Premises    Left Cluster Left IP Left Port Right Cluster Right IP Right Port NAT     A 10.0.0.1 4500 B 10.1.0.1 4500 no   C 192.168.0.4 4501 D 192.168.128.4 4502 no   A 1.1.1.1 4500 C 2.1.1.1 4501 yes   A 1.1.1.1 4500 D 2.1.1.1 4502 yes   B 1.1.1.2 4500 C 2.1.1.1 4501 yes   B 1.1.1.2 4500 D 2.1.1.1 4502 yes    The recommended configuration for the gateways behind the on-premises router which has a single external IP with no IP routing or mapping to the private network is to have a dedicated and distinct port number for the NATT discovery protocol (as well as the encapsulation)\nkubectl annotate node $GWC --kubeconfig C gateway.submariner.io/natt-discovery-port=4491 kubectl annotate node $GWC --kubeconfig C gateway.submariner.io/udp-port=4501 kubectl annotate node $GWD --kubeconfig D gateway.submariner.io/natt-discovery-port=4492 kubectl annotate node $GWD --kubeconfig D gateway.submariner.io/udp-port=4502 # restart the gateways to pick up the new setting for cluster in C D; do kubectl delete pod -n submariner-operator -l app=submariner-gateway --kubeconfig $cluster done  If HA is configured on the on-premise clusters, each gateway behind the 2.1.1.1 router should have a dedicated UDP port. For example if we had two clusters and two gateways on each cluster, four ports would be necessary.\n Router Port Mapping Under this configuration it\u0026rsquo;s important to map the UDP ports on the 2.1.1.1 router to the private IPs of the gateways.\n   External IP Port Internal IP Port Protocol     2.1.1.1 4501 192.168.0.4 4501 UDP   2.1.1.1 4491 192.168.0.4 4491 UDP   2.1.1.1 4502 192.168.128.4 4502 UDP   2.1.1.1 4492 192.168.128.4 4492 UDP    Without port mapping it\u0026rsquo;s entirely possible that the connectivity will be established without issues. This can happen because the router\u0026rsquo;s NAT will not generally modify the source port of the outgoing UDP packets, and future packets arriving on this port will be redirected to the internal IP which initiated connectivity. However if the 2.1.1.1 router randomizes the source port on NAT or if other applications on the internal network were already using the 4501-4502 or 4491-4492 ports, the remote ends would not be able to contact gateway C or D over the expected ports.\n Alternative to Port Mapping If port mapping is not possible, we can enable a server/client model for connections where we designate the clusters with a dedicated public IP or the clusters with the ability to get mapped ports as preferred servers. In this way, only the non-preferred server clusters will initiate connections to the preferred server clusters.\nFor example, given clusters A, B, C, and D, we designate A and B as preferred servers:\nsubctl join --kubeconfig A --preferred-server .... broker_info.subm subctl join --kubeconfig B --preferred-server .... broker_info.subm This means that the gateways for clusters A and B will negotiate which one will be the server based on the Endpoint names. Clusters C and D will connect to clusters A and B as clients. Clusters C and D will connect normally.\nMultiple on-premise sites In this case case, A \u0026amp; B cluster gateways have direct reachability over their private IPs (10.0.0.1 and 10.1.0.1) possibly with large MTU capabilities. The same is true for the C \u0026amp; D cluster gateways (192.168.0.4 and 192.168.128.4).\nBetween all other cluster pairs reachability is only possible over their public IPs, the IP packets from A,B,C \u0026amp; D will undergo SNAT translation at the border with the public network also, while on transit via the public network the MTU will be limited to 1500 bytes or less.\nEndpoints for Multiple On-Premises    Endpoint Private IP Public IP     A 10.0.0.1 1.1.1.1   B 10.1.0.1 1.1.1.1   C 192.168.0.4 2.1.1.1   D 192.168.128.4 2.1.1.1    Connections for Multiple On-Premises    Left Cluster Left IP Left Port Right Cluster Right IP Right Port NAT     A 10.0.0.1 4501 B 10.1.0.1 4502 no   C 192.168.0.4 4501 D 192.168.128.4 4502 no   A 1.1.1.1 4501 C 2.1.1.1 4501 yes   A 1.1.1.1 4501 D 2.1.1.1 4502 yes   B 1.1.1.1 4502 C 2.1.1.1 4501 yes   B 1.1.1.1 4502 D 2.1.1.1 4502 yes    Every gateway must have its own port number for NATT discovery, as well as for encapsulation, and the ports on the NAT gateway should be mapped to the internal IPs and ports of the gateways.\nkubectl annotate node $GWA --kubeconfig A gateway.submariner.io/natt-discovery-port=4491 kubectl annotate node $GWA --kubeconfig A gateway.submariner.io/udp-port=4501 kubectl annotate node $GWB --kubeconfig B gateway.submariner.io/natt-discovery-port=4492 kubectl annotate node $GWB --kubeconfig B gateway.submariner.io/udp-port=4502 kubectl annotate node $GWC --kubeconfig C gateway.submariner.io/natt-discovery-port=4491 kubectl annotate node $GWC --kubeconfig C gateway.submariner.io/udp-port=4501 kubectl annotate node $GWD --kubeconfig D gateway.submariner.io/natt-discovery-port=4492 kubectl annotate node $GWD --kubeconfig D gateway.submariner.io/udp-port=4502 # restart the gateways to pick up the new setting for cluster in A B C D; do kubectl delete pod -n submariner-operator -l app=submariner-gateway --kubeconfig $cluster done  If HA is configured on the on-premises clusters, each gateway behind the routers should have a dedicated UDP port. For example if we had two clusters and two gateways on each network, four ports would be necessary.\n Router Port Mapping for Multiple On-Oremises Under this configuration it\u0026rsquo;s important to map the UDP ports on the 2.1.1.1 router to the private IPs of the gateways.\nOn the 2.1.1.1 router    External IP Port Internal IP Port Protocol     2.1.1.1 4501 192.168.0.4 4501 UDP   2.1.1.1 4491 192.168.0.4 4491 UDP   2.1.1.1 4502 192.168.128.4 4502 UDP   2.1.1.1 4492 192.168.128.4 4492 UDP    On the 1.1.1.1 router    External IP Port Internal IP Port Protocol     1.1.1.1 4501 10.0.0.1 4501 UDP   1.1.1.1 4491 10.0.0.1 4491 UDP   1.1.1.1 4502 10.1.0.1 4502 UDP   1.1.1.1 4492 10.1.0.1 4492 UDP    Without port mapping it\u0026rsquo;s entirely possible that the connectivity will be established without issues, this happens due to the fact that route\u0026rsquo;s NAT will not generally modify the src port of the outgoing UDP packets, and future packets arriving this port will be redirected to the internal IP which initiated connectivity, but if the 2.1.1.1 router randomizes the source port on NAT, or if other applications on the internal network were already using the 4501-4502,4491-4492 ports, the remote ends would not be able to contact gateway C or D over the expected ports.\n Double NAT Traversal In this case case, A \u0026amp; B cluster gateways have direct reachability over their private IPs (10.0.0.1 and 10.1.0.1) possibly with large MTU capabilities, while between cluster C and D (192.168.0.4 and 192.168.0.4 too), reachability over the private IPs is not possible but it would be possible over the private floating IPs 10.2.0.1 and 10.2.0.2. However Submariner is unable to detect such floating IPs.\nEndpoints for Double NAT    Endpoint Private IP Public IP     A 10.0.0.1 1.1.1.1   B 10.1.0.1 1.1.1.1   C 192.168.0.4 2.1.1.1   D 192.168.0.4 2.1.1.1    A problem will exist between C \u0026amp; D because they can\u0026rsquo;t reach each other neither on 2.1.1.1 or ther IPs since the private CIDRs overlap.\nThis is a complicated topology that is still not supported in Submariner. Possible solutions to this could be:\n  Modifying the CIDRs of the virtual networks for clusters C \u0026amp; D, and then peer the virtual routers of those virtual networks to perform routing between C \u0026amp; D. Then C \u0026amp; D would be able to connect over the private IPs to each other.\n  Support manual addition of multiple IPs per gateway, so each Endpoint would simply expose a list of IPs with preference instead of just a Public/Private IP.\n  "
},
{
	"uri": "/community/",
	"title": "Community",
	"tags": [],
	"description": "",
	"content": " Code of Conduct Contributor Roles Getting Help Releases Roadmap  "
},
{
	"uri": "/operations/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Overview You have followed steps in Deployment but something has gone wrong. You\u0026rsquo;re not sure what and how to fix it, or what information to collect to raise an issue. Welcome to the Submariner troubleshooting guide where we will help you get your deployment working again.\nBasic familiarity with the Submariner components and architecture will be helpful when troubleshooting so please review the Architecture section.\nThe guide has been broken into different sections for easy navigation.\nAutomated Troubleshooting Use the subctl utility to automate troubleshooting and collecting debugging information.\nInstall subctl:\ncurl -Ls https://get.submariner.io | VERSION=\u0026lt;your Submariner version\u0026gt; bash Set KUBECONFIG to point at your clusters:\nexport KUBECONFIG=\u0026lt;kubeconfig0 path\u0026gt;:\u0026lt;kubeconfig1 path\u0026gt; Show overview of, and diagnose issues with, each cluster:\nsubctl show all subctl diagnose all Diagnose common firewall issues between a pair of clusters:\nsubctl diagnose firewall inter-cluster \u0026lt;kubeconfig0 path\u0026gt; \u0026lt;kubeconfig1 path\u0026gt;  Merged KUBECONFIGs are currently not supported by subctl diagnose firewall.\n Collect details about an issue you\u0026rsquo;d like help with:\nsubctl gather tar cfz submariner-\u0026lt;timestamp\u0026gt;.tar.gz submariner-\u0026lt;timestamp\u0026gt; When reporting an issue, it may also help to include the information in the bug-report.md template.\nManual Troubleshooting Pre-requisite Before we begin troubleshooting, run subctl version to obtain which version of the Submariner components you are running.\nRun kubectl get services -n \u0026lt;service-namespace\u0026gt; | grep \u0026lt;service-name\u0026gt; to get information about the service you\u0026rsquo;re trying to access. This will provide you with the Service Name, Namespace and ServiceIP. If Globalnet is enabled, you will also need the globalIp of the service by running\nkubectl get globalingressip \u0026lt;service-name\u0026gt;'\nConnectivity Issues Submariner deployment completed successfully but Services/Pods on one cluster are unable to connect to Services on another cluster. This can be due to multiple factors outlined below.\nCheck the Connection Statistics If you are unable to connect to a remote cluster, check its connection status in the Gateway resource.\nkubectl describe Gateway -n submariner-operator\nSample output:\n- endpoint: backend: libreswan cable_name: submariner-cable-cluster1-172-17-0-7 cluster_id: cluster1 healthCheckIP: 10.1.128.0 hostname: cluster1-worker nat_enabled: false private_ip: 172.17.0.7 public_ip: \u0026#34;\u0026#34; subnets: - 100.1.0.0/16 - 10.1.0.0/16 latencyRTT: average: 447.358µs last: 281.577µs max: 5.80437ms min: 158.725µs stdDev: 364.154µs status: connected statusMessage: Connected to 172.17.0.7:4500 - encryption alg=AES_GCM_16, keysize=128 rekey-time=13444 The Gateway Engine uses the Health Check IP of the endpoint to verify connectivity. The connection Status will be marked as error, if it cannot reach this IP, and the Status Message will provide more information about the possible failure reason. It also provides the statistics for the connection.\nService Discovery Issues If you are able to connect to remote service by using ServiceIP or globalIp, but not by service name, it is a Service Discovery Issue.\nService Discovery not working This is good time to familiarize yourself with Service Discovery Architecture if you haven\u0026rsquo;t already.\nCheck ServiceExport for your Service For a Service to be accessible across clusters, you must first export the Service via subctl which creates a ServiceExport resource. Ensure the ServiceExport resource exists and check if its status condition indicates `Exported\u0026rsquo;. Otherwise, its status condition will indicate the reason it wasn\u0026rsquo;t exported.\nkubectl describe serviceexport -n \u0026lt;service-namespace\u0026gt; \u0026lt;service-name\u0026gt;\nNote that you can also use shorthand svcex for serviceexport and svcim for serviceimport.\nSample output:\nName: nginx-demo Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceExport Metadata: Creation Timestamp: 2020-11-25T06:21:01Z Generation: 1 Resource Version: 5254 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/default/serviceexports/nginx-demo UID: 77509e43-8fd1-4173-805c-e03c4581ebbf Status: Conditions: Last Transition Time: 2020-11-25T06:21:01Z Message: Awaiting sync of the ServiceImport to the broker Reason: AwaitingSync Status: False Type: Valid Last Transition Time: 2020-11-25T06:21:01Z Message: Service was successfully synced to the broker Reason: Status: True Type: Valid Events: \u0026lt;none\u0026gt; Check Lighthouse CoreDNS Service All cross-cluster service queries are handled by Lighthouse CoreDNS server. First we check if the Lighthouse CoreDNS Service is running properly.\nkubectl -n submariner-operator get service submariner-lighthouse-coredns\nIf it is running fine, note down the ServiceIP for the next steps. If not, check the logs for an error.\nIf the error is due to a wrong image, run kubectl -n submariner-operator get deployment submariner-lighthouse-coredns and make sure Image is set to quay.io/submariner/lighthouse-coredns:\u0026lt;version\u0026gt; and refers to the correct version.\nFor any other errors, capture the information and raise a new issue.\nIf there\u0026rsquo;s no error, then check if the Lighthouse CoreDNS server is configured correctly. Run kubectl -n submariner-operator describe configmap submariner-lighthouse-coredns and make sure it has following configuration:\nclusterset.local:53 { lighthouse errors health ready } Check CoreDNS Configuration Submariner requires the CoreDNS deployment to forward requests for the domain clusterset.local to the Lighthouse CoreDNS server in the cluster making the query. Ensure this configuration exists and is correct.\nFirst we check if CoreDNS is configured to forward requests for domain clusterset.local to Lighthouse CoreDNS Server in the cluster making the query.\nkubectl -n kube-system describe configmap coredns\nIn the output look for something like this:\nclusterset.local:53 { forward . \u0026lt;lighthouse-coredns-serviceip\u0026gt; ======\u0026gt; ServiceIP of lighthouse-coredns service as noted in previous section } If the entries highlighted above are missing or ServiceIp is incorrect, it means CoreDNS wasn\u0026rsquo;t configured correctly. It can be fixed by running kubectl edit configmap coredns and making the changes manually. You may need to repeat this step on every cluster.\nCheck submariner-lighthouse-agent Next we check if the submariner-lighthouse-agent is properly running. Run kubectl -n submariner-operator get pods submariner-lighthouse-agent and check the status of Pods.\nIf the status indicates the ImagePullBackOff error, run kubectl -n submariner-operator describe deployment submariner-lighthouse-agent and check if Image is set correctly to quay.io/submariner/lighthouse-agent:\u0026lt;version\u0026gt;. If it is and the same error still occurs, raise an issue here or ping us on the community slack channel.\nIf the status indicates any other error, run kubectl -n submariner-operator get pods to get the name of the lighthouse-agent Pod. Then run kubectl -n submariner-operator logs \u0026lt;lighthouse-agent-pod-name\u0026gt; to get the logs. See if there are any errors in the log. If yes, raise an issue with the log contents, or you can continue reading through this guide to troubleshoot further.\nIf there are no errors, grep the log for the service name that you\u0026rsquo;re trying to query as we may need the log entries later for raising an issue.\nCheck ServiceImport resources If the steps above did not indicate an issue, next we check if the ServiceImport resources were properly created for the service you\u0026rsquo;re trying to access. The format of a ServiceImport resources\u0026rsquo;s name is as follows:\n\u0026lt;service-name\u0026gt;-\u0026lt;service-namespace\u0026gt;-\u0026lt;cluster-id\u0026gt;\nRun kubectl get serviceimports --all-namespaces |grep \u0026lt;your-service-name\u0026gt; on the Broker cluster to check if a resource was created for your service. If not, then check the Lighthouse Agent logs on the cluster where service was created and look for any error or warning messages indicating a failure to create the ServiceImport resource for your service. The most common error is Forbidden if the RBAC wasn\u0026rsquo;t configured correctly. Depending on the deployment method used, \u0026lsquo;subctl\u0026rsquo; or \u0026lsquo;helm\u0026rsquo;, it should\u0026rsquo;ve been done for you. Create an issue with relevant log entries.\nIf the ServiceImport resource was created correctly on the Broker cluster, the next step is to check if it exists on the cluster where you\u0026rsquo;re trying to access the service. Follow the same steps as earlier to get the list of the ServiceImport resources and check if the ServiceImport for your service exists. If not, check the logs of the Lighthouse Agent on the cluster where you are trying to access the service. As described earlier, it will most commonly be an issue with RBAC otherwise create an issue with relevant log entries.\nIf the ServiceImport resource was created properly on the cluster, run kubectl -n submariner-operator describe serviceimport \u0026lt;your-serviceimport-name\u0026gt; and check if it has the correct ClusterID and ServiceIP:\nName: nginx-demo-default-cluster2 Namespace: submariner-operator Labels: lighthouse.submariner.io/sourceCluster=cluster2 lighthouse.submariner.io/sourceName=nginx-demo lighthouse.submariner.io/sourceNamespace=default submariner-io/clusterID=cluster2 Annotations: cluster-ip: 100.2.33.171 origin-name: nginx-demo origin-namespace: default API Version: multicluster.x-k8s.io/v1alpha1 Kind: ServiceImport Metadata: Creation Timestamp: 2020-11-25T06:21:02Z Generation: 1 Resource Version: 5312 Self Link: /apis/multicluster.x-k8s.io/v1alpha1/namespaces/submariner-operator/serviceimports/nginx-demo-default-cluster2 UID: a4c4abe0-1c84-4118-ae09-760b26f7fe3c Spec: Ips: 100.2.33.171 Ports: Session Affinity Config: Type: ClusterSetIP Events: \u0026lt;none\u0026gt; For headless Service, you need to check EndpointSlice resource.\nIf the data is not correct, you can manually edit the ServiceImport resource to set the correct IP as a workaround and create an issue with relevant information.\nIf the ServiceImport Ips are correct but still not being returned from DNS queries, check the connectivity to the cluster using subctl show endpoint. The Lighthouse CoreDNS Server only returns IPs from connected clusters.\nCheck EndpointSlice resources For a headless Service, next we check if the EndpointSlice resources were properly created for the service you\u0026rsquo;re trying to access. EndpointSlice resources are created in the same namespace as the source Service. The format of a EndpointSlice resource\u0026rsquo;s name is as follows:\n\u0026lt;service-name\u0026gt;--\u0026lt;cluster-id\u0026gt;\nRun kubectl get endpointslices --all-namespaces |grep \u0026lt;your-service-name\u0026gt; on the Broker cluster to check if a resource was created for your Service. If not, then check the Lighthouse Agent logs on the cluster where the Service was created and look for any error or warning messages indicating a failure to create the ServiceImport resource for your Service. The most common error is Forbidden if the RBAC wasn\u0026rsquo;t configured correctly. This is supposed to be done automatically during deployment so please file an issue with the relevant log entries.\nIf the EndpointSlice resource was created correctly on the Broker cluster, the next step is to check if it exists on the cluster where you\u0026rsquo;re trying to access the Service. Follow the same steps as earlier to get the list of the EndpointSlice resources and check if the EndpointSlice for the Service exists. If not, check the logs of the Lighthouse Agent on the cluster where you are trying to access the Service. As described earlier, it will most commonly be an issue with RBAC so create an issue with relevant log entries.\nIf the EndpointSlice resource was created properly on the cluster, run kubectl -n \u0026lt;your-service-namespace\u0026gt; describe endpointslice \u0026lt;your-endpointslice-name\u0026gt; and check if it has the correct endpoint addresses:\nName: nginx-ss-cluster2 Namespace: default Labels: endpointslice.kubernetes.io/managed-by=lighthouse-agent.submariner.io lighthouse.submariner.io/sourceCluster=cluster2 lighthouse.submariner.io/sourceName=nginx-ss lighthouse.submariner.io/sourceNamespace=default multicluster.kubernetes.io/service-name=nginx-ss-default-cluster2 Annotations: \u0026lt;none\u0026gt; AddressType: IPv4 Ports: Name Port Protocol ---- ---- -------- web 80 TCP Endpoints: - Addresses: 10.242.0.5 -----\u0026gt; Pod IP Conditions: Ready: true Hostname: web-0 -----\u0026gt; Pod hostname Topology: kubernetes.io/hostname=cluster2-worker2 - Addresses: 10.242.224.4 Conditions: Ready: true Hostname: web-1 Topology: kubernetes.io/hostname=cluster2-worker Events: \u0026lt;none\u0026gt; If the Addresses are correct but still not being returned from DNS queries, try querying IPs in a specific cluster by prefixing the query with \u0026lt;cluster-id\u0026gt;. If that returns the IPs correctly, then check the connectivity to the cluster using subctl show endpoint. The Lighthouse CoreDNS Server only returns IPs from connected clusters.\nFor errors querying specific Pods of a StatefulSet, check that the Hostname is correct for the endpoint.\nIf still not working, file an issue with relevant log entries.\n"
},
{
	"uri": "/getting-started/quickstart/openshift/globalnet/",
	"title": "On AWS with Globalnet",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two OpenShift Container Platform (OCP) clusters on AWS with full stack automation, also known as installer-provisioned infrastructure (IPI). Once the OpenShift clusters are deployed, we deploy Submariner to interconnect the two clusters. Since the two clusters share the same Cluster and Service CIDR ranges, Globalnet will be enabled.\nPrerequisites Before we begin, the following tools need to be downloaded and added to your $PATH:\n OpenShift installer, pull secret, and command line interface. All can be downloaded from here. AWS CLI which can be downloaded from here.  Please ensure that the tools you downloaded above are compatible with your OpenShift Container Platform version. For more information, please refer to the official OpenShift documentation.\n Setup Your AWS Profile Configure the AWS CLI with the settings required to interact with AWS. These include your security credentials, the default AWS Region, and the default output format:\n$ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text  Create and Deploy cluster-a In this step you will deploy cluster-a using the default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-a openshift-install create cluster --dir cluster-a When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nCreate and Deploy cluster-b In this step you will deploy cluster-b using the same default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    openshift-install create install-config --dir cluster-b openshift-install create cluster --dir cluster-b When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nInstall subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nPrepare AWS Clusters for Submariner Submariner Gateway nodes need to be able to accept traffic over UDP ports (4500 and 4490 by default). Submariner also uses UDP port 4800 to encapsulate traffic from the worker and master nodes to the Gateway nodes, and TCP port 8080 to retrieve metrics from the Gateway nodes. Additionally, the default OpenShift deployment does not allow assigning an elastic public IP to existing worker nodes, which may be necessary on one end of the tunnel connection.\nsubctl cloud prepare is a command designed to update your OpenShift installer provisioned infrastructure for Submariner deployments, handling the requirements specified above.\nThe default EC2 instance type for the Submariner gateway node is c5d.large, optimized for better CPU which is found to be a bottleneck for IPsec and Wireguard drivers. Please ensure that the AWS Region you deploy to supports this instance type. Alternatively, you can choose to deploy using a different instance type.\n Prepare OpenShift-on-AWS cluster-a for Submariner:\nexport KUBECONFIG=cluster-a/auth/kubeconfig subctl cloud prepare aws --ocp-metadata path/to/cluster-a/metadata.json Prepare OpenShift-on-AWS cluster-b for Submariner:\nexport KUBECONFIG=cluster-b/auth/kubeconfig subctl cloud prepare aws --ocp-metadata path/to/cluster-b/metadata.json Note that certain parameters, such as the tunnel UDP port and AWS instance type for the gateway, can be customized. For example:\nsubctl cloud prepare aws --ocp-metadata path/to/metadata.json --natt-port 4501 --gateway-instance m4.xlarge Submariner can be deployed in HA mode by setting the gateways flag:\nsubctl cloud prepare aws --ocp-metadata path/to/metadata.json --gateways 3 Install Submariner with Service Discovery and Globalnet To install Submariner with multi-cluster service discovery and support for overlapping CIDRs follow the steps below.\nUse cluster-a as Broker with service discovery and globalnet enabled subctl deploy-broker --kubeconfig cluster-a/auth/kubeconfig --globalnet Join cluster-a and cluster-b to the Broker subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --clusterid cluster-a subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --clusterid cluster-b  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-b.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the Headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification The contexts on both config files are named admin and need to be modified before running the verify command. Here is how this can be done using yq:\nyq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-a\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-a/auth/kubeconfig yq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-b\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-b/auth/kubeconfig This will perform automated verifications between the clusters.\nexport KUBECONFIG=cluster-a/auth/kubeconfig:cluster-b/auth/kubeconfig subctl verify --kubecontexts cluster-a,cluster-b --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/getting-started/quickstart/openshift/",
	"title": "OpenShift",
	"tags": [],
	"description": "",
	"content": " On AWS On AWS with Globalnet Hybrid vSphere and AWS  "
},
{
	"uri": "/development/website/",
	"title": "Contributing to the Website",
	"tags": [],
	"description": "",
	"content": "The Submariner documentation website is based on Hugo, Grav, the Hugo Learn theme, and is written in Markdown format.\nYou can always click the Edit this page link at the top right of each page, but if you want to test your changes locally before submitting you can:\n  Fork the submariner-io/submariner-website on GitHub.\n  Check out your copy locally:\ngit clone ssh://git@github.com/\u0026lt;your-user\u0026gt;/submariner-website.git cd submariner-website make server   An instance of the website is now running locally on your machine and is accessible at http://localhost:1313.\n  Edit files in src. The browser should automatically reload so you can view your changes.\n  Eventually commit, push, and pull-request your changes. You can find a good guide about the GitHub workflow here.\n  Your changes will be verified by CI. Check the job results for details of any errors.\n  "
},
{
	"uri": "/operations/deployment/calico/",
	"title": "Calico CNI",
	"tags": [],
	"description": "",
	"content": "Typically, the Kubernetes network plugin (based on kube-proxy) programs iptables rules for Pod networking within a cluster. When a Pod in a cluster tries to access an external IP, the plugin performs specific Network Address Translation (NAT) manipulation on the traffic as it does not belong to the local cluster. Similarly, Submariner also programs certain iptables rules and it requires these rules to be applied prior to the ones programmed by the network plugin. Submariner tries to preserve the source IP of the Pods for cross-cluster communication for visibility, ease of debugging, and security purposes.\nOn clusters deployed with Calico as the network plugin, the rules inserted by Calico take precedence over Submariner, causing issues with cross-cluster communication. To make Calico compatible with Submariner, it needs to be configured, via IPPools, not to perform NAT on the subnets associated with the Pod and Service CIDRs of the remote clusters. Once the IPPools are configured in the clusters, Calico will not perform NAT for the configured CIDRs and allows Submariner to support cross-cluster connectivity.\nWhen using Submariner Globalnet with Calico, please avoid the default globalnet-cidr (i.e., 242.0.0.0/8) as its used internally within Calico. You can explicitly specify a non-overlapping globalnet-cidr while deploying Submariner.\n As an example, consider two clusters, East and West, deployed with the Calico network plugin and connected via Submariner. For cluster East, the Service CIDR is 100.93.0.0/16 and the Pod CIDR is 10.243.0.0/16. For cluster West, they are 100.92.0.0/16 and 10.242.0.0/16. The following IPPools should be created:\nOn East Cluster:\n$ cat \u0026gt; svcwestcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: svcwestcluster spec: cidr: 100.92.0.0/16 natOutgoing: false disabled: true EOF cat \u0026gt; podwestcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: podwestcluster spec: cidr: 10.242.0.0/16 natOutgoing: false disabled: true EOF DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-eastcluster.yaml\u0026gt; calicoctl create -f svcwestcluster.yaml DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-eastcluster.yaml\u0026gt; calicoctl create -f podwestcluster.yaml On West Cluster:\ncat \u0026gt; svceastcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: svceastcluster spec: cidr: 100.93.0.0/16 natOutgoing: false disabled: true EOF cat \u0026gt; podeastcluster.yaml \u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: podeastcluster spec: cidr: 10.243.0.0/16 natOutgoing: false disabled: true EOF DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-westcluster.yaml\u0026gt; calicoctl create -f svceastcluster.yaml DATASTORE_TYPE=kubernetes KUBECONFIG=\u0026lt;kubeconfig-westcluster.yaml\u0026gt; calicoctl create -f podeastcluster.yaml "
},
{
	"uri": "/community/getting-help/",
	"title": "Getting Help",
	"tags": [],
	"description": "",
	"content": "Talk to Us We would love to hear from you, how you are using Submariner, and what we can do to make it better.\nGitHub Check out Submariner\u0026rsquo;s GitHub and consider contributing. Pick up an issue to work on or propose an enhancement by reporting a new issue; once your code is ready to be reviewed, you can propose a pull request. You can find a good guide about the GitHub workflow here.\nSlack Share your ideas in the #submariner channel in Kubernetes\u0026rsquo; Slack. If you need it, you can request an invite to Kubernetes\u0026rsquo; Slack instance.\nCommunity Calendar Submariner\u0026rsquo;s meetings are open to everyone. All meetings are documented on Submariner\u0026rsquo;s Community Calendar. The bi-weekly Submariner Users \u0026amp; Community Meeting (Tuesdays at 5:00pm CET) is a good place to start.\nMailing List Join the submariner-dev or submariner-users mailing lists.\n"
},
{
	"uri": "/development/licenses/",
	"title": "Licenses",
	"tags": [],
	"description": "",
	"content": "Content contributed to the Submariner project must be made available under one of two licenses.\nContributions to projects other than the website These must be made available under the Apache License, version 2.0. Go files must start with the following header:\n/* SPDX-License-Identifier: Apache-2.0 Copyright Contributors to the Submariner project. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ This is enforced by our CI.\nContributions to the website Contributions to the website must be made available under the Creative Commons Attribution 4.0 International license (CC BY 4.0).\n"
},
{
	"uri": "/community/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "v0.11.1 This is a bugfix release:\n All exported headless Services are now given a Globalnet ingress IP when Globalnet is enabled (#1634). Deployments without Globalnet no longer fail because of an invalid GlobalCIDR range (#1668). subctl gather no longer panics when retrieving some Pod container status information (#1684).  v0.11.0 This release mainly focused on stability, bug fixes, and improving the integration between Submariner and Open Cluster Management via the submariner-addon.\n subctl cloud prepare command now supports Google Cloud Platform as well as generic Kubernetes clusters. --ignore-requirements flag was added to subctl join command which ignores Submariner requirements checks.  v0.10.1  Inter-connecting clusters with overlapping CIDRs (Globalnet):  The initial Globanet implementation is deprecated in favor of a new implementation which is more performant and scalable. Globalnet now allows users to explicitly request global IPs at the cluster level, for specific namespaces, or for specific Pods. The new Globalnet implementation is not backward-compatible with the initial Globalnet solution and there is no upgrade path. Globalnet now supports headless Services. The default globalnetCIDR range is changed from 169.254.0.0/16 to 242.0.0.0/8 and each cluster is allocated 64K Global IPs. Globalnet no longer annotates Pods and Services with global IPs but stores this information in ClusterGlobalEgressIP, GlobalEgressIP, and GlobalIngressIP resources.   A new experimental load balancer mode was introduced which is designed to simplify the deployment of Submariner in cloud environments where worker nodes do not have access to a dedicated public IP. In this mode, the Submariner Operator creates a LoadBalancer Service that exposes both the encapsulation dataplane port as well as the NAT-T discovery port. This mode can be enabled by using subctl join --load-balancer. Submariner now supports inter-cluster connections based on the VXLAN protocol. This is useful in cases where encryption, such as with IPsec or WireGuard, is not desired, for example on connections that are already encrypted where the overhead of double encryption is not necessary or performant. This can be enabled by setting the --cable-driver vxlan option during subctl join. Submariner now supports SRV DNS queries for both ClusterIP and Headless Services. This facilitates Service discovery using port name and protocol. For a ClusterIP Service, this resolves to the port number and the domain name. For a Headless Service, the name resolves to multiple answers, one for each Pod backing the Service. Improved the Submariner integration with the Calico CNI. subctl benchmark latency and subctl benchmark throughput now take a new flag --kubecontexts as input instead of two kubeconfig files.  v0.9.1  The --kubecontext flag in subctl commands now works properly. Simplified subctl cloud prepare aws to extract the credentials, infrastructure ID, and region from a local configuration file (if available). The natt-discovery-port and udp-port options can now be set via node annotations.  v0.9.0  The gateway Pod has been renamed from submariner to submariner-gateway. The Helm charts now use Submariner\u0026rsquo;s Operator to deploy and manage Submariner. Broker creation is now managed by the Operator instead of subctl. Each Submariner Pod now has its own service account with appropriate privileges. The Lighthouse CoreDNS server metrics are now exposed. The submariner_connections metric is renamed to submariner_requested_connections. The service-discovery flag of subctl deploy-broker has been deprecated in favor of the components flag. For cases in which cross-cluster connectivity is provided without Submariner, subctl can now just deploy Service Discovery. Improved Service CIDR discovery for K3s deployments. All Submariner Prometheus metrics are now prefixed with submariner_. With Globalnet deployments, Global IPs are now assigned to exported Services only. Previously, Globalnet annotated every Service in the cluster, whether or not it was exported. The name of the CoreDNS custom ConfigMap for service discovery can now be specified on subctl join. The strongswan cable driver that was deprecated in the v0.8.0 release is now removed. The Lighthouse-specific API is now removed in favor of Kubernetes Multicluster Services API. A new tool, subctl diagnose, was added that detects issues with the Submariner deployment that may prevent it from working properly. subctl commands now check if the subctl version is compatible with the deployed Submariner version. New flags, repository and version, were added to the subctl deploy-broker command. New Lighthouse metrics were added that track the number of services imported from and exported to other clusters. subctl show connections now also shows average rtt values. A new tool, subctl gather, was added that collects various information from clusters to aid in troubleshooting a Submariner deployment. Each gateway can now use a different port for IPsec/WireGuard communication via the gateway.submariner.io/udp-port node label. Gateways now implement a NAT-Traversal (NAT-T) discovery protocol that can be enabled via the gateway.submariner.io/natt-discovery-port node label. A cluster can now be configured in IPsec server mode via the preferred-server flag on subctl join.  v0.8.1  Submariner Gateway Health Check is now supported with Globalnet deployments. Support deploying OVN in kind using make clusters using=ovn for E2E testing and development environments. Support debugging the Libreswan cable driver. Fix the cable driver label in the Prometheus latency metrics. Support non-TLS connections for OVN databases. Services can now be recreated without needing to recreate their associated ServiceExport objects. Service Discovery no longer depends on Submariner-provided connectivity. Improved Service Discovery verification suite. The ServiceImport object now includes Port information from the original Service. subctl show now indicates when the target cluster doesn\u0026rsquo;t have Submariner installed.  v0.8.0  Added support for connecting clusters that use the OVNKubernetes CNI plugin in non-Globalnet deployments. Support for Globalnet will be available in a future release. The active Gateway now performs periodic health checks on the connections to remote clusters, updates the Gateway connection status, and adds latency statistics. Gateways now export the following connection metrics on TCP port 8080 which can be used with Prometheus. These are currently only supported for the Libreswan cable driver:  The count of bytes transmitted and received between Gateways. The number of connections between Gateways and their corresponding status. The timestamp of the last successful connection established between Gateways. The RTT latency between Gateways.   The Libreswan cable driver is now the default. The strongSwan cable driver is deprecated and will be removed in a future release. The Lighthouse DNS always returns the IP address of the local exported ClusterIP Service, if available, otherwise it load-balances between the same Services exported from other clusters in a round-robin fashion. Lighthouse has fully migrated to use the proposed Kubernetes Multicluster Services API (ServiceExport and ServiceImport). The Lighthouse-specific API is deprecated and will be removed in a future release. On upgrade from v0.7.0, exported Services will automatically be migrated to the new CRDs. Broker resiliency has been improved. The dataplane is no longer affected in any way if the Broker is unavailable. The subctl benchmark tests now accept a verbose flag to enable full logging. Otherwise only the results are presented.  v0.7.0 StatefulSet support for service discovery and benchmark tooling  This release mainly focused on adding support for StatefulSets in Lighthouse for service discovery and adding new subctl commands to benchmark the network performance across clusters.\n  Lighthouse enhancements/changes:  Added support for accessing individual Pods in a StatefulSet using their host names. A Service in a specific cluster can now be explicitly queried. Removed support for the supercluster.local domain to align with the Kubernetes MultiCluster Service API.   Added new subctl benchmark commands for measuring the throughput and round trip latency between two Pods in separate clusters or within the same cluster. The data path is no longer disrupted when the Globalnet Pod is restarted. The Route Agent component now runs on all worker nodes including those with taints.  When upgrading to 0.7.0 on a cluster already running Submariner, the current state must be cleared:\n Remove the Submariner namespaces: kubectl delete ns submariner-operator submariner-k8s-broker Remove the Submariner cluster roles: kubectl delete clusterroles submariner-lighthouse submariner-operator submariner-operator:globalnet  v0.6.0 Improved Submariner High Availability and various Lighthouse enhancements  This release mainly focused on support for headless Services in Lighthouse, as well as improving Submariner\u0026rsquo;s High Availability (HA).\n The DNS domains have been updated from \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.supercluster.local to \u0026lt;service\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local to align with the change in Kubernetes Multicluster Service API. Both domains will be supported for 0.6.0 but 0.7.0 will remove support for supercluster.local. Please update your deployments and applications.\n  Lighthouse has been enhanced to:  Be aware of the local cluster Gateway connectivity so as not to announce the IP addresses for disconnected remote clusters. Support headless Services for non-Globalnet deployments. Support for Globalnet will be available in a future release. Be aware of a Service\u0026rsquo;s backend Pods so as not to announce IP addresses for Services that have no active Pods. Use Round Robin IP resolution for Services available in multiple clusters. Enable service discovery by default for subctl deployments.   subctl auto-detects the cluster ID from the kubeconfig file\u0026rsquo;s information when possible. Submariner\u0026rsquo;s Pods now shut down gracefully and do proper cleanup which reduces downtime during Gateway failover. The Operator now automatically exports Prometheus metrics; these integrate seamlessly with OpenShift Prometheus if user workload monitoring is enabled, and can be included in any other Prometheus setup. Minimum Kubernetes version is now 1.17. HostNetwork to remote Service connectivity fixes for AWS clusters. The project\u0026rsquo;s codebase quality and readability has been improved using various linters.  v0.5.0 Lighthouse service discovery alignment  This release mainly focused on continuing the alignment of Lighthouse\u0026rsquo;s service discovery support with the Kubernetes Multicluster Services KEP.\n  Lighthouse has been modified per the Kubernetes Multicluster Services KEP as follows:  The MultiClusterService resource has been replaced by ServiceImport. The ServiceExport resource is now updated with status information as lifecycle events occur.   Lighthouse now allows a ServiceExport resource to be created prior to the associated Service. Network discovery was moved from subctl to the Submariner Operator. Several new commands were added to subctl: export service, show versions, show connections, show networks, show endpoints, and show gateways. The subctl info command has been removed in lieu of the new show networks command. The Globalnet configuration has been moved from the broker-info.subm file to a ConfigMap resource stored on the Broker cluster. Therefore, the new subctl cannot be used on brownfield Globalnet deployments where this information was stored as part of broker-info.subm. subctl now supports joining multiple clusters in parallel without having to explicitly specify the globalnet-cidr for the cluster to work around this issue. The globalnet-cidr will automatically be allocated by subctl for each cluster. The separate --operator-image parameter has been removed from subctl join and the --repository and --version parameters are now used for all images. The Submariner Operator status now includes Gateway information. Closed technical requirements for Submariner to become a CNCF project, including Developer Certificate of Origin compliance and additional source code linting.  v0.4.0 Libreswan cable driver, Kubernetes multicluster service discovery  This release is mainly focused on Submariner\u0026rsquo;s Libreswan cable driver implementation, as well as standardizing Lighthouse\u0026rsquo;s service discovery support with the Kubernetes Multicluster Services KEP.\n  Libreswan IPsec cable driver is available for testing and is covered in Submariner\u0026rsquo;s CI. Lighthouse has been modified per the Kubernetes Multicluster Services KEP as follows:  A ServiceExport object needs to be created alongside any Service that is intended to be exported to participant clusters. Supercluster services can be accessed with \u0026lt;service-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.clusterset.local.   Globalnet overlapping CIDR support improvements and bug fixes. Multiple CI improvements implemented from Shipyard. CI tests are now run via GitHub Actions. Submariner\u0026rsquo;s Operator now completely handles the Lighthouse deployment via the ServiceDiscovery CRD. subctl verify is now available for connectivity, service-discovery and gateway-failover.  v0.3.0 Lighthouse Service Discovery without KubeFed  This release is focused on removing the KubeFed dependency from Lighthouse, improving the user experience, and adding experimental WireGuard support as an alternative to IPsec.\n  Lighthouse no longer depends on KubeFed. All metadata exchange is handled over the Broker as MultiClusterService CRs. Experimental WireGuard support has been added as a pluggable CableDriver option in addition to the current default IPsec. Submariner reports the active and passive gateways as a gateway.submariner.io resource. The Submariner Operator reports a detailed status of the deployment. The gateway redundancy/failover tests are now enabled and stable in CI. Globalnet hostNetwork to remote globalIP is now supported. Previously, when a Pod used hostNetworking it was unable to connect to a remote Service via globalIP. A GlobalCIDR can be manually specified when joining a cluster with Globalnet enabled. This enables CI speed optimizations via better parallelism. Operator and subctl are more robust via standard retries on updates. subctl creates a new individual access token for every new joined cluster.  v0.2.0 Overlapping CIDR support  This release is focused on overlapping CIDR support between clusters.\n  Support for overlapping CIDRs between clusters (Globalnet). Enhanced end-to-end scripts, which will be shared between repositories in the Shipyard project (ongoing work). Improved end-to-end deployment by using a local registry. Refactoring to support pluggable drivers (in preparation for WireGuard support).  v0.1.1 Submariner with more light  This release is focused on stability for Lighthouse.\n  Cleaner logging for submariner-engine. Cleaner logging for submariner-route-agent. Fixed issue with wrong token stored in .subm file (submariner-operator#244). Added flag to disable the OpenShift CVO (submariner-operator#235). Fixed several service discovery bugs (submariner-operator#194, submariner-operator#167). Fixed several panics on nil network discovery. Added checks to ensure the CIDRs for joining cluster don\u0026rsquo;t overlap with existing ones. Fix context handling related to service discovery/KubeFed (submariner-operator#180). Use the correct CoreDNS image for OpenShift.  v0.1.0 Submariner with some light  This release has focused on stability, bugfixes and making Lighthouse available as a developer preview via subctl deployments.\n  Several bugfixes and enhancements around HA failover (submariner#346, submariner#348, submariner#332). Migrated to DaemonSets for Submariner gateway deployment. Added support for hostNetwork to remote Pod/Service connectivity (submariner#298). Auto detection and configuration of MTU for vx-submariner, jumbo frames support (submariner#301). Support for updated strongSwan (submariner#288). Better iptables detection for some hosts (submariner#227).   subctl and the Submariner Operator have the following improvements:\n  Support for verify-connectivity checks between two connected clusters. Deployment of Submariner gateways based on DaemonSet instead of Deployment. Rename submariner Pods to submariner-gateway Pods for clarity. Print version details on crash (subctl). Stop storing IPsec key on Broker during deploy-broker, now it\u0026rsquo;s only contained into the .subm file. Version command for subctl. Nicer spinners during deployment (thanks to kind).  v0.0.3 \u0026ndash; KubeCon NA 2019 Submariner has been greatly enhanced to allow administrators to deploy into Kubernetes clusters without the necessity for Layer 2 adjacency for nodes. Submariner now allows for VXLAN interconnectivity between nodes (facilitated by the route agent). subctl was created to make deployment of Submariner easier.\nv0.0.2 Second Submariner release v0.0.1 First Submariner release "
},
{
	"uri": "/operations/known-issues/",
	"title": "Known Issues",
	"tags": [],
	"description": "",
	"content": "General  Minimum supported Kubernetes version is 1.17. Submariner only supports kube-proxy in iptables mode. IPVS is not supported at this time. CoreDNS is supported out of the box for *.clusterset.local service discovery. KubeDNS needs manual configuration. Please refer to the GKE Quickstart Guide for more information. Clusters deployed with the Calico network plug-in require further configuration to be compatible with Submariner. Please refer to the Calico-specific deployment instructions. The Gateway load balancer support is still experimental and needs more testing. Submariner Gateway metrics submariner_gateway_rx_bytes and submariner_gateway_tx_bytes will not be collected when using the VXLAN cable driver.  Globalnet  Currently, Globalnet is not supported with the OVN network plug-in. The subctl benchmark latency command is not compatible with Globalnet deployments at this time.  Deploying with Helm on OpenShift When deploying Submariner using Helm on OpenShift, Submariner needs to be granted the appropriate security context for its service accounts:\noc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-routeagent oc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-gateway oc adm policy add-scc-to-user privileged system:serviceaccount:submariner:submariner-globalnet This is handled automatically in subctl and submariner-addon.\n"
},
{
	"uri": "/development/release-process/",
	"title": "Release Process",
	"tags": [],
	"description": "",
	"content": "These docs describe how to create a Submariner release.\nRelease Concepts Project Release Order Submariner\u0026rsquo;s projects have a dependency hierarchy among their Go libraries and container images, which drives their release order.\nThe Go dependency hierarchy is:\nshipyard \u0026lt;- admiral \u0026lt;- [submariner, lighthouse, cloud-prepare] \u0026lt;- submariner-operator\nThe container image dependency hierarchy is:\nsubctl binary \u0026lt;- shipyard-dapper-base image \u0026lt;- [admiral, cloud-prepare, submariner, lighthouse, submariner-operator]\nProjects in brackets are siblings and do not depend on each other. Dependencies of siblings require all siblings to have aligned versions.\nChoosing Versions Version numbers are required to be formatted following the schema norms where they are used.\n Git: vx.y.z (example: v0.8.0) Containers: x.y.z (example: 0.8.0) Stable branches: release-x.y (example: release-0.8) Milestone releases: Append -mN starting at 1 (example: v0.8.0-m1) Release candidates: Append -rcN starting at 0 (example: v0.8.0-rc0) Single-project testing release: Append -preN starting at 0 (example: v0.8.0-pre0) Release errors: Append .N starting at 1 (example: v0.8.0-m1.1)  Creating Releases The following sections are an ordered series of steps to create a Submariner relese.\nThe release process is mostly automated and uses a YAML file created in the releases repository that describes the release. This file is updated for each step in the release process.\nOnce the changes for a step are reviewed and merged, a CI job will run to create the release(s) for the step and create the required pull requests in preparation for the next step to be reviewed and merged. Once all these pull requests have been merged, you can continue onto the next step.\nFor most projects, after a release is created, another job will be initiated to build release artifacts and publish to Quay. This will take several minutes. You can monitor the progress from the project\u0026rsquo;s main page. In the branches/tags pull-down above the file list heading, select the tag for the new version. A small yellow circle icon should be present to the right of the file list heading which indicates a job is in progress. You can click it to see details. There may be several checks for the job listed but the important one is \u0026ldquo;Release Images\u0026rdquo;. When complete, the indicator icon will change to either a green check mark on success or a red X on failure. A failure likely means the artifacts were not published to Quay, in which case select the failed check, inspect the logs, correct the issue and re-run the job.\nRelease Notes (Final Releases) If you\u0026rsquo;re creating a release meant for general consumption, not a milestone or release candidate, release notes must also be created.\nIt\u0026rsquo;s best to start working with the broader community to create release notes well before the release. Create a PR to start the process, and work with contributors to get everything added and reviewed.\nAutomated Release Creation Process Most of the release can be done in a series of mostly-automated steps. After each step, a Pull Request is sent with the correct YAML content for the release, this needs to be reviewed. Once the pull request is merged, the release process will continue automatically and the next step can be initiated shortly after making sure the release jobs on the releases and any participating repositories are done.\nThe GITHUB_TOKEN environment variable in the shell you\u0026rsquo;re using for the automation must be set to a Personal Access Token you create. The token needs at least public_repo scope for the automated release to work.\nexport GITHUB_TOKEN=\u0026lt;token\u0026gt;  To run the automated release, simply clone the releases repository and execute:\nmake release VERSION=\u0026#34;0.8.0\u0026#34; Make sure to specify the proper version you\u0026rsquo;re intending to release (e.g. for rc0 specify VERSION=\u0026quot;0.8.0-rc0\u0026quot;).\nBy default, the action will try to push to the GitHub account used in the origin remote. If you want to use a specific GitHub account, set GITHUB_ACTOR to the desired account, e.g.\nmake release VERSION=\u0026#34;0.8.0\u0026#34; GITHUB_ACTOR=\u0026#34;octocat\u0026#34;  You can run the process without pushing the PR automatically (obviating the need to set GITHUB_TOKEN). To do so, run the make command with dryrun=true.\n The command runs, gathers the data for the release, updates the release YAML and pushes it for review. Once the review process is done, and the automated release steps on the CI have finished successfully, simply run the same command again to advance to the next stage.\nOnce there isn\u0026rsquo;t anything else to do, the command will inform you. At this point, continue manually with any steps not automated yet, starting with Verify Release.\nManual Release Creation Process These instructions are here as a backup in case the automated creation process has problems, and to serve as a guide.\nStable Releases: Create Stable Branches If you\u0026rsquo;re creating a stable release, you need to create a stable branch for backports in each repository. Milestone releases don\u0026rsquo;t receive backports and therefore don\u0026rsquo;t need branches.\nThe release automation process can create stable branches for you. To do so, navigate to the releases repository.\n  Create a new file in the releases directory (you can copy the example.yaml file). For our example, we\u0026rsquo;ll name it v0.8.0.yaml.\n  Fill in the version/name/branch fields for the release, following the naming scheme below. The status field must be set to branch for this phase.\nversion: v0.8.0 name: 0.8.0 branch: release-0.8 status: branch   Commit your changes, create a pull request, and have it reviewed.\n  Once the pull request is merged, it will trigger a CI job to create the stable branches and pin them to Shipyard on that stable branch.\nStep 1: Create Shipyard Release Navigate to the releases repository.\n  Create a new file in the releases directory (you can copy the example.yaml file). For our example, we\u0026rsquo;ll name it v0.8.0.yaml.\n  Fill in the general fields for the release with the status field set to shipyard. Also add the shipyard component with the hash of the desired or latest commit ID on which to base the release. To obtain the latest, first navigate to the shipyard project. The heading above the file list shows the latest commit on the devel branch including the first 7 hex digits of the commit ID hash.\nIf this is not a final release, set the pre-release field to true (that is uncomment the pre-release line below). This includes release candidates. This is important so it is not labeled as the Latest release in GitHub.\nWhen releasing on a stable branch, make sure to specify the branch as outlined below. Otherwise, omit it.\nversion: v0.8.0 name: 0.8.0 #pre-release: true branch: release-0.8 status: shipyard components: shipyard: \u0026lt;hash goes here\u0026gt;   Commit your changes, create a pull request, and have it reviewed.\n  Verify:\n The releases/release job passed. The Shipyard release was created. The submariner/shipyard-dapper-base image is on Quay.    Pull requests will be created for projects that consume Shipyard to update them to the new version in preparation for the subsequent steps. The automation will leave a comment with a list of them. Make sure all those PRs are merged and their release jobs pass.\n  Step 2: Create Admiral Release Once the pull request to pin Admiral to the new Shipyard version is merged, we can proceed to updating the release YAML file to create an Admiral release.\n  Edit the release yaml file (v0.8.0.yaml). Update the status field to admiral and add the admiral component with the latest commit ID hash:\n-status: shipyard +status: admiral  components: shipyard: \u0026lt;hash goes here\u0026gt; + admiral: \u0026lt;hash goes here\u0026gt;   Commit your changes, create a pull request, and have it reviewed.\n  Verify:\n The releases/release job passed. The Admiral release was created.    Pull requests will be created for projects that consume Admiral to update them to the new version in preparation for the subsequent steps. The automation will leave a comment with a list of them. Make sure all those PRs are merged and their release jobs pass.\n  Step 3: Create cloud-prepare, Lighthouse, and Submariner Releases Once the pull requests to pin the cloud-prepare, Lighthouse and Submariner projects to the new Admiral version are merged:\n  Update the release YAML file status field to projects and add the submariner, cloud-prepare and lighthouse components with their latest commit ID hashes:\n-status: admiral +status: projects  components: shipyard: \u0026lt;hash goes here\u0026gt; admiral: \u0026lt;hash goes here\u0026gt; + cloud-prepare: \u0026lt;hash goes here\u0026gt; + lighthouse: \u0026lt;hash goes here\u0026gt; + submariner: \u0026lt;hash goes here\u0026gt;   Commit your changes, create a pull request, and have it reviewed.\n  Verify:\n The releases/release job passed. The cloud-prepare release was created. The Lighthouse release was created. The Submariner release was created. The submariner/submariner-gateway image is on Quay. The submariner/submariner-route-agent image is on Quay. The submariner/submariner-globalnet image is on Quay. The submariner/submariner-networkplugin-syncer image is on Quay. The submariner/lighthouse-agent image is on Quay. The submariner/lighthouse-coredns image is on Quay.    Automation will create a pull request to pin submariner-operator to the released versions. Make sure that PRs is merged and the release job passes.\n  Step 4: Create Operator and Charts Releases Once the pull request to pin submariner-operator has been merged, we can create the final release:\n  Update the release YAML file status field to released. Add the submariner-operator and submariner-charts components with their latest commit ID hashes.\n-status: projects +status: released  components: shipyard: \u0026lt;hash goes here\u0026gt; admiral: \u0026lt;hash goes here\u0026gt; cloud-prepare: \u0026lt;hash goes here\u0026gt; lighthouse: \u0026lt;hash goes here\u0026gt; submariner: \u0026lt;hash goes here\u0026gt; + submariner-charts: \u0026lt;hash goes here\u0026gt; + submariner-operator: \u0026lt;hash goes here\u0026gt;   Commit your changes, create a pull request, and have it reviewed.\n  Verify:\n The releases/release job passed. The subctl artifacts were released The submariner-operator release was created. The submariner/submariner-operator image is on Quay.    If the release wasn\u0026rsquo;t marked as a pre-release, the releases/release job will also create pull requests in each consuming project to unpin the Shipyard Dapper base image version, that is set it back to devel. For ongoing development we want each project to automatically pick up the latest changes to the base image.\n  Step 5: Verify Release You can follow any of the quick start guides.\nStep 6: Update OperatorHub.io The k8s-operatorhub/community-operators Git repository is a source for sharing Kubernetes Operators with the broader community via OperatorHub.io. OpenShift users will find Submariner\u0026rsquo;s Operator in the official Red Hat catalog.\n  Clone the submariner-operator repository.\n  Make sure you have operator-sdk v1 installed.\n  Generate new package manifests:\nmake packagemanifests VERSION=${new_version} FROM_VERSION=${previous_version} CHANNEL=${channel} For example:\nmake packagemanifests VERSION=0.11.1 FROM_VERSION=0.11.0 CHANNEL=alpha-0.11 Generated package manifests should be in /packagemanifests/${VERSION}/.\n  Fork and clone the k8s-operatorhub/community-operators repository.\n  Update the Kubernetes Operator:\n  Copy the generated package from Step 3 into operators/submariner.\n  Copy the generated package definition /packagemanifests/submariner.package.yaml into operators/submariner/.\n  Test the Operator by running:\nOPP_AUTO_PACKAGEMANIFEST_CLUSTER_VERSION_LABEL=1 OPP_PRODUCTION_TYPE=k8s \\ curl -sL https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-pipeline/ci/latest/ci/scripts/opp.sh | bash \\ -s -- all operators/submariner/${VERSION}   Preview the Operator on OperatorHub.io\n  Once everything is fine, review this checklist and create a new PR on k8s-operatorhub/community-operators.\n  For more details check the full documentation.\n    Step 7: Announce Release E-Mail Once the release and release notes are published, make an announcement to both Submariner mailing lists.\n submariner-dev submariner-users  See the v0.8.0 email example.\nTwitter Synthesize the release notes and summarize the key points in a Tweet. Link to the release notes for details.\n @submarinerio  See the v0.8.0 Tweet example.\n"
},
{
	"uri": "/getting-started/architecture/service-discovery/",
	"title": "Service Discovery",
	"tags": [],
	"description": "",
	"content": "The Lighthouse project provides DNS discovery for Kubernetes clusters connected by Submariner in multi-cluster environments. Lighthouse implements the Kubernetes Multi-Cluster Service APIs.\nArchitecture The below diagram shows the basic Lighthouse architecture:\nLighthouse Agent The Lighthouse Agent runs in every cluster and accesses the Kubernetes API server running in the Broker cluster to exchange service metadata information with other clusters. Local Service information is exported to the Broker and Service information from other clusters is imported.\nAgent Workflow The workflow is as follows:\n Lighthouse Agent connects to the Broker\u0026rsquo;s Kubernetes API server. For every Service in the local cluster for which a ServiceExport has been created, the Agent creates a corresponding ServiceImport resource and exports it to the Broker to be consumed by other clusters. For every ServiceImport resource in the Broker exported from another cluster, it creates a copy of it in the local cluster.  Lighthouse DNS Server The Lighthouse DNS server runs as an external DNS server which owns the domain clusterset.local. CoreDNS is configured to forward any request sent to clusterset.local to the Lighthouse DNS server, which uses the ServiceImport resources that are distributed by the controller for DNS resolution. The Lighthouse DNS server supports queries using an A record and an SRV record.\nWhen a single Service is deployed to multiple clusters, Lighthouse DNS server prefers the local cluster first before routing the traffic to other remote clusters in a round-robin fashion.\n Server Workflow The workflow is as follows:\n A Pod tries to resolve a Service name using the domain name clusterset.local. CoreDNS forwards the request to the Lighthouse DNS server. The Lighthouse DNS server will use its ServiceImport cache to try to resolve the request. If a record exists it will be returned, else an NXDomain error will be returned.  "
},
{
	"uri": "/getting-started/quickstart/openshift/vsphere-aws/",
	"title": "Hybrid vSphere and AWS",
	"tags": [],
	"description": "",
	"content": "This quickstart guide covers the necessary steps to deploy two OpenShift Container Platform (OCP) clusters: one on VMware vSphere with user provisioned infrastructure (UPI) and the other one on AWS with full stack automation, also known as installer-provisioned infrastructure (IPI). Once the OpenShift clusters are deployed, we deploy Submariner with Service Discovery to interconnect the two clusters.\nPrerequisites Before we begin, the following tools need to be downloaded and added to your $PATH:\n OpenShift installer, pull secret, and command line interface. All can be downloaded from here. AWS CLI which can be downloaded from here.  Please ensure that the tools you downloaded above are compatible with your OpenShift Container Platform version. For more information, please refer to the official OpenShift documentation.\n Create and Deploy cluster-a on vSphere (On-Prem) In this step you will deploy cluster-a using the default IP CIDR ranges:\n   Pod CIDR Service CIDR     10.128.0.0/14 172.30.0.0/16    Before you deploy an OpenShift Container Platform cluster that uses user-provisioned infrastructure, you must create the underlying infrastructure. Follow the OpenShift documenation for installation instructions on supported versions of vSphere.\nSubmariner Gateway nodes need to be able to accept IPsec traffic. For on-premises clusters behind corporate firewalls, the default IPsec UDP ports might be blocked. To overcome this, Submariner supports NAT Traversal (NAT-T) with the option to set custom non-standard ports. In this example, we use UDP 4501 and UDP 501. Ensure that those ports are allowed on the gateway node and on the corporate firewall.\nSubmariner also uses VXLAN to encapsulate traffic from the worker and master nodes to the Gateway nodes. Ensure that firewall configuration on the vSphere cluster allows 4800/UDP across all nodes in the cluster in both directions.\n   Protocol Port Description     UDP 4800 Overlay network for inter-cluster traffic   UDP 4501 IPsec traffic   UDP 501 IPsec traffic    When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nCreate and Deploy cluster-b on AWS Setup Your AWS Profile Configure the AWS CLI with the settings required to interact with AWS. These include your security credentials, the default AWS Region, and the default output format:\n$ aws configure AWS Access Key ID [None]: .... AWS Secret Access Key [None]: .... Default region name [None]: .... Default output format [None]: text Create and Deploy cluster-b In this step you will deploy cluster-b, modifying the default IP CIDRs to avoid IP address conflicts with cluster-a. You can change the IP addresses block and prefix based on your requirements. For more information on IPv4 CIDR conversion, please check this page.\nIn this example, we will use the following IP ranges:\n   Pod CIDR Service CIDR     10.132.0.0/14 172.31.0.0/16    openshift-install create install-config --dir cluster-b Change the Pod network CIDR from 10.128.0.0/14 to 10.132.0.0/14:\nsed -i \u0026#39;s/10.128.0.0/10.132.0.0/g\u0026#39; cluster-b/install-config.yaml Change the Service network CIDR from 172.30.0.0/16 to 172.31.0.0/16:\nsed -i \u0026#39;s/172.30.0.0/172.31.0.0/g\u0026#39; cluster-b/install-config.yaml And finally deploy the cluster:\nopenshift-install create cluster --dir cluster-b When the cluster deployment completes, directions for accessing your cluster, including a link to its web console and credentials for the kubeadmin user, display in your terminal.\nInstall subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nPrepare AWS Cluster for Submariner Submariner Gateway nodes need to be able to accept traffic over UDP ports (4500 and 4490 by default). Submariner also uses UDP port 4800 to encapsulate traffic from the worker and master nodes to the Gateway nodes, and TCP port 8080 to retrieve metrics from the Gateway nodes. Additionally, the default OpenShift deployment does not allow assigning an elastic public IP to existing worker nodes, which may be necessary on one end of the tunnel connection.\nsubctl cloud prepare is a command designed to update your OpenShift installer provisioned infrastructure for Submariner deployments, handling the requirements specified above.\nThe default EC2 instance type for the Submariner gateway node is c5d.large, optimized for better CPU which is found to be a bottleneck for IPsec and Wireguard drivers. Please ensure that the AWS Region you deploy to supports this instance type. Alternatively, you can choose to deploy using a different instance type.\n Prepare OpenShift-on-AWS cluster-b for Submariner:\nexport KUBECONFIG=cluster-b/auth/kubeconfig subctl cloud prepare aws --ocp-metadata path/to/cluster-b/metadata.json --natt-port 4501 Note that certain parameters, such as the tunnel UDP port and AWS instance type for the gateway, can be customized. For example:\nsubctl cloud prepare aws --ocp-metadata path/to/metadata.json --natt-port 4501 --gateway-instance m4.xlarge Submariner can be deployed in HA mode by setting the gateways flag:\nsubctl cloud prepare aws --ocp-metadata path/to/metadata.json --gateways 3 Install Submariner with Service Discovery To install Submariner with multi-cluster service discovery, follow the steps below:\nUse cluster-b (AWS) as Broker with Service Discovery enabled subctl deploy-broker --kubeconfig cluster-b/auth/kubeconfig Join cluster-b (AWS) and cluster-a (vSphere) to the Broker subctl join --kubeconfig cluster-b/auth/kubeconfig broker-info.subm --ikeport 501 --nattport 4501 subctl join --kubeconfig cluster-a/auth/kubeconfig broker-info.subm --ikeport 501 --nattport 4501  Verify Deployment To manually verify the deployment, follow the steps below using either a headless or ClusterIP nginx service deployed in cluster-b.\nDeploy ClusterIP Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 subctl export service --namespace default nginx Deploy Headless Service export KUBECONFIG=cluster-b/auth/kubeconfig kubectl -n default create deployment nginx --image=nginxinc/nginx-unprivileged:stable-alpine kubectl -n default expose deployment nginx --port=8080 --cluster-ip=None subctl export service --namespace default nginx Verify Run nettest from cluster-a to access the nginx service:\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- /bin/bash curl nginx.default.svc.clusterset.local:8080 To access a Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt; as follows:\ncurl cluster-b.nginx.default.svc.clusterset.local:8080 Verify StatefulSets A StatefulSet uses a headless Service. Create a web.yaml as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: nginxinc/nginx-unprivileged:stable-alpine ports: - containerPort: 80 name: web Use this yaml to create a StatefulSet web with nginx-ss as the Headless Service.\nexport KUBECONFIG=cluster-a/auth/kubeconfig kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss curl nginx-ss.default.svc.clusterset.local:8080 To access the Service in a specific cluster, prefix the query with \u0026lt;cluster-id\u0026gt;:\ncurl cluster-a.nginx-ss.default.svc.clusterset.local:8080 To access an individual pod in a specific cluster, prefix the query with \u0026lt;pod-hostname\u0026gt;.\u0026lt;cluster-id\u0026gt;:\ncurl web-0.cluster-a.nginx-ss.default.svc.clusterset.local:8080 Perform automated verification The contexts on both config files are named admin and need to be modified before running the verify command. Here is how this can be done using yq:\nyq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-a\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-a/auth/kubeconfig yq e -i \u0026#39;.contexts[0].name = \u0026#34;cluster-b\u0026#34; | .current-context = \u0026#34;cluster-a\u0026#34;\u0026#39; cluster-b/auth/kubeconfig This will perform automated verifications between the clusters.\nexport KUBECONFIG=cluster-a/auth/kubeconfig:cluster-b/auth/kubeconfig subctl verify --kubecontexts cluster-a,cluster-b --only service-discovery,connectivity --verbose  "
},
{
	"uri": "/development/",
	"title": "Development",
	"tags": [],
	"description": "",
	"content": " Backports Building and Testing Code Review Guide Contributing to the Website  Docs Style Guide   Licenses Release Process Security  Security Reporting Container Requirements   Working with Shipyard  Adding Shipyard to a Project Advanced Features    Research notice Please note that this repository is participating in a study into sustainability of open source projects. Data will be gathered about this project\u0026rsquo;s GitHub repositories for approximately the next 12 months, starting from September 2021.\nData collected will include number of contributors, number of PRs, time taken to close/merge these PRs, and issues closed.\nFor more information, please visit the study informational page or download the participant information sheet.\n"
},
{
	"uri": "/development/security/reporting/",
	"title": "Security Reporting",
	"tags": [],
	"description": "",
	"content": "Submariner welcomes and appreciates responsible disclosure of security vulnerabilities.\nIf you know of a security issue with Submariner, please report it to submariner-security@googlegroups.com. Submariner Project Owners receive security disclosures by default. They may share disclosures with others as required to make and propagate fixes.\nSubmariner aspires to follow the Kubernetes security reporting process, but is far too small of a project to implement those practices. Where applicable, Submariner will follow the principles of the Kubernetes process.\n"
},
{
	"uri": "/getting-started/quickstart/external/",
	"title": "External Network (Experimental)",
	"tags": [],
	"description": "",
	"content": "This guide covers experimenting with the external network use case. In this use case, pods running in a Kubernetes cluster can access external applications outside of the cluster and vice versa by using DNS resolution supported by Lighthouse or manually using the Globalnet ingress IPs. In addition to providing connectivity, the source IP of traffic is also preserved.\nThis feature is experimental. The configuration mechanism and observed behavior may change.\n Prerequisites   Prepare:\n Two or more Kubernetes clusters One or more non-cluster hosts that exist in the same network segment to one of the Kubernetes clusters  In this guide, we will use the following Kubernetes clusters and non-cluster host.\n   Name IP Description     cluster-a 192.168.122.26 Single-node cluster   cluster-b 192.168.122.27 Single-node cluster   test-vm 192.168.122.142 Linux host    In this example, everything is deployed in the 192.168.122.0/24 segment. However, it is only required that cluster-a and test-vm are in the same segment. Other clusters, cluster-b and any additional clusters, can be deployed in different segments or even in any other networks in the internet. Also, clusters can be multi-node clusters.\nSubnets of non-cluster hosts should be distinguished from those of the clusters to easily specify the external network CIDR. In this example, cluster-a and cluster-b belong to 192.168.122.0/25 and test-vm belongs to 192.168.122.128/25. Therefore, the external network CIDR for this configuration is 192.168.122.128/25. In test environments with just one host, an external network CIDR like 192.168.122.142/32 can be specified. However, design of the subnets need to be considered when more hosts are used.\n  Choose the Pod CIDR and the Service CIDR for Kubernetes clusters and deply them.\nIn this guide, we will use the following CIDRs:\n   Cluster Pod CIDR Service CIDR     cluster-a 10.42.0.0/24 10.43.0.0/16   cluster-b 10.42.0.0/24 10.43.0.0/16    Note that we will use Globalnet in this guide, therefore overlapping CIDRs are supported. One of the easiest way to create this environment will be to deploy two K3s clusters by the steps described here until \u0026ldquo;Deploy cluster-b on node-b\u0026rdquo;, with modifying deploy commands to just curl -sfL https://get.k3s.io | sh - to use default CIDR.\n  In this configuration, global IPs are used to access between the gateway node and non-cluster hosts, which means packets are sent to IP addresses that are not part of the actual network segment. To make such packets not to be dropped, anti-spoofing rules need to be disabled for the hosts and the gateway node.\n Setup Submariner Ensure kubeconfig files Ensure that kubeconfig files for both clusters are available. This guide assumes cluster-a\u0026rsquo;s kubeconfig file is named kubeconfig.cluster-a and cluster-b\u0026rsquo;s is named kubeconfig.cluster-b.\nInstall subctl Download the subctl binary and make it available on your PATH.\ncurl -Ls https://get.submariner.io | bash export PATH=$PATH:~/.local/bin echo export PATH=\\$PATH:~/.local/bin \u0026gt;\u0026gt; ~/.profile If you have Go and the source code, you can build and install subctl instead:\ncd go/src/submariner-io/submariner-operator go install github.com/submariner-io/submariner-operator/pkg/subctl (and ensure your go/bin directory is on your PATH).\nUse cluster-a as the Broker with Globalnet enabled subctl deploy-broker --kubeconfig kubeconfig.cluster-a --globalnet Label gateway nodes When Submariner joins a cluster to the broker via the subctl join command, it chooses a node on which to install the gateway by labeling it appropriately. By default, Submariner uses a worker node for the gateway; if there are no worker nodes, then no gateway is installed unless a node is manually labeled as a gateway. Since we are deploying k3s all-in-one nodes, there are no worker nodes, so it is necessary to label the single node as a gateway. By default, the node name is the hostname. In this example, the hostnames are \u0026ldquo;cluster-a\u0026rdquo; and \u0026ldquo;cluster-b\u0026rdquo;, respectively.\nExecute the following on cluster-a:\nkubectl label node cluster-a submariner.io/gateway=true Execute the following on cluster-b:\nkubectl label node cluster-b submariner.io/gateway=true Join cluster-a to the Broker with external CIDR added as cluster CIDR Carefully review the CLUSTER_CIDR and EXTERNAL_CIDR and run:\nCLUSTER_CIDR=10.42.0.0/24 EXTERNAL_CIDR=192.168.122.128/25 subctl join --kubeconfig kubeconfig.cluster-a broker-info.subm --clusterid cluster-a --natt=false --clustercidr=${CLUSTER_CIDR},${EXTERNAL_CIDR} Join cluster-b to the Broker subctl join --kubeconfig kubeconfig.cluster-b broker-info.subm --clusterid cluster-b --natt=false Deploy DNS server on cluster-a for non-cluster hosts Create a list of upstream DNS servers as upstreamservers:\nNote that dnsip is the IP of DNS server for the test-vm, which is defined as nameserver in /etc/resolve.conf.\ndnsip=192.168.122.1 lighthousednsip=$(kubectl get svc --kubeconfig kubeconfig.cluster-a -n submariner-operator submariner-lighthouse-coredns -o jsonpath=\u0026#39;{.spec.clusterIP}\u0026#39;) cat \u0026lt;\u0026lt; EOF \u0026gt; upstreamservers server=/svc.clusterset.local/$lighthousednsip server=$dnsip EOF Create configmap of the list:\nexport KUBECONFIG=kubeconfig.cluster-a kubectl create configmap external-dnsmasq -n submariner-operator --from-file=upstreamservers Create a dns.yaml as follows:\napiVersion: apps/v1 kind: Deployment metadata: name: external-dns-cluster-a namespace: submariner-operator labels: app: external-dns-cluster-a spec: replicas: 1 selector: matchLabels: app: external-dns-cluster-a template: metadata: labels: app: external-dns-cluster-a spec: containers: - name: dnsmasq image: registry.access.redhat.com/ubi8/ubi-minimal:latest ports: - containerPort: 53 command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;microdnf install -y dnsmasq; ln -s /upstreamservers /etc/dnsmasq.d/upstreamservers; dnsmasq -k\u0026#34; ] securityContext: capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] volumeMounts: - name: upstreamservers mountPath: /upstreamservers volumes: - name: upstreamservers configMap: name: external-dnsmasq --- apiVersion: v1 kind: Service metadata: namespace: submariner-operator name: external-dns-cluster-a spec: ports: - name: udp port: 53 protocol: UDP targetPort: 53 selector: app: external-dns-cluster-a Use this YAML to create DNS server, and assign global ingress IP:\nkubectl apply -f dns.yaml subctl export service -n submariner-operator external-dns-cluster-a Check global ingress IP:\nkubectl --kubeconfig kubeconfig.cluster-a get globalingressip external-dns-cluster-a -n submariner-operator NAME IP external-dns-cluster-a 242.0.255.251 Set up non-cluster hosts Modify routing for global CIDR on test-vm:\nNote that subm_gw_ip is the gateway node IP of the cluster in the same network segment of the hosts. In the case of the example of this guide, it is the node IP of cluster-a. Also, 242.0.0.0/8 is the default globalCIDR.\nsubm_gw_ip=192.168.122.26 ip r add 242.0.0.0/8 via ${subm_gw_ip} To persist above configuration across reboot, check the document for each Linux distribution. For example, on Centos 7, to set presistent route for eth0, below command is required:\necho \u0026#34;242.0.0.0/8 via ${subm_gw_ip}dev eth0\u0026#34; \u0026gt;\u0026gt; /etc/sysconfig/network-scripts/route-eth0 Modify /etc/resolv.conf to change DNS server for the host on test-vm:\nFor example)\n Before:  nameserver 192.168.122.1  After:  nameserver 242.0.255.251 Check that the DNS server itself can be resolved:\nnslookup external-dns-cluster-a.submariner-operator.svc.clusterset.local Server: 242.0.255.251 Address: 242.0.255.251#53 Name: external-dns-cluster-a.submariner-operator.svc.clusterset.local Address: 10.43.162.46 Verify Deployment Verify Manually Deploy HTTP server on hosts Run on test-vm:\n# Python 2.x: python -m SimpleHTTPServer 80 # Python 3.x: python -m http.server 80 Verify access to External hosts from clusters Create Service, Endpoints, ServiceExport to access the test-vm from cluster-a:\nNote that Endpoints.subsets.addresses needs to be modified to IP of test-vm.\nexport KUBECONFIG=kubeconfig.cluster-a cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: test-vm spec: ports: - protocol: TCP port: 80 targetPort: 80 EOF cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Endpoints metadata: name: test-vm subsets: - addresses: - ip: 192.168.122.142 ports: - port: 80 EOF subctl export service -n default test-vm Check global ingress IP for test-vm, on cluster-a:\nkubectl get globalingressip test-vm NAME IP test-vm 242.0.255.253 Verify access to test-vm from clusters:\nexport KUBECONFIG=kubeconfig.cluster-a kubectl -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- bash curl 242.0.255.253 export KUBECONFIG=kubeconfig.cluster-b kubectl -n default run tmp-shell --rm -i --tty --image quay.io/submariner/nettest -- bash curl 242.0.255.253 On test-vm, check the console log of HTTP server that there are accesses from pods\nCurrently, headless service without selector is not supported for Globalnet, therefore service without selector needs to be used. This feature is under discussion in submariner-io/submariner#1537.\n Currently, DNS resolution for service without selector is not supported, therefore global IPs need to be used to access to the external hosts. This feature is under discussion in submariner-io/lighthouse#603. Note that there is a workaround to make it resolvable by manually creating endpointslice, as described here.\n Verify access to Deployment from non-cluster hosts Create Deployment in cluster-b:\nexport KUBECONFIG=kubeconfig.cluster-b kubectl -n default create deployment nginx --image=k8s.gcr.io/nginx-slim:0.8 kubectl -n default expose deployment nginx --port=80 subctl export service --namespace default nginx From test-vm, verify access:\ncurl nginx.default.svc.clusterset.local Check the console log of HTTP server that there is access from test-vm:\nkubectl logs -l app=nginx Verify access to Statefulset from non-cluster hosts A StatefulSet uses a headless Service. Create a web.yaml file as follows:\napiVersion: v1 kind: Service metadata: name: nginx-ss labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: ports: - port: 80 name: web clusterIP: None selector: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026#34;nginx-ss\u0026#34; replicas: 2 selector: matchLabels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss template: metadata: labels: app.kubernetes.io/instance: nginx-ss app.kubernetes.io/name: nginx-ss spec: containers: - name: nginx-ss image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web Apply the above YAML to create a web StatefulSet with nginx-ss as the headless service:\nexport KUBECONFIG=kubeconfig.cluster-b kubectl -n default apply -f web.yaml subctl export service -n default nginx-ss From test-vm, verify access:\ncurl nginx-ss.default.svc.clusterset.local curl cluster-b.nginx-ss.default.svc.clusterset.local curl web-0.cluster-b.nginx-ss.default.svc.clusterset.local curl web-1.cluster-b.nginx-ss.default.svc.clusterset.local Check the console log of the HTTP server to verify there are accesses from test-vm:\nkubectl logs web-0 kubectl logs web-1 Verify source IP of the access from Statefulset Confirm the global egress IPs for each pod managed by Statefulset:\n From Cluster:  export KUBECONFIG=kubeconfig.cluster-b kubectl get globalingressip | grep web pod-web-0 242.1.255.251 pod-web-1 242.1.255.250  From Hosts:  nslookup web-0.cluster-b.nginx-ss.default.svc.clusterset.local Server: 242.0.255.251 Address: 242.0.255.251#53 Name: web-0.cluster-b.nginx-ss.default.svc.clusterset.local Address: 242.1.255.251 nslookup web-1.cluster-b.nginx-ss.default.svc.clusterset.local Server: 242.0.255.251 Address: 242.0.255.251#53 Name: web-1.cluster-b.nginx-ss.default.svc.clusterset.local Address: 242.1.255.250 Verify the source IP of each access from each pod to test-vm is the same to its global egress IP:\n Access from web-0  export KUBECONFIG=kubeconfig.cluster-b kubectl exec -it web-0 -- bash curl 242.0.255.253 exit  Access from web-1  export KUBECONFIG=kubeconfig.cluster-b kubectl exec -it web-1 -- bash curl 242.0.255.253 exit  Check the console log in test-vm  "
},
{
	"uri": "/operations/cleanup/",
	"title": "Uninstalling Submariner",
	"tags": [],
	"description": "",
	"content": "To properly uninstall Submariner from a cluster, follow the steps below:\nMake sure KUBECONFIG for all participating clusters is exported and all participating clusters are accessible via kubectl.\n   Delete Submariner-related namespaces\nFor each participating cluster, issue the following command:\nkubectl delete namespace submariner-operator For the Broker cluster, issue the following command:\nkubectl delete namespace submariner-k8s-broker For submariner version 0.9 and above, also delete submariner-operator namespace from the Broker cluster by issuing the following command:\nkubectl delete namespace submariner-operator   Delete the Submariner CRDs\nFor each participating cluster, issue the following command:\nfor CRD in `kubectl get crds | grep -iE \u0026#39;submariner|multicluster.x-k8s.io\u0026#39;| awk \u0026#39;{print $1}\u0026#39;`; do kubectl delete crd $CRD; done   Delete Submariner\u0026rsquo;s ClusterRoles and ClusterRoleBindings\nFor each participating cluster, issue the following command:\nroles=\u0026#34;submariner-operator submariner-operator-globalnet submariner-lighthouse submariner-networkplugin-syncer\u0026#34; kubectl delete clusterrole,clusterrolebinding $roles --ignore-not-found   Remove the Submariner gateway labels\nFor each participating cluster, issue the following command:\nkubectl label --all node submariner.io/gateway-   For OpenShift deployments, delete Lighthouse entry from default DNS.\nFor each participating cluster, issue the following command:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: operator.openshift.io/v1 kind: DNS metadata: finalizers: - dns.operator.openshift.io/dns-controller name: default spec: servers: [] EOF This deletes the lighthouse entry from the Data section in Corefile of the configmap.\n#lighthouse-start AUTO-GENERATED SECTION. DO NOT EDIT clusterset.local:53 { forward . 100.3.185.93 } #lighthouse-end Verify that the lighthouse entry is deleted from Corefile of dns-default configmap by running following command on an OpenShift cluster\nkubectl describe configmap dns-default -n openshift-dns For Kubernetes deployments, manually edit the Corefile of coredns configmap and delete the lighthouse entry by issuing below commands\nkubectl edit cm coredns -n kube-system This will also restart the coredns. Below command can also be issued to manually restart coredns.\nkubectl rollout restart -n kube-system deployment/coredns Verify that the lighthouse entry is deleted from Data section in Corefile of dns-default config map by running following command on a Kubernetes cluster\nkubectl describe configmap coredns -n kube-system  Following commands need to be executed from inside the cluster nodes.\n   Remove Submariner\u0026rsquo;s iptables chains\nOn all nodes in each participating cluster, issue the following commands:\niptables --flush SUBMARINER-INPUT iptables -D INPUT $(iptables -L INPUT --line-numbers | grep SUBMARINER-INPUT | awk \u0026#39;{print $1}\u0026#39;) iptables --delete-chain SUBMARINER-INPUT iptables -t nat --flush SUBMARINER-POSTROUTING iptables -t nat -D POSTROUTING $(iptables -t nat -L POSTROUTING --line-numbers | grep SUBMARINER-POSTROUTING | awk \u0026#39;{print $1}\u0026#39;) iptables -t nat --delete-chain SUBMARINER-POSTROUTING iptables -t mangle --flush SUBMARINER-POSTROUTING iptables -t mangle -D POSTROUTING $(iptables -t mangle -L POSTROUTING --line-numbers | grep SUBMARINER-POSTROUTING | awk \u0026#39;{print $1}\u0026#39;) iptables -t mangle --delete-chain SUBMARINER-POSTROUTING ipset destroy SUBMARINER-LOCALCIDRS ipset destroy SUBMARINER-REMOTECIDRS If Globalnet is enabled in the setup, additionally issue the following commands on gateway nodes:\niptables -t nat --flush SUBMARINER-GN-INGRESS iptables -t nat -D PREROUTING $(iptables -t nat -L PREROUTING --line-numbers | grep SUBMARINER-GN-INGRESS | awk \u0026#39;{print $1}\u0026#39;) iptables -t nat --delete-chain SUBMARINER-GN-INGRESS iptables -t nat --flush SUBMARINER-GN-EGRESS iptables -t nat --delete-chain SUBMARINER-GN-EGRESS iptables -t nat -t nat --flush SUBMARINER-GN-MARK iptables -t nat --delete-chain SUBMARINER-GN-MARK   Delete the vx-submariner interface\nOn all nodes in each participating cluster, issue the following command:\nip link delete vx-submariner   If Globalnet release 0.9 (or earlier) is enabled in the setup, issue the following commands to remove the annotations from all the Pods and Services.\nFor each participating cluster, issue the following command:\nfor ns in `kubectl get ns -o jsonpath=\u0026#34;{.items[*].metadata.name}\u0026#34;`; do kubectl annotate pods -n $ns --all submariner.io/globalIp- kubectl annotate services -n $ns --all submariner.io/globalIp- done   "
},
{
	"uri": "/other-resources/",
	"title": "Other Resources",
	"tags": [],
	"description": "",
	"content": "There are multiple presentations and demo recordings about Submariner available online.\nConference Presentations  Here Be Services: Beyond the Cluster Boundary with Multicluster Services, KubeCon NA (10/2021) Multi-Cluster Service Deployments with Operators and KubeCarrier, KubeCon EU (05/2021) ODCN’s Journey to Connecting OpenShift Clusters Securely and Transparently with Submariner, OpenShift Commons at KubeCon EU (05/2021)  Connecting Kubernetes Clusters with Submariner, DevConf.CZ (03/2021) Multicluster Network Connectivity Submariner, Computing on the Edge with Kubernetes (10/2020) Hybrid Cloud and Multicluster Service Discovery, KubeCon China (07/2019) (slides) Solving Multicluster Network Connectivty with Submariner, KubeCon North America (11/2019) (slides)  Demo Recordings  Submariner in 60s (05/2020) Connecting hybrid Kubernetes clusters using Submariner (03/2020) Cross-cluster service discovery in Submariner using Lighthouse (03/2020) Deploying Submariner with subctl (12/2019)  Blogs  Set up Istio Multicluster with Submariner in Red Hat Advanced Cluster Management for Kubernetes (10/2021) Multi-Cluster monitoring with Prometheus and Submariner (09/2021) Connecting stateful applications in multicluster environments with RHACM and Submariner (04/2021) Deep Dive with RHACM and Submariner - Connecting multicluster overlay networks (04/2021) Connecting managed clusters with Submariner in Red Hat Advanced Cluster Management for Kubernetes (04/2021) Geographically Distributed Stateful Workloads Part One: Cluster Preparation (11/2020) Geographically Distributed Stateful Workloads Part Two: CockroachDB (11/2020) Geographically Distributed Stateful Workloads Part Three: Keycloak (06/2021) Geographically Distributed Stateful Workloads Part Four: Kafka (08/2021) Geographically Distributed Stateful Workloads Part Five: YugabyteDB (09/2021) Multicluster Service Discovery in OpenShift with Submariner and Lighthouse (Part 1) (08/2020) Multicluster Service Discovery in OpenShift with Submariner and Lighthouse (Part 2) (08/2020) Kubernetes Multi-Cloud and Multi-Cluster Connectivity with Submariner (02/2020)  SIG Presentations  CNCF SIG-Network introduction to Submariner and consideration for CNCF Sandbox donation (2021/03/18) K8s SIG-Multicluster demo of Submariner\u0026rsquo;s KEP1645 Multicluster Services implementation (2020/09/22) K8s SIG-Multicluster demo of Submariner\u0026rsquo;s multicluster networking deployed by Submariner\u0026rsquo;s Operator and subctl (2019/12/17)  Academic Papers  Kubernetes and the Edge? (10/2020)  If you find additional material that isn\u0026rsquo;t listed here, please feel free to add it to this page by editing it. The website contributing guide is here.\n"
},
{
	"uri": "/getting-started/architecture/globalnet/",
	"title": "Globalnet Controller",
	"tags": [],
	"description": "",
	"content": "Introduction Submariner is a tool built to connect overlay networks of different Kubernetes clusters. These clusters can be on different public clouds or on-premises. An important use case for Submariner is to connect disparate independent clusters into a ClusterSet.\nHowever, by default, a limitation of Submariner is that it doesn\u0026rsquo;t handle overlapping CIDRs (ServiceCIDR and ClusterCIDR) across clusters. Each cluster must use distinct CIDRs that don\u0026rsquo;t conflict or overlap with any other cluster that is going to be part of the ClusterSet.\nThis is largely problematic because most actual deployments use the default CIDRs for a cluster so every cluster ends up using the same CIDRs. Changing CIDRs on existing clusters is a very disruptive process and requires a cluster restart. So Submariner needs a way to allow clusters with overlapping CIDRs to connect together.\nArchitecture To support overlapping CIDRs in connected clusters, Submariner has a component called Global Private Network, Globalnet (globalnet). This Globalnet is a virtual network specifically to support Submariner\u0026rsquo;s multi-cluster solution with a global CIDR. Each cluster is given a subnet from this virtual Global Private Network, configured as new cluster parameter GlobalCIDR (e.g. 242.0.0.0/8) which is configurable at time of deployment. User can also manually specify GlobalCIDR for each cluster that is joined to the Broker using the flag globalnet-cidr passed to subctl join command. If Globalnet is not enabled in the Broker or if a GlobalCIDR is preconfigured in the cluster, the supplied globalnet-cidr will be ignored.\nCluster-scope global egress IPs By default, every cluster is assigned a configurable number of global IPs, represented by a ClusterGlobalEgressIP resource, which are used as egress IPs for cross-cluster communication. Multiple IPs are supported to avoid ephemeral port exhaustion issues. The default is 8. The IPs are allocated from a configurable global CIDR. Applications running on the host network that access remote clusters also use the cluster-level global egress IPs.\nNamespace-scope global egress IPs A user can assign a configurable number of global IPs per namespace by creating a GlobalEgressIP resource. These IPs are also allocated from the global CIDR and are used as egress IPs for all or selected pods in the namespace and take precedence over the cluster-level global IPs. In addition, the global IPs allocated for a GlobalEgressIP that targets specific pods in a namespace take precedence over the global IPs allocated for a GlobalEgressIP that just targets the namespace.\nService global ingress IPs Exported ClusterIP type services are automatically allocated a global IP from the global CIDR for ingress. For headless services, each backing pod is allocated a global IP that is used for both ingress and egress. However, if a backing pod matches a GlobalEgressIP then its allocated IPs are used for egress.\nRouting and iptable rules are configured to use the corresponding global IPs for ingress and egress. All address translations occur on the active Gateway node of the cluster.\nsubmariner-globalnet Submariner Globalnet is a component that provides cross-cluster connectivity from pods to remote services using their global IPs. Compiled as binary submariner-globalnet, it is responsible for maintaining a pool of global IPs, allocating IPs from the global IP pool to pods and services, and configuring the required rules on the gateway node to provide cross-cluster connectivity using global IPs. Globalnet also supports connectivity from the nodes (including pods that use host networking) to the global IP of remote services. It mainly consists of two key components: the IP Address Manager and Globalnet.\nIP Address Manager (IPAM) The IP Address Manager (IPAM) component does the following:\n Creates a pool of IP addresses based on the GlobalCIDR configured on cluster. Allocates IPs from the global pool for all ingress and egress, and releases them when no longer needed.  Globalnet This component is responsible for programming the routing entries, iptable rules and does the following:\n Creates initial iptables chains for Globalnet rules. For each GlobalEgressIP, creates corresponding SNAT rules to convert the source IPs for all the matching pods to the corresponding global IP(s) allocated to the GlobalEgressIP object. For each exported service, creates an ingress rule to direct all traffic destined to the Service\u0026rsquo;s global IP to the service\u0026rsquo;s kube-proxy iptables chain which in turn directs traffic to service\u0026rsquo;s backend pods. Clean up the rules from the gateway node on the deletion of a Pod, Service, or ServiceExport`.  Globalnet currently relies on kube-proxy and thus will only work with deployments that use kube-proxy.\nService Discovery - Lighthouse Connectivity is only part of the solution as pods still need to know the IPs of services on remote clusters.\nThis is achieved by enhancing lighthouse with support for Globalnet. The Lighthouse controller uses a service\u0026rsquo;s global IP when creating the ServiceImport for services of type ClusterIP. For headless services, backing pod\u0026rsquo;s global IP is used when creating the EndpointSlice resources to be distributed to other clusters. The Lighthouse plugin then uses the global IPs when replying to DNS queries.\nBuilding Nothing extra needs to be done to build submariner-globalnet as it is built with the standard Submariner build.\nUsage Refer to the Quickstart Guides on how to deploy Submariner with Globalnet enabled. For most deployments users will not need to do anything else once deployed. However, users can create GlobalEgressIPs or edit the ClusterGlobalEgressIP for specific use cases.\nEphemeral Port Exhaustion By default, 8 cluster-scoped global IPs are allocated which allows for ~8x64k active ephemeral ports. If those are still not enough for a cluster, this number can be increased by setting the NumberOfIPs field in the ClusterGlobalEgressIP with the well-known name cluster-egress.submariner.io:\napiVersion: submariner.io/v1alpha1 kind: ClusterGlobalEgressIP metadata: name: cluster-egress.submariner.io spec: NumberOfIPs: 9  Only the ClusterGlobalEgressIP resource with the name cluster-egress.submariner.io is recognized by Globalnet. This resource is automatically created with the default number of IPs.\n Global IPs for a Namespace If it\u0026rsquo;s desired for all pods in a namespace to use a unique global IP instead of one of the cluster-scoped IPs, a user can create a GlobalEgressIP resource in that namespace:\napiVersion: submariner.io/v1alpha1 kind: GlobalEgressIP metadata: name: ns-egressip namespace: ns1 spec: NumberOfIPs: 1 The example above will allocate 1 global IP which will be used as egress IP for all pods in namespace ns1.\nNumberOfIPs can have minimum value of 0 and maximum of 20\n Global IPs for a set of pods If it\u0026rsquo;s desired for a set of pods in a namespace to use unique global IP(s), a user can create a GlobalEgressIP resource in that namespace with the podSelector field set:\napiVersion: submariner.io/v1alpha1 kind: GlobalEgressIP metadata: name: db-pods namespace: ns1 spec: podSelector: matchLabels: role: db NumberOfIPs: 1 The example above will allocate 1 global IP which will be used as egress IP for all pods matching label role=db in namespace ns1.\n"
},
{
	"uri": "/community/roadmap/",
	"title": "Roadmap",
	"tags": [],
	"description": "",
	"content": "Submariner organizes all current and upcoming work using GitHub Issues, Projects, and Milestones.\nPlanning Process In preparation for sprint planning meetings (see Submariner\u0026rsquo;s Community Calendar), GitHub Issues should be raised for work that is to be a part of a sprint. Issues targeted for a sprint should be added to the upcoming Project\u0026rsquo;s \u0026ldquo;Backlog\u0026rdquo; column. During sprint planning meetings, Issues will be discussed and moved to the \u0026ldquo;TODO\u0026rdquo; column and the milestone will be set to the targeted release. As contributors make progress during sprints, Issues should be moved through the \u0026ldquo;In Progress\u0026rdquo;/\u0026ldquo;Review/Verify\u0026rdquo;/\u0026ldquo;Done\u0026rdquo; columns of the Project. If an Issue is implemented during a release but additional work (like an ACK-fixed verification) tracked by the relevant Issue is necessary, the Issue can be carried over to the next Project but the Milestone should reflect where the code was shipped for accurate release note creation.\nCurrent Work Current and near-future work is tracked by Submariner\u0026rsquo;s open Projects.\nFuture Work Some high-level goals are summarized here, but the primary source for tracking future work are Submariner\u0026rsquo;s GitHub Issues.\n Network Policy across clusters (Coastguard) Support for finer-grained connectivity policies (https://github.com/submariner-io/enhancements/pull/4/files) Dynamic routing with BGP to support multi-path forwarding across gateways Testing with multi-cluster Istio Scale improvements  Suggesting Work If we are missing something that would make Submariner more useful to you, please let us know. The best way is to file an Issue and include information on how you intend to use Submariner with that feature.\n"
},
{
	"uri": "/development/security/containers/",
	"title": "Container Requirements",
	"tags": [],
	"description": "",
	"content": "Current privilege setup is as follows, for non-test containers deployed by Submariner. Production containers not described here don’t use extra capabilities.\n   Container Capabilities Privilege escalation Privileged Read-only root Runs as non-root Host network Volume mounts     Gateway1 All Yes Yes No No Yes    Route agent1 All Yes Yes No No Yes    Globalnet1 All Yes Yes No No Yes    Lighthouse CoreDNS NET_BIND_SERVICE2 No No Yes Yes No /etc/coredns, read-only      This container needs to run iptables. \u0026#x21a9;\u0026#xfe0e;\n This is required to bind to port 53. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "/development/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": " Security Reporting Container Requirements  "
},
{
	"uri": "/development/shipyard/",
	"title": "Working with Shipyard",
	"tags": [],
	"description": "",
	"content": "Overview The Shipyard project provides common tooling for creating Kubernetes clusters with kind (Kubernetes in Docker) and provides a common Go framework for creating end to end tests. Shipyard contains common functionality shared by other projects. Any project specific functionality should be part of that project.\nA base image quay.io/submariner/shipyard-dapper-base is created from Shipyard and contains all the tooling to build other projects and run tests in a consistent environment.\nShipyard has several folders at the root of the project:\n package: Contains the ingredients to build the base image. scripts: Contains general scripts for Shipyard make targets.  shared: Contains all the shared scripts that projects can consume. These are copied into the base image under $SCRIPTS_DIR.  lib: Library functions that shared scripts, or consuming projects, can use. resources: Resource files to be used by the shared scripts.     test: Test library to be used by other projects.  Shipyard ships with some Makefile targets which can be used by consuming projects and are used by Shipyard\u0026rsquo;s CI to test and validate itself. It also has some specific Makefile targets which are used by the project itself.\nUsage Add Shipyard to a Project To enable usage of Shipyard\u0026rsquo;s functionality, please see Adding Shipyard to a Project.\nUse Shipyard in Your Project Once Shipyard has been added to a project, you can use any of the Makefile targets that it provides.\nAny variables that you need to pass to these targets should be specified in your Dockerfile.dapper so they\u0026rsquo;re available in the Dapper environment. For example:\nENV DAPPER_ENV=\u0026#34;REPO TAG QUAY_USERNAME QUAY_PASSWORD TRAVIS_COMMIT CLUSTERS_ARGS DEPLOY_ARGS\u0026#34;Have Shipyard Targets Depend on Your Project\u0026rsquo;s Targets Having any of the Shipyard Makefile targets rely on your project\u0026rsquo;s specific targets can be done easily by adding the dependency in your project\u0026rsquo;s Makefile. For example:\nclusters: build images Use an Updated Shipyard Image in Your Project If you\u0026rsquo;ve made changes to Shipyard\u0026rsquo;s base image and need to test them in your project, run:\nmake dapper-image in the Shipyard directory. This creates a local image with your changes available for consumption in other projects.\nShared Makefile Targets Shipyard ships a Makefile.inc file which defines these basic targets:\n clusters: Creates the kind-based cluster environment. deploy : Deploys submariner components in the cluster environment (depends on clusters). clean-clusters: Deletes the kind environment (if it exists) and any residual resources. clean-generated: Deletes all generated files. clean: Cleans everything up (running clusters and generated files). release: Uploads the requested image(s) to Quay.io. vendor/modules.txt: Populates go modules (in case go.mod exists in the root directory).  If your project uses Shipyard then it has all these targets and supports all the variables these targets support.\nAny variables supported by these targets can be either declared as environment variables or assigned on the make command line (takes precedence over environment variables).\nClusters A Make target that creates a kind-based multi-cluster environment with just the default Kubernetes deployment:\nmake clusters Respected variables:\n CLUSTERS_ARGS: Any arguments (flags and/or values) to be sent to the clusters.sh script. To get a list of available arguments, run: scripts/shared/clusters.sh --help  Deploy A Make target that deploys Submariner components in a kind-based cluster environment (if one isn\u0026rsquo;t created yet, this target will first invoke the clusters target to do so):\nmake deploy Respected variables:\n Any variable from clusters target (only if it wasn\u0026rsquo;t created). DEPLOY_ARGS: Any arguments (flags and/or values) to be sent to the deploy.sh script. To get a list of available arguments, run: scripts/shared/deploy.sh --help  Clean-clusters To clean up all the kind clusters deployed in any of the previous steps, use:\nmake clean-clusters This command will remove the clusters and any resources that might\u0026rsquo;ve been left in docker that are not needed any more (images, volumes, etc).\nClean-generated To clean up all generated files, use:\nmake clean-generated This will remove any file which can be re-generated and doesn’t need to be tracked.\nClean To clean everything up, use:\nmake clean This removes any running clusters and all generated files.\nRelease Uploads the built images to Quay.io:\nmake release release_images=\u0026#34;\u0026lt;image name\u0026gt;\u0026#34; Respected variables:\n QUAY_USERNAME, QUAY_PASSWORD: Needed in order to log in to Quay. release_images: One or more image names to release separated by spaces. release_tag: A tag to use for the release (default is latest). repo: The Quay repo to use (default is quay.io/submariner).  Specific Makefile Targets Shipyard has some project-specific targets which are used to build parts of the projects:\n dapper-image: Builds the base image that can be used by other projects. validate: Validates the go code that Shipyard provides, and the shared shell scripts.  Dapper-Image Builds the basic image which is then used by other projects to build the code and run tests:\nmake dapper-image Respected variables:\n dapper_image_flags: Any additional flags and values to be sent to the build_image.sh script.  "
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Submariner Submariner enables direct networking between Pods and Services in different Kubernetes clusters, either on-premises or in the cloud.\nWhy Submariner As Kubernetes gains adoption, teams are finding they must deploy and manage multiple clusters to facilitate features like geo-redundancy, scale, and fault isolation for their applications. With Submariner, your applications and services can span multiple cloud providers, data centers, and regions.\nSubmariner is completely open source, and designed to be network plugin (CNI) agnostic.\nWhat Submariner Provides  Cross-cluster L3 connectivity using encrypted VPN tunnels Service Discovery across clusters subctl, a friendly deployment tool Support for interconnecting clusters with overlapping CIDRs  A few requirements need to be met before you can begin. Check the Prerequisites section for more information.\n Check the Quickstart Guides section for deployment instructions.\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/getting-started/architecture/networkplugin-syncer/",
	"title": "Network Plugin Syncer",
	"tags": [],
	"description": "",
	"content": "The Network Plugin Syncer provides a framework for components to interface with the configured Kubernetes Container Network Interface (CNI) plugin to perform any API/database tasks necessary to facilitate routing cross-cluster traffic, like creating API objects that the CNI plugin will process or working with the specific CNI databases.\nThe detected CNI plugin implementation configured for the cluster is received by the Network Plugin Syncer, and executes the appropriate plugin handler component, if any.\nThe following table highlights the differences with the Route Agent:\n    Route Agent Network Plugin Syncer     Configures the CNI plugin  x   Configures low level network elements on the host x    Runs as a Kubernetes Deployment  x   Runs as a Kubernetes Daemonset on every host x     This component is only necessary for specific Kubernetes CNI plugins like OVN Kubernetes.\n "
},
{
	"uri": "/getting-started/architecture/networkplugin-syncer/ovn-kubernetes/",
	"title": "OVN Kubernetes",
	"tags": [],
	"description": "",
	"content": "A specific handler component is deployed for the OVN Kubernetes CNI plugin.\nOVN is a project that builds on top of Open vSwitch providing a rich high level API for describing virtual network components like Logical Routers, Logical Switches, Load balancers, Logical Ports. OVN Kubernetes is a Cloud Management System Plugin (CMS plugin) in terms of the OVN project.\nThe OVN Kubernetes handler watches for Submariner Endpoints and Kubernetes Nodes and interfaces with the OVN databases (OVN NorthDB and SouthDB) to store and maintain information necessary for Submariner, including:\n  A logical router named submariner_router that handles the communication to remote clusters and has a leg on the network which can talk to the ovn-k8s-gw0 interface on the Gateway node. This router is pinned to the active Gateway chassis.\n  Routing policies added to the existing ovn_cluster_router which redirect traffic targeted for remote routers through the submariner_router.\n  A submariner_join logical switch that connects the submariner_router with the ovn_cluster_router.\n  The handler architecture The following diagram illustrates the OVN Kubernetes handler architecture where the blue elements represent the OVN Kubernetes native network elements and the yellow elements are introduced by Submariner.\n"
},
{
	"uri": "/getting-started/architecture/route-agent/",
	"title": "Route Agent",
	"tags": [],
	"description": "",
	"content": "The Route Agent component runs on every node in each participating cluster. It is responsible for setting up the necessary host network elements on top of the existing Kubernetes CNI plugin.\nThe Route Agent receives the detected CNI plugin as part of its configuration.\nkube-proxy iptables For CNI plugins that utilize kube-proxy in iptables mode, the Route Agent is responsible for setting up VXLAN tunnels and routing the cross cluster traffic from the node to the cluster’s active Gateway Engine which subsequently sends the traffic to the destination cluster.\nWhen running on the same node as the active Gateway Engine, Route Agent creates a VXLAN VTEP interface to which Route Agent instances running on the other worker nodes in the local cluster connect by establishing a VXLAN tunnel with the VTEP of the active Gateway Engine node. The MTU of the VXLAN tunnel is configured based on the MTU of the default interface on the host minus the VXLAN overhead.\nRoute Agents use Endpoint resources synced from other clusters to configure routes and to program the necessary iptables rules to enable full cross-cluster connectivity.\nWhen the active Gateway Engine fails and a new Gateway Engine takes over, Route Agents will automatically update the route tables on each node to point to the new active Gateway Engine node.\nOVN Kubernetes For the OVN Kubernetes CNI plugin, host network routing is configured on all nodes and, on the active Gateway node, IP forwarding is configured between the ovn-k8s-gw0 and cable interfaces.\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]